# 笔记

## 20230901

### 1_Cross-stage neural pattern similarity in the hippocampus predicts false memory derived from post-event inaccurate information_Nature Communications 2023_认知神经科学

> 作者：Xuhao Shao 1,2,3,4, Ao Li1, Chuansheng Chen5, Elizabeth F. Loftus5 &Bi Zhu

> 贡献：

该研究发现，人脑海马体在记忆测试阶段同时存在原始事件和虚假信息的记忆痕迹。而且，每个人的大脑海马体对虚假信息的记忆表征只能预测自己的错误记忆行为。此外，当人脑海马体的虚假信息记忆痕迹较强而原始事件记忆痕迹较弱时，**这种记忆信号差异能够引发大脑外侧前额叶的监控**。

该研究系统阐明了虚假信息引发人类错误记忆的脑机制，支持并扩展了学习记忆的**多重痕迹理论**和**激活监控理论**，对了解人脑记忆重构本质有重要科学意义，对教育和法律有重要应用价值。

> 主要内容：

**虚假信息效应**（misinformation effect）是指原始事件记忆因事后虚假信息而改变。它包括三阶段：首先个体看到某个事件，然后接受事后虚假信息，最后完成针对原始事件的记忆测验(图1)。

人脑海马体似乎与错误信息效应有关键关系，然而海马体的记忆表征在目击原始事件、接受虚假信息、记忆测验这**三个阶段**是如何变化的还不清晰。

人们提出了三个理论观点来解释错误信息效应： non-retention, trace-alteration, and multiple-trace models

![image-20230824105309582](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230824105309582.png)

* 不保留（a）

  原始事件的表征不会形成或者是在接收错误信息之前就会丢失；

* 路径更改（b）

  原始事件的海马体表征在 post-event 阶段被保留，但随后被错误信息覆盖，因此不会延续到记忆测试阶段；

* 多路径（c）

  原始事件的海马表征在三个阶段保持完整，它们在记忆测试阶段与错误信息的表征**竞争**。与其他两个观点不同的是，多路径模型在记忆测试阶段预测时，原始事件的海马体表征的存在，即使错误信息可以有效地产生错误记忆。

  这里涉及的一个关键机制是**认知控制**，由前额叶皮层的某些部分提供的服务。在记忆测试中，解决海马体中两个记忆痕迹之间的竞争，腹外侧、内侧和背外侧前额叶皮层可能参与选择目标相关的记忆原始信息，抑制不适当的记忆错误信息，并监控错误信息和原始信息之间的差异。

  然而，在记忆测试中，**前额叶皮层的哪些部分可能与海马体协同工作来解决这种冲突尚不清楚**。

因此，该研究采用功能磁共振脑成像技术和基于模型的多体素模式分析方法，**计算了人脑海马体在跨记忆阶段的神经模式相似性，揭示了人脑海马体中真假信息记忆表征互相竞争而产生错误记忆的神经机制**。

首先，进行了一项行为研究，确保为神经成像研究修改的错误信息范式能够按照预期进行工作（即错误信息组比中性和一致的组导致了更多的错误记忆）。

然后，实验二通过比较记忆反应类型（即真实记忆（true）、错误记忆（false）、干扰选项（foil）、正确控制（correct）和错误控制（incorrect））之间的行为和神经指标来检验错误信息效应。利用 fMRI 数据的表征相似性分析，比较了三个阶段的海马模式相似性，并检验了上述三种理论的预测。我们的研究结果显示，**海马体的跨阶段神经模式相似性预测了错误信息效应。**

> 结果：

**Behavioral results**

在第一个实验中，不同组别的参与者在记忆测试中的表现存在差异。统计分析显示，组别（错误信息组、中性信息组和一致信息组）与记忆类型（真实、虚假、干扰、正确和错误）之间存在显著交互作用。

预期结果是，错误信息组的真实记忆（即原始信息的认可率）低于中性信息组，而中性信息组又低于一致信息组。相反，错误信息组的虚假记忆（即对虚假信息的认可率）高于中性信息组，而中性信息组又高于一致信息组。

![image-20230824160943556](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230824160943556.png)

这种错误信息效应在两个实验中得到了进一步的证实。在第一个实验的错误信息组中，虚假记忆的比例（41%）高于干扰信息的认可率（9%）。在第二个实验（fMRI 研究）中，错误信息组的虚假记忆（42%）也高于干扰信息的认可率（14%)。

**Hippocampal representations of true, false, correct, and incorrect memories**

结果显示，在某些情况下，四种记忆类型的海马区模式相似性在对应项目上高于不对应项目，这表明存在项目特异性表示。这种情况在不同的阶段对于不同的记忆类型有所不同。

对于真实记忆，海马区模式相似性在原始事件和后续事件之间（OP）中，对应项目高于不对应项目，但在其他阶段（OM、PM）没有这种情况。
对于虚假记忆，项目特异性的效应明显，但是阶段的效应以及阶段与项目特异性之间的交互作用并不明显。
对于正确对照和错误对照，项目特异性的效应也是明显的，但阶段的效应和交互作用并不明显。
在两个阶段的两两组合中，只有在后续事件和记忆测试之间（PM）中，虚假记忆和正确对照的项目特异性的效应显著，而真实记忆和错误对照则不显著。此外，在这个阶段，虚假记忆和正确对照的物品特异性效应也大于真实记忆。

![image-20230824154617384](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230824154617384.png)

真实记忆在某些情况下与项目特异性相关，而虚假记忆和正确对照则在后续事件和记忆测试之间具有更强的项目特异性表示。这些结果与多重痕迹理论相吻合。

**Hippocampal representation of post-event information predicts false memory**

通过对虚假记忆和海马区后续事件信息表示之间的个体内和个体间相关性的比较，研究发现每个个体在海马区对后续事件信息的表示上有部分独特性，而这种表示与虚假记忆的行为模式相关联。然而，这种映射关系在其他情况下不明显，如海马区对原始信息的表示以及正确对照的行为模式。

**Prefrontal activity correlates with hippocampal representations when false memory occurs**

通过全脑探索性分析，并在记忆测试期间的海马区活动水平控制下，我们发现了两个集群，位于左侧外侧前额叶皮层和右侧外侧前额叶皮层。这些集群的活动与虚假记忆中 PM 中的海马区项目特异性表示正相关程度高于 OM，且虚假记忆中该效应比正确对照组更加明显。

**Cortical representations and their connectivity with hippocampus**

当前研究主要关注虚假记忆效应下的海马区表示。然而，皮层表示也可能在这个过程中发挥作用。因此，研究探索了在 OM 和 PM 的项目特异性表示方面，真实记忆和虚假记忆之间是否存在其他大脑区域的差异。

多个大脑区域（例如左角回）在真实记忆的OM中显示出比虚假记忆更大的项目特异性表示，但只有后扣带回在虚假记忆的PM中显示出比真实记忆更大的项目特异性表示（图6a）；

外侧顶叶皮层（例如左角回）显示出真实记忆的个体特异性神经行为相关，而内侧顶叶皮层（例如后扣带回）显示出虚假记忆的个体特异性神经行为相关（图6b）；

基于以海马区为种子的全脑分析，海马区与一些皮层区域之间的相关系数（例如左角回）在OM中在真实记忆中比虚假记忆更高，但海马区与枕上皮层（延伸至后扣带回）之间在PM中的相关系数在虚假记忆中比真实记忆更高（图6c）。

![image-20230824160807288](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230824160807288.png)

### 2_CLIPN for Zero-Shot OOD Detection: Teaching CLIP to Say No_ICCV 2023_有代码

> 作者：Hualiang Wang, Yi Li, Huifeng Yao, Xiaomeng Li*

> 代码：https://github.com/xmed-lab/CLIPN

> 贡献：

背景：现有 OOD 方法基本上是在卷积网络或 Transformer 上做的改进，对于难 OOD 样本的性能较差；另外使用 CLIP 来做 OOD 检测的方法还没怎么被探索。

![image-20230825194027987](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230825194027987.png)

本文致力于探索 CLIP 中的 *open-world* knowledge，从而使模型能够检测 ID-ness 较高的难 OOD 样本。

由于 CLIP 模型不具备 ”no“ 的逻辑（如下图），因此本文主要的贡献在于教会 CLIP 说 ”no“，即提出模型 **CLIPN**。为了让 CLIP 具备通过阳性语提示和否定语义提示来区分 OOD 和 ID 样本的能力，作者设计了可学习的 “no” 提示和 “no” 文本编码器，以捕获图像中的否定语义。另外还引入了两个损失函数：图像文本二元对立损失和文本语义对立损失，教 CLIPN 将图像与 “no” 提示关联起来，从而使其能够识别未知样本。然后作者提出了两种无阈值的推理算法，利用来自 “no” 提示和文本编码器的否定语义来执行 OOD 检测。

![image-20230825194737288](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230825194737288.png)

> 方法：

Image Encoder 和 Text Encoder 来自 CLIP，训练时都冻结参数；”no“ text encoder 用 Text Encoder 初始化，训练时更新。可学习的 ”no“ 提示采用 CoOp 那种半可学习的形式（由可学习参数+类名构成）。

![image-20230825195417103](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230825195417103.png)

**训练损失的设计：**

* **Image-Text Binary-Opposite Loss (ITBO)**

  在特征空间中通过 match-ness 让图片和对应的 ”no“ text 对应上：

  ![image-20230825200248240](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230825200248240.png)，![image-20230825200303728](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230825200303728.png)

* **Text Semantic-Opposite Loss (TSO)**

  在特征空间中，与同一张图片对应的正向语义和负向语义应当远离彼此：

  ![image-20230825200418176](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230825200418176.png)，g 是 ”no“ text encoder 的输出；

![image-20230825200053287](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230825200053287.png)

**Inference algorithm of CLIPN**

对于 ID 类别，通过![image-20230825200856265](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230825200856265.png) 计算对应 ID 中 C 个类别的概率；

对于 OOD 类别，作者提出两个  threshold-free 的算法来检测：

* **Competing-to-win Algorithm（CTW）**

  $p^{no}$ 由公式4计算得到，然后根据 $p^{no}$ 和 $p^{yes}(即1-p^{no})$ 的大小来判断是否为 OOD，如果 $p^{no} > p^{yes}$，则检测为 OOD；![image-20230825201332103](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230825201332103.png)

* **Agreeing-to-differ Algorithm（ATD）**

  如果 ID 的每个类别分数都差不多的话，CTW 方法则显得有些莽撞（因为根据 max p_ik 选出的类别不具有说服力了），因此作者提出类似投票表决的方法来检测 OOD，首先根据所有 ID 类的分类概率和 match-ness 添加一个 unknown 分类概率：![image-20230825204750112](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230825204750112.png)，然后 OOD 的判别如下：![image-20230825204915719](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230825204915719.png)

  即根据公式 8 计算得出的 unknown 类概率如果大于所有 ID 类概率，则将该图片检测为 OOD。

> 总结：

1. 本文在 CLIP 的训练数据中，加入了 ”no“ text prompt 进行联合训练，让 CLIPN 具备识别 ”no“ 的能力；
2. 本文中预训练好的 ”no“ text encoder 可能可以用用；
3. ATD 这种类似于投票表决的方法挺好（先用所有已知类的分类分数结合一个类似于 ID-ness 的分数进行加权平均得到 ”已知类分数“，然后用 1 减它表示未知类分数）。

## 20230908

### 3_IOMatch: Simplifying Open-Set Semi-Supervised Learning with Joint Inliers and Outliers Utilization_ICCV 2023_有代码

> 作者：Zekun Li, Lei Qi, Yinghuan Shi*, Yang Gao

> 代码：https://github.com/nukezil/IOMatch

> 贡献：

背景：开放集半监督任务的当前主流方法倾向于首先检测异常值，然后将其过滤掉。作者发现，当标签极其稀缺时，这种方法可能会导致更严重的性能下降，因为不可靠的异常值检测器可能会错误地排除相当一部分有价值的内部值。

为了解决这个问题，作者提出 IOMatch，联合利用内部值和异常值。使用多二元分类器与标准闭集分类器相结合来产生统一的开放集分类目标，将所有异常值作为一个新类；通过采用这些目标作为开放集伪标签，使用所有未标记样本（包括内部值和异常值）优化开集分类器。

> 方法：

总体流程：

**对于 labeled batch**，首先进行弱增强，然后编码得到特征 $h_{i}$，闭集分类器作用于 $h_{i}$ 后的到闭集分类概率 $p_{i}$，使用标准交叉熵损失进行优化![image-20230905180450041](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230905180450041.png)，使用一个投射变换头将 $h_{i}$ 降维得到 $z_{i}$， $z_{i}$ 送入多二元分类器获得 inliers or outliers 的分数 $o_{i}$；

**对于unlabeled batch** ，分别进行弱增强和强增强，并获得相应的 $h,p,z,o$。另外开集分类器会预测 unlabeled samples 的开集概率分布，其中所有的 outliers 会被视为一个单一的 new class。

![image-20230905175434803](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230905175434803.png)

在每次迭代时，使用闭集分类器和多二元分类器来产生开集 targets，开集 target 生成方式如 Figure 2 所示。

<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230905175726155.png" alt="image-20230905175726155"  />

 **Unified Open-Set Targets Production**

对于 unlabeled sample，结合闭集分类结果和多二元分类结果 ![image-20230905181620364](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230905181620364.png)来衡量其属于某个可见类的概率；那么某个 unlabeled sample 是 outlier 的概率为![image-20230905181716115](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230905181716115.png)；

合在一起为：![image-20230905181802043](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230905181802043.png)，这将作为所有 unlabeled samples 的开集 targets。

**Joint Inliers and Outliers Utilization**

对于 **Outliers**：![image-20230905182336038](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230905182336038.png)，s 代表强增强版本；

对于 **Inliers**：![image-20230905182425843](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230905182425843.png)，其中 F 表示使用的双层过滤策略，![image-20230905182510138](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230905182510138.png)

总体的算法流程如下：

![image-20230905121210853](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230905121210853.png)



> 总结：

1. 对弱增强版本的无标签数据，使用闭集分类器结合多二元分类器的方式为对应的强增强版本样例产生伪标签
2. **闭集分类器结合二元分类器的使用确定某个样例属于已知类的概率**

### 4_Dual Compensation Residual Networks for Class Imbalanced Learning_TPAMI 2023_无代码

> 作者：Ruibing Hou, Hong Chang, Bingpeng Ma, Shiguang Shan, Xilin Chen

> 贡献：

背景：对于类不平衡的数据，学习可推广的表征和分类器对于数据驱动的深度模型来说是一个挑战。大多数研究试图重新平衡数据分布，这容易导致模型对尾部类别过拟合并对头部类别欠拟合。

因此本文提出双补偿残差网络（ DCRNets，Dual Compensation Residual Networks），以更好地适应尾部和头部类。首先，作者提出**双特征补偿模块（FCM）**和 **Logit补偿模块（LCM）**来缓解过拟合的问题。这两个模块的设计是基于观察的结果：导致过拟合的一个重要因素是在尾类的训练数据和测试数据之间存在严重的特征漂移，即尾部类别的测试特征倾向于向多个相似头部类别的特征漂移。因此，FCM 估计每个尾部类别的多模式特征漂移方向，并对其进行补偿。此外，LCM 将 FCM估计的确定性特征漂移向量沿类内变化进行转换，从而覆盖更大的有效补偿空间，从而更好地拟合测试特征。然后，作者还提出了一种残余平衡多代理分类器（**RBMC**）来缓解欠拟合的问题。由于观察到再平衡策略阻碍了分类器学习足够的头部知识并最终导致不拟合，RBMC 利用具有残差路径的统一学习来促进分类器学习。

> 方法：

如下图，输入端又两个分支，分别是均匀采样分支和类平衡采样分支。其中分均匀采样支主要旨在学习可推广的特征；类平衡采样分支则用于学习 less biased 分类器。

基于此，还有三个提出的新模块：FCM 和 LCM 旨在通过补偿训练数据和测试数据之间的特征漂移来缓解尾部类的过拟合问题；RBMC 旨在通过添加一个从均匀分支到类平衡分支的残差路径来缓解头部类的欠拟合问题。

![image-20230906134433909](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230906134433909.png)

**Feature Compensation Module**

**特征飘移的分析：**

1. 由于尾部类别包含少量的训练样本，导致特征提取器仅能到不完整的特征表示，因此会对训练样本的部分特征过拟合。而有的尾部样本会和某些头部样本存在相似的特点，因此尾部类别的测试特征可能会向相似头部类别的特征空间漂移。
2. 一个尾类别的测试样本通常具有不同的特征，如不同的形状、大小和姿势。因此，不同的测试样本可能与不同的头部类别共享特征，导致向多个相似的头部类别漂移。

​	**特征飘移的计算：**——基于类别之间的相似性

首先基于类原型之间的相似性来找到尾部类的相似头部类：![image-20230906142913439](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230906142913439.png)，

```St stores m nearest head categories with respect to the tail category t.```

特征飘移量：![image-20230906143107650](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230906143107650.png)，$\alpha$是补偿系数，由于学习特征的泛化往往与训练样本的数量呈正相关，因此训练样本较少的类别更容易更接近相似的头部类别，因此系数设置为和样本训练数量相关：![image-20230906143139847](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230906143139847.png)，Nmax表示训练样本数量最多的类中包含的训练样本数。

​	**特征飘移补偿**：

基于上述评估的特征飘移向量，FCM 补偿原始的训练特征，以减少特征漂移。根据![image-20230906150609859](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230906150609859.png)计算漂移概率 $s_{tj}$和保留特征概率 $s_{tt}$。

补偿特征计算公式：![image-20230906150757081](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230906150757081.png)。

![image-20230906151319405](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230906151319405.png)

**Logit补偿模块**：

虽然 FCM 可以粗略估计特征漂移方向，但尾类的测试特征在训练过程中是未知的，通常表现出复杂的分布。因此，由 FCM 估计的漂移向量容易出现误差，难以覆盖所有的漂移方向。为此，作者提出了**将不确定性纳入漂移估计**的 LCM。加入不确定性可以防止后续的分类器对 FCM 误差的过拟合，从而提高其鲁棒性。此外，引入不确定性可以扩展估计区域，覆盖更多的漂移方向，从而产生更多多样化的尾类特征，以改善分类边界。

具体做法为，引入高斯噪声：![image-20230906151624490](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230906151624490.png)

补偿logits：![image-20230906152829547](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230906152829547.png)，

**残差平衡多代理分类器：**

如上分析，一个尾部类的测试特征倾向于向多个相似的头部类漂移。因此，一个尾类的测试特征的分布呈现多模态势。在简单的线性分类器中，每个类的权值向量作为一个单一的代理，这在多模式设置下很难进行优化。为此，作者考虑了一个多代理分类器来更好地捕获复杂的特征分布。

![image-20230906153745166](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230906153745166.png)

![image-20230906153759484](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230906153759484.png)

在这种设计中，RBMC 在CBS（通过残差分类器）和US（通过均匀分类器）方案下共同学习。因此，通过均匀分类器可以恢复被 CBS 破坏的头部信息。从这个意义上说，RBMC 可以利用来自整个训练数据集的信息来缓解拟合不足问题，同时避免了对头部类的高度倾斜。

总体算法流程：

![image-20230906154229168](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230906154229168.png)



> 总结：

* 根据类别原型的相似性来评估特征飘移的程度
* 尾类特征飘移 ---> 头部类特征（一到多）
* LCM 中通过加入噪声，增加随机性来减少分类器对 FCM 错误的偏差，并且提高分类器的鲁棒性；

## 20230915

### 5_Semi-Supervised Object Detection in the Open World_xx_无代码

> 作者：Garvita Allabadi Ana Lucic Peter Pao-Huang Yu-Xiong Wang Vikram Adve

> 贡献：

背景：现有的半监督目标检测方法假设训练和未标注数据集中存在一组固定的类，即分布内（ID）数据。当这些方法部署在开放世界中时，由于未标注和测试数据可能包含训练过程中 unseen 的目标，即分布外（OOD）数据，性能会显著降低。

在本文中探讨的两个关键问题是：能检测到这些 OOD 样本吗？如果能，能从中学习吗？

考虑到这些因素，作者提出了**开放世界半监督检测框架**（OWSSD），该框架可以有效地检测 OOD 数据，以及从 ID 和 OOD 数据中学习的半监督学习 pipeline。作者介绍了一种**基于集成的 OOD 检测器**，该检测器由仅根据 ID 数据训练的轻量级自动编码器网络组成。

> 方法：

本文提出的框架由三个组成部分组成：

1. 用于开放世界目标检测的类不可知proposal生成器；
2. 用于 OOD 检测的基于集成的网络；
3. 在学习过程中包括 ID 和 OOD 数据的 OOD 感知半监督学习pipeline

采用师生模式，使用两阶段的训练过程。

在第一阶段，使用弱增广的标记数据来训练教师模型；

在第二阶段，使用标记数据和强增广的未标记数据训练学生模型，并使用一致性正则化范式，即使在增广后，也强制模型对未标记样本输出相同的预测。

![image-20230913151518488](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230913151518488.png)

**Open-World Object Proposals**：

使用 OLN 网络在标记的数据上训练，然后用于无标记数据来生成 proposals，并基于(1) objectness scores 和(2) individual areas of proposals（较小的框会被过滤掉，以减少整体噪声） 筛选 proposals。

**Ensemble-Based OOD Detector**：

![image-20230913152454458](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230913152454458.png)

![image-20230913153322965](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230913153322965.png)<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230913153338899.png" alt="image-20230913153338899" style="zoom:80%;" />

> 总结：

* 使用 OLN 产生类不可知 proposals
* 基于多个低纬度数据特征自编码网络集成的投票决策机制

### 6_Attention with or without working memory: mnemonic reselection of attended information_Trends in Cognitive Sciences 2023

> 作者：Yingtao Fu,1,3 Chenxiao Guan,1,3 Joyce Tam,2 Ryan E. O’Donnell,2 Mowei Shen,1,* Brad Wyble,2,*and Hui Chen ,1,*

> 贡献：

长期以来，注意一直被视为人类实现信息选择的决定性因素，即通过注意选择少部分重要信息进行深入加工，并将其存储至工作记忆，而未被注意的信息则被过滤。因此，注意往往被比喻为信息进入工作记忆的“闸门”。

陈辉教授课题组近年来的系列研究证明，即使已经注意并使用过的信息，工作记忆仍然会对其进行选择性存储，挑战了“注意决定工作记忆”这一传统观点。本文系统梳理了这部分研究的新进展，并在此基础上，首次提出了**“工作记忆再选择”**理论模型。该理论认为，人类信息选择存在两个过程：**注意选择过程**和**记忆再选择过程**。对于那些已被注意选择的信息，还存在第二轮的记忆选择，该过程最终决定了哪些信息可以被记忆存储。该理论模型描述了注意选择和记忆选择的协同作用过程，强调了人类信息选择的灵活性和适应性。

> 模型：

属性失忆（Attribute Amnesia）：人们经常无法报告他们刚才刚刚关注到的信息。

这种报告的失败被认为是由于缺乏将被参与的信息整合到工作记忆中，这表明了注意力和工作记忆之间的分离。

在这些发现的基础上，本文提出了一个称为**记忆重新选择**的新概念来描述被参与信息中的第二轮选择。这些发现挑战了关于注意力和工作记忆是如何相互关联的传统观点，并为将注意力和记忆建模为可分离的过程提供了新的线索。

**工作记忆再选择模型**

![image-20230914224956130](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230914224956130.png)

* 图的最下面部分：表示对外部世界的注意力选择。（选择的对象具有高分辨率，而未选择的对象则显得模糊）
* 图的中间部分：代表我们在关注的选定对象的具体特征（如颜色和形状）
* 图的最上面部分：显示了工作记忆如何**重选**关注的特征。（例如，一个特征（颜色）被引入工作记忆，而其他特征（如形状）则被排除）

> 总结：

* **属性遗忘**将任务中注意和记忆信息编码解耦，证明注意力选择的信息并不总是被工作记忆选择，甚至可能受到积极抑制；
* 因此本文提出了一个**记忆重选模型**，该模型描述了如何通过选择性存储关注的信息来形成工作记忆表征。

## 20230922

### 7_On the Robustness of Open-World Test-Time Training: Self-Training with Dynamic Prototype Expansion_ICCV 2023_有代码

> 作者：Yushu Li1   Xun Xu2   Yongyi Su1   Kui Jia1

> 代码：https://github.com/Yushu-Li/OWTTT

> 贡献：

背景：提高模型泛化能力是推动基于视觉的感知方法落地的重要基础，测试段训练和适应（Test-Time Training/Adaptation）通过在测试段调整模型参数权重，将模型泛化至未知的目标域数据分布段。现有 TTT/TTA 方法通常着眼于在闭环世界的目标域数据下提高测试段训练性能。

为促进开放场景下的 TTT 应用，研究的重点已转移到调查 TTT 方法可能失败的场景。人们在更现实的开放世界环境下开发稳定和强大的 TTT 方法已经做出了许多努力。

而在本文工作中，作者深入研究了一个很常见但被忽略的**开放世界场景**，其中目标域可能包含从显著不同的环境中提取的测试数据分布，例如与源域不同的语义类别，或者只是随机噪声。作者将这样的测试数据称为**强分布外数据**（strong OOD），相应的，**弱 OOD 数据**则是分布偏移的测试数据，例如常见的合成损坏。

如下图所示，作者首先对现有的 TTT 方法在 OWTTT 设定下进行评估，发现通过自训练和分布对齐的 TTT 方法都会受到强 OOD 样本的影响。这些结果表明，应用现有的 TTT 技术无法在开放世界中实现安全的测试时训练。作者将它们的失败归因于以下两个原因：1、基于自训练的 TTT 很难处理强 OOD 样本，因为它必须将测试样本分配给已知的类别。尽管可以通过应用半监督学习中采用的阈值来过滤掉一些低置信度样本，但仍然不能保证滤除所有强 OOD 样本；2、当计算强 OOD 样本来估计目标域分布时，基于分布对齐的方法将会受到影响。全局分布对齐和类别分布对齐都可能受到影响，并导致特征分布对齐不准确。

![image-20230915223017951](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230915223017951.png)

因此本文提出了一种**自适应阈值的强域外数据样本过滤**方法，提高了自训练 TTT 方法的在开放世界的鲁棒性。该方法进一步提出了一种基于**动态扩展原型**来表征强域外样本的方法，以改进弱 / 强域外数据分离效果。最后，通过分布对齐来约束自训练。

> 方法：

**任务设定：**
TTT 的目的是使源域预训练模型适应目标域，其中目标域可能会相对于源域有分布迁移。在标准的封闭世界 TTT 中，源域和目标域的标签空间是相同的。然而在开放世界 TTT 中，目标域的标签空间包含源域的目标空间，也就是说目标域具有未见过的新语义类别。
为了避免 TTT 定义之间的混淆，本文采用 TTAC 中提出的顺序测试时间训练（sTTT）协议进行评估。在 sTTT 协议下，测试样本被顺序测试，并在观察到小批量测试样本后进行模型更新。对到达时间戳 t 的任何测试样本的预测不会受到到达 t+k（其 k 大于 0）的任何测试样本的影响。

**TTT by Prototype Clustering**

受到域适应任务中使用聚类的工作启发，将测试段训练视为发现目标域数据中的簇结构。通过将代表性原型识别为聚类中心，在目标域中识别聚类结构，并鼓励测试样本嵌入到其中一个原型附近。原型聚类的目标定义为最小化样本与聚类中心余弦相似度的负对数似然损失：

![image-20230915224441997](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230915224441997.png)

如果强 OOD 样本被强制归类为任何源类别，对有噪声的标记样本进行自训练会混淆网络对弱OOD样本的鉴别能力。因此作者开发了一种**无超参数的方法来滤除强 OOD 样本**，以避免调整模型权重的负面影响。具体来说，为每个测试样本定义一个强 OOD 分数 os 作为与源域原型的最高相似度：

![image-20230915224656897](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230915224656897.png)

由于观察到离群值服从双峰分布，如下图所示。因此，我们没有指定固定阈值，而是将最佳阈值定义为分离两种分布的的最佳值。具体来说，问题可以表述为将离群值分为两个簇，最佳阈值将最小化中的簇内方差。优化下式可以通过以 0.01 的步长穷举搜索从 0 到 1 的所有可能阈值来有效实现：

![image-20230915225449312](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230915225449312.png)![image-20230915225504921](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230915225504921.png)

![image-20230915225418889](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230915225418889.png)

**Open-World TTT by Prototype Expansion：**

扩展强 OOD 原型池需要同时考虑源域和强 OOD 原型来评估测试样本。为了从数据中动态估计簇的数量，之前的研究了类似的问题。确定性硬聚类算法 DP-means 是通过测量数据点到已知聚类中心的距离而开发的，当距离高于阈值时将初始化一个新聚类。DP-means 被证明相当于优化 K-means 目标，但对簇的数量有额外的惩罚，为动态原型扩展提供了一个可行的解决方案。
为了减轻估计额外超参数的难度，首先定义一个测试样本，其具有扩展的强 OOD 分数作为与现有源域原型和强 OOD 原型的最近距离，![image-20230915225835048](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230915225835048.png)。因此，测试高于此阈值的样本将建立一个新的原型。为了避免添加附近的测试样本，增量地重复此原型扩展过程。

随着其他强 OOD 原型的确定，定义了用于测试样本的原型聚类损失，并考虑了两个因素。

首先，分类为已知类的测试样本应该嵌入到更靠近原型的位置并远离其他原型，这定义了 K 类分类任务；

其次，被分类为强 OOD 原型的测试样本应该远离任何源域原型，这定义了 K+1 类分类任务。

原型聚类损失定义为：![image-20230915225959496](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230915225959496.png)

**Distribution Alignment Regularization：**

由于自训练容易受到错误伪标签的影响，目标域包含 OOD 样本时，情况会更加恶化。为了降低失败的风险，作者进一步将分布对齐作为自训练的正则化约束：

![image-20230915230115642](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230915230115642.png)

算法流程：

![image-20230915230224438](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230915230224438.png)

> 总结：

* 原型扩张 ---> 相当于对 OOD 也建立老巢，以降低其对 ID 的影响

### 8_Adaptive Plasticity Improvement for Continual Learning_CVPR 2023_有代码

> 作者：Yan-Shuo Liang and Wu-Jun Li*

> 代码：https://github.com/liangyanshuo/Adaptive-Plasticity-Improvement-for-Continual-Learning

> 贡献：

背景：许多研究都试图解决持续学习中的灾难性遗忘（CF）问题。然而，在旧任务上追求不遗忘可能会损害模型对新任务的可塑性。虽然已经提出了一些方法来实现稳定性-可塑性的权衡，但**没有一种方法考虑评估模型的可塑性和提高可塑性**。

在这项工作中，作者提出了一种新的方法，称为**自适应可塑性改进（API）**。除了能够克服对旧任务的 CF 外，API 还试图评估模型的可塑性，然后在必要时自适应地提高模型的可塑性，以便学习新任务。

> 方法：

如下图，以一个简单的三层神经网络为例解释 API：除了最后一层，其他层都根据相应需要来决定是否增加输入维度，并增加相应的权重参数。

对于每个 task t，API 在克服遗忘的基础上，首先评估模型的可塑性，然后根据评估结果考虑是否增加输入维度，即扩张权重参数，如果可塑性足够，那么就不需要扩张权重。

![image-20230721100143169](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230721100143169.png)

API 采用梯度修正策略 GPM 来克服 CF。基于该策略的方法修正了新的任务梯度，使其不影响模型在旧任务上的表现。但由于 GPM 存在一个问题，即内存占用会一直增加，因此作者使用 **DualGPM** 来解决内存占用一直增加的问题：<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230721102803997.png" alt="image-20230721102803997" style="zoom:80%;" />

![image-20230721103227991](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230721103227991.png)

**Plasticity Evaluation**

gradient retention ratio (GRR)：<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230721102903418.png" alt="image-20230721102903418" style="zoom:80%;" />

扩张标准：<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230721103019037.png" alt="image-20230721103019037" style="zoom:80%;" />

![image-20230721103310368](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230721103310368.png)

> 总结：

* 从梯度的角度去考虑问题，在克服遗忘的前提下同时保持模型持续学习过程中的可塑性；

## 20230929

### 9_EdaDet: Open-Vocabulary Object Detection Using Early Dense Alignment_ICCV 2023_代码待上传

> 作者：Cheng Shi, Sibei Yang†（上科大）

> 贡献：

背景：开放词汇目标检测旨在在只有基础类别的训练标签的情况下，定位和识别基础类别和新类别的对象。过去的方法利用视觉-语言模型的强大的零样本识别能力，将对象级别的嵌入与类别的文本嵌入进行对齐。然而，**现有方法中存在对基础类别过拟合的问题**，即与基础类别最相似的新类别的性能特别差，因为它们被识别为相似的基础类别。

 过去的方法通常通过生成与新类别相关的伪提议，并使用基础和新类别进行训练来解决开放词汇目标检测问题。然而，这些方法需要额外的训练资源，这些资源与新类别有很大的重叠或相关性，限制了它们的实际应用。

本文首先发现现有方法由于**丢失了关键的细粒度局部图像语义**而无法实现强大的基础到新类别的泛化能力。然后，作者提出了**早期密集对齐（EDA）**方法，以弥合泛化的局部语义和对象级别预测之间的差距。EDA 使用对象级别监督来学习密集级别而不是对象级别的对齐，以保持局部细粒度语义的一致性。

> project page：https://chengshiest.github.io/edadet

> 方法：

作者将开放词汇表检测分解为两个后续分支： (1)生成类不可知的对象建议，(2)识别这些对象建议的开放词汇表类别。

![image-20230908165323649](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230908165323649.png)

如图 3b 所示。作者认为，分类和回归这两个分支之间高度相关的预测损害了类别不可知的建议框生成。因此，作者将开放词汇表分类从解码器更浅层的提案生成分支中分离出来，如图3a所示，以深入地解耦这两个分支。

**Early Semantic Alignment at Dense Level (Eda)**

与之前的方法将 object-level 特征对齐到 base classifier（基类文本特征）不同，本文在早期阶段使用 object-level 的监督将密集局部图片信息（dense local image semantics）对齐到 base classifier。然后根据 dense probabilities 将proposals 分类到不同的类别。由于密集级别对齐可以保持细粒度的识别能力，以区分相似的新类别和基类别的局部语义细节，从而能够区分它们。

局部图片语义信息：![image-20230908170131620](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230908170131620.png)

CLIP’s dense probability map：![image-20230908170220071](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230908170220071.png)

最终计算得到的 dense score map S：![image-20230908170247919](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230908170247919.png)

某个 proposal 属于某个类别的概率：![image-20230908170423444](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230908170423444.png)

**Global Semantic Alignment**

若只有局部对齐会导致图像的全局语义信息的丢失，因此作者将集成的局部图像语义与 CLIP 的图像编码器进行对齐，以提高密集对齐。

![image-20230908170557818](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230908170557818.png)

> 总结：

* 尽早解耦分类头与回归头，减少两者的干预程度；
* 在 low-level 处引入细粒度密集图片语义信息的监督，使模型能够辨别相似类别之间的差异，从而减少模型将与基类较为相似的新类误检为基类；
* mark：等代码出来看看代码

### 10_OpenGCD: Assisting Open World Recognition with Generalized Category Discovery_arxiv 2023.08_有代码

> 作者：Fulin Gao, Weimin Zhong, Zhixing Cao, Xin Peng, Zhi Li

> 代码：https://github.com/Fulin-Gao/OpenGCD

> 贡献：

背景：OWR需要解决三个主要任务：开放集识别、广义类别发现和增量学习。现有的方法大多假设第二个任务完全由人工完成，缺乏自动化的解决方案。

为了填补这一空白，本文提出了**OpenGCD**方法，通过结合三个关键思想顺序解决了上述问题。

​	首先，通过评估分类器预测的不确定性，评估实例的来源（未知或特定已知）；

​	其次，作者首次将广义类别发现（GCD）技术引入 OWR，以帮助人们对未标记数据进行分组；

​	最后，为了顺利进行增量学习和 GCD，作者为每个类别保留了相等数量的信息丰富的示例。

一些相关工作：

**Open set recognition**：在 OSR 场景中，训练集中存在对世界的不完整知识，在测试过程中，可能碰到未知类别。这就要求在线模型不仅能对已知/见过的类别进行分类，还要能拒绝未知/未见/新颖的类别。**1-vs-all 原则**、**阈值化**和**未知概率估计**是最受欢迎的三种 OSR 策略 。1-vs-all 原则为基础的方法是最早出现的，但相对较为繁琐；基于阈值的方法具有高兼容性和低计算开销；用于估计未知概率的方法是最直观的。

**Generalized category discover**：在 GCD 场景中，未标注的测试集（可用于训练）可能包含了在训练过程中已知/已见过和未知的类别。它要求模型不仅能对已知/见过的类别进行分类，还需要对未知/未见/新颖的类别进行聚类。GCD 与 OSR 的三个主要区别在于1、是否支持在线运行，2、未标注的测试集是否可用于训练，以及3、未知类别应该被拒绝还是聚类。此外，如果测试集和训练集是类不相交的，这个问题就退化为了 NCD。

<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230921144210191.png" alt="image-20230921144210191" style="zoom: 67%;" />

> 方法：

特征嵌入：使用在未标记的 ImageNet 数据上训练的 Vision Transformer（ViT）模型作为特征提取器，获得实例的特征表示；

样本选择：使用 DS3 算法从标记的特征集中选择信息量丰富的实例，并将选定的样本存储在缓冲区中以供进一步使用；

​	**DS3** ：Dissimilarity-based sparse subset selection

分类器（重新）拟合：使用多层感知器（MLP）或 SVM/XGBoost 分类器对选定的样本进行训练，使分类器学习已知类别的现有知识；

基于不确定性的开放集识别：根据预测概率为未标记的实例分配标签，通过近似分类器预测的不确定性来获得新的概率分布，将不确定性较高的实例分配给标签0（未知），其他实例分配给已知类别；

![image-20230921152545996](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230921152545996.png)

辅助广义类别发现的手动注释：使用广义类别发现（GCD）来过滤和分组来自新类别的特征，首先使用 GCD 对被 OSR 拒绝的未标记特征进行过滤和分组，然后进行手动修正和标记，并使用 GCD 确定估计的类别数；

估计类别数：使用 ss-k-means++ 算法估计类别数，确定已知类别的质心并选择剩余的质心作为新类别，估计的类别数在 GCD 过程中使用；

<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230921151929487.png" alt="image-20230921151929487" style="zoom:80%;" />

> 总结：

* DS3样本选择算法：以多样性为目标而选择的样本从原始数据中保留了**尽可能多的空间信息**，避免了未知空间的扩展，从而**减少了开放空间的风险**，即将已知实例归类为未知实例的风险。

## 20231006

### 11_Modeling Inter-Class and Intra-Class Constraints in Novel Class Discovery_CVPR 2023_有代码

> 作者：Wenbin Li, Zhichen Fan, Jing Huo, Yang Gao

> 代码：https://github.com/FanZhichen/NCD-IIC

> 贡献：

背景：新类发现任务（NCD）的目的是学习一个模型，该模型将 common knowledge 从一个类不相交的标记数据集转移到另一个未标记的数据集，并在其中发现新的类。作者发现，现有的方法并没有充分利用 NCD 设置的本质（即忽视了标记和未标记类别之间的不相交特性）。

为此，本文提出了基于**对称 KL 散度**（sKLD）对 NCD 中的**类间**和**类内**约束进行建模。

* 类间 sKLD 约束：有效地利用标记类和非标记类之间的不相交关系，增强了嵌入空间中不同类的可分性；
* 类内 sKLD 约束：明确地约束样本与其相应增强样本之间的内部关系，并同时确保训练过程的稳定性。

结果：实验结果表明，所提出的方法大幅优于现有的最先进方法。该方法有效区分了未标记数据和标记数据，有助于发现新类别。类间 sKLD 约束在提高未标记类别的性能方面起到了关键作用。该方法防止将标记图像错误分类为未标记类别，反之亦然。

<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230708223409247.png" alt="image-20230708223409247" style="zoom:80%;" />

> 方法：

与之前的工作一致，假设未标记类 $C_{u}$ 的数量是预先已知的。

思路比较简单，对于分类头的输出，通过对称 KL 散度尽可能增大有标记数据和无标记数据的分类差异，减小有标记数据（无标记数据）的分类差异。

<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230708231548494.png" alt="image-20230708231548494" />

 类间 KL 散度约束使每个小批量中标记和无标签样本之间的距离更大：

<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230708231619879.png" alt="image-20230708231619879" style="zoom:80%;"/>

<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230708232125312.png" alt="image-20230708232125312" style="zoom:80%;" />

类内 KL 散度约束确保同一图像的不同增强之间的一致性：

<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230708232235545.png" alt="image-20230708232235545" style="zoom:80%;" />

除上述两个损失外，还有标准交叉熵（CE）损失：<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230708231920678.png" alt="image-20230708231920678" style="zoom:80%;" />

总体训练损失函数为：

<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230708231941021.png" alt="image-20230708231941021" style="zoom:80%;" />

> 总结：

对分类头输出的 logits 分布进行约束。

### 12_When Noisy Labels Meet Long Tail Dilemmas:A Representation Calibration Method_ICCV 2023_无代码

> 作者：Manyi Zhang1,* Xuyang Zhao2,∗ Jun Yao3 Chun Yuan1,† Weiran Huang4,

> 贡献：

背景: 深度学习在许多领域取得了快速进展，但实际中很难获得完美的数据集，其中一部分数据由于固有的模糊性和注释者的错误而被错误标记，同时数据也存在类别不平衡的问题。过去的方法主要针对噪声标签和长尾数据**分别进行处理**，但当这两种不完美的情况同时存在时，这些方法无法很好地工作。
本文的动机是开发更先进的方法来解决在长尾数据上学习噪声标签的实际问题。作者提出了一种称为 **RCAL** （representation calibration method）的表示校准方法来解决这个问题。

RCAL 使用无监督对比学习提取的表示，并假设每个类别中实例的表示符合多元高斯分布。从由错误标记和类别不平衡数据导致的污染的表示中恢复出潜在的表示分布；并从恢复的分布中采样额外的数据点以提高泛化能力。

> 方法：

问题背景：K （K >= 2）类别的分类问题。在本文中，目的是通过只使用不平衡和噪声标记的训练数据集来稳健地学习一个分类器。

RCAL 包含两个阶段：

1. 通过对比学习进行表征增强
2. 通过表征校准提高分类器的鲁棒性

<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231006161059755.png" alt="image-20231006161059755" style="zoom:80%;" />

**Enhancing Representations by Contrastive Learning**

从直观上看，由于自监督对比学习中的表示学习不能访问训练数据的标签，因此所获得的表示不会受到错误标签的影响。作者采用MOCO设定，对于输入 $x$，首先获得两个对应的随机增强版本 $x^q, x^k$，然后分别送入 query、key encoder中获得编码特征 $z^q, z^k$，随后使用2-layer MLP 将其投射到低维特征空间$\hat{z}^q, \hat{z}^k$，对比训练损失：

![image-20231006162000316](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231006162000316.png)

其中来自query encoder 的降维前表征  $z^q$将用于后续的表征校准。

**Distributional Representation Calibration**

在本文中，作者假设在被分类不平衡和噪声标签损坏之前，每类训练数据的深度表示都符合多元高斯分布。（已有工作证明这一点）

受到相似的类具有相似均值和协方差的启发，借用头部类的统计数据来帮助尾部类的校准。具体地说，通过计算不同类的表示方法之间的欧氏距离来度量相似性，对于尾部类别，选择与其最相似的topk个头部类别的统计数据对该尾部类的统计数据进行校准；

<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231006164100633.png" alt="image-20231006164100633" style="zoom:80%;" /><img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231006164111993.png" alt="image-20231006164111993" style="zoom:80%;" />

**Individual Representation Calibration**

为了进一步提高表示的鲁棒性，作者进行了个体表示校准，包含两部：

* 考虑到自监督对比学习提供了鲁棒表示，作者严格规定后续学习表示与对比学习带来的表示之间的距离：

![image-20231006164311173](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231006164311173.png)

* 为了进一步使学习到的表示具有鲁棒性来处理长尾情况下的噪声标签，采用了混合方法：

![image-20231006164355740](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231006164355740.png)

![image-20231006164406107](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231006164406107.png)![image-20231006164417636](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231006164417636.png)是交叉熵损失

## 20231013

### 13_Detect Every Thing with Few Examples_arxiv 202309_有代码

> 作者：Xinyu Zhang, Yuting Wang, Abdeslam Boularias

> 代码：[mlzxy/devit (github.com)](https://github.com/mlzxy/devit)

> 贡献：

背景: 目标识别和定位是计算机视觉中的核心任务之一。本文关注的是**开放集物体检测**，即检测训练过程中未见过的任意类别的物体。
过去的研究主要采用开放词汇的范式，利用视觉-语言骨干来表示类别。然而，使用语言作为类别表示存在一些局限性，如某些物体难以准确描述或缺乏简洁的名称，以及视觉概念与语言之间的关联是不断演变的。此外，基于语言的分类方法没有充分利用图像注释信息。

本文提出了一种名为 ==DE-ViT== 的开放集物体检测器，**它使用示例图像而不是语言来学习新的物体类别**。与过去的少样本方法相比，DE-ViT 不需要在新类别上进行进一步的训练，从而避免了复杂和繁琐的过程。此外，DE-ViT 在实践中的准确性也超过了开放词汇的解决方案，特别是在大规模数据集（如COCO和LVIS）上。

文中指出*open-vocabulary object detection* 设定存在以下限制：

1. 某些物体很难仅用语言来准确描述，或者缺乏简洁的定义；
2. 视觉概念和语言之间的联系是不断发展的，而不是静态的，而开放词汇模型只涉及到语料库中以前已连接的对象和名称；
3. 基于语言的分类在图像注释可用时不会利用这些注释。

> 方法：

总体架构与流程如下图：DE-ViT 使用仅基于视觉的 DINOv2 骨干网络，并构建每个类别的原型向量。

两个技术点：

* 将多分类任务转化为二分类任务
* 提出了一种新颖的区域传播技术用于定位

![image-20231007101959431](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231007101959431.png)

Similarity map **s** 使用proposal features 和 prototypes计算（D是特征维度）：

<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231007103310552.png" alt="image-20231007103310552" style="zoom:80%;" />

ViT backbones, prototypes, 以及 the similarity maps 在检测器训练阶段保持冻结状态。

 **CLASSIFICATION WITH AN UNKNOWN NUMBER OF CLASSES**

![image-20231007105211388](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231007105211388.png)

average 的特征仅用来选择 topk 个类，分类使用的还是下面分支的特征。

**LOCALIZATION WITH REGION PROPAGATION**

![image-20231007114500369](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231007114500369.png)

直觉上，只有与对象重叠的 proposal 才是重要的，因为其他的 proposal 会被分类网络拒绝。如果我们扩大这种 proposal ，它们可能会覆盖底层对象的整个区域。因此，原始 proposal  可以通过预测扩展 proposal 内的目标区域来细化。

> 总结：

* 通过对 proposal box 进行扩张（或缩小）的方式来提高该 proposal 分类正确的概率。

### 14_Unsupervised Recognition of Unknown Objects for Open-World Object Detection_arxiv 202308_有代码

> 作者：Ruohuan Fang, Guansong Pang, Lei Zhou, Xiao Bai, Jin Zheng

> 代码：[frh23333/mepu-owod: Code Implementation of "Unsupervised Recognition of Unknown Objects for Open-World Object Detection" (github.com)](https://github.com/frh23333/mepu-owod)

> 贡献：

背景：文中指出根据 objectness score 选择伪未知类候选框的缺点：这种方法可以成功地检测出与已知对象具有相似特征的未知对象。然而，它们对已知类别存在严重的==标签偏差==问题，也就是说，它们倾向于将所有与已知对象不同的区域（包括未知对象区域）检测为背景。

【我自己的想法是，根据 objectness score 选择伪未知类候选框的缺陷在于，在选择框的时候，很有可能误选到已知类对象（见如下示意图，数字表示 objectness score ）。】

为了解决标签偏差问题，本文提出了一种名为 **MEPU**（**m**odel and **e**xtend **p**seudo **u**nknown） 的方法，将未知对象识别问题分解为两个子问题：通过无监督区域生成器生成的原始伪标签进行无监督建模，以及扩展未在这些伪标签中涵盖的新未知对象。通过引入重建误差基于 Weibull 模型的模块（REW），利用特征频率建模并识别未知对象，MEPU 方法有效地解决了标签偏差问题。此外，通过将 REW 产生的软标签与无分类器的自训练相结合，进一步扩展了伪标签的覆盖范围，从而减轻了标签偏差问题的影响。

> 方法：

MEPU方法由两部分组成：基于重构误差的 Weibull 模型（**REW**）和增强的目标定位网络（**ROLNet**）

REW 模块旨在对由无监督区域 proposal 生成器生成的伪标签中隐藏的未知对象进行建模。它使用无监督方法从常见背景区域中识别各种类型的未知对象，并为所有伪标签分配软标签，以估计其作为真实未知对象的可能性。
ROLNet 模块利用 REW 得分和无分类头的目标定位网络（OLN）进行自训练过程。它通过使用 REW 得分和 OLN 来识别和定位未标记区域中的新未知对象，有效地扩展未知对象集合。

![image-20231008102618234](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231008102618234.png)

***Reconstruction Error-based Weibull Modeling of Foreground and Background Regions***

![image-20231008105316981](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231008105316981.png)

背景区域往往具有频繁出现的特征，与不同前景对象区域的罕见特征相比，使其更好地重建，导致重建误差更小。如下图，很明显，已知目标区域的重建误差通常比背景区域的重建误差要大得多。虽然未知物体可能与已知物体有不同的外观，但我们可以假设它们的低出现频率和高重建误差相似（因为存在各种类型的未知物体）。因此，**本文利用采样的已知对象的重建误差来估计所有前景区域的分布**。

![image-20231008105454140](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231008105454140.png)

给定图像 I 种的伪未知对象框，通过求平均的方式计算其重建误差，然后通过下式评估其是 unknown 对象的分数，作为 soft label。

![image-20231008105939309](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231008105939309.png)

然后，该 soft label 会被用于计算加权分类损失：

![image-20231008110205235](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231008110205235.png)

![image-20231008110215881](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231008110215881.png)

**ROLNet: REW-enhanced Object Localization Network for Extending the Set of Unknown Objects**

![image-20231008110746462](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231008110746462.png)

> 总结：

* 背景大差不差所以重构损失小，前景各有不同所以重构差异大；以此来选择合适的未知类（前景）伪候选框。

# 实验

* idea：结合文本和随机
  * 例如，先随机选择 5/10 个候选框，然后再根据选出的候选框与 caption 中名词的相似性来筛选（选择依据，topk 或相似性阈值）最终的 unknown 候选框；
    * 没什么效果，和仅随机相比，效果没有明显提升。
* 若仅使用文本相似性选择伪标签框，unknown recall 较高（24.93），但是对已知类 mAP 影响较大
  * 分析：可能是由于 clip text encoder 编码得到的类别文本特征之间的相似度本来就很高（80个类别各自之间，大部分相似度都在0.8，0.9的数值上），所以导致仅根据相似性选伪标签的话，会以较大的概率误选到包含已知类对象的框，从而造成对已知类学习产生很大偏差。
    * 或许可以试试，先以较小的相似性阈值初步选择候选框，然后再随机选择，最后再加一步高阈值筛选的操作。

|        | t1     | t1    | t1    | t1    |
| ------ | ------ | ----- | ----- | ----- |
|        | WI     | AOSE  | C-m   | UR    |
| randun | 0.0582 | 4,375 | 60.10 | 23.09 |
| r5c0.8 | 0.0625 | 4770  | 59.73 | 23.28 |
| c0.8   | 0.0462 | 1723  | 55.11 | 24.93 |

r5c0.8：先随机选 5 个，然后根据相似性阈值筛选（ > 0.8）

c0.8：仅根据相似性阈值（ > 0.8）选择 unknown 框

* 消融实验思路
  * 多模态信息（caption 文本）引导的未知类伪标签选择（使用相似性阈值+topk_cap）（存在的问题：由于CLIP的训练物料范围极大，导致各类别特征值区分度不够，因此对已知类检测性能影响较大）
  * 在此基础上，结合使用随机选择的方式来选择未知类伪标签，以达到对已知类去偏的效果。（知识再选择）

## 与现有方法对比

（仅包含已被接收的文章202308）

|                                                              |            | Task 1 |        |       |       | Task 2 |        |       |       |       |       | Task 3 |        |       |       |       |       | Task 4 |       |       |
| ------------------------------------------------------------ | ---------- | ------ | ------ | ----- | ----- | ------ | ------ | ----- | ----- | ----- | ----- | ------ | ------ | ----- | ----- | ----- | ----- | ------ | ----- | ----- |
|                                                              |            | WI     | AOSE   | C-m   | UR    | WI     | AOSE   | m-AP  |       |       | UR    | WI     | AOSE   | m-AP  |       |       | UR    | m-AP   |       |       |
|                                                              |            | P-m    | C-m    | B     | P-m   | C-m    | B      | P-m   | C-m   | B     |       |        |        | P-m   | C-m   | B     |       | P-m    | C-m   | B     |
| Faster-RCNN + Finetuning                                     |            | 0.0699 | 13,396 | 56.40 |       | 0.0371 | 12,291 | 51.00 | 25.00 | 38.00 |       | 0.0213 | 9,174  | 38.20 | 13.60 | 30.00 |       | 29.70  | 13.00 | 25.60 |
| DDETR + Finetuning                                           |            | 0.0608 | 33,270 | 60.30 |       | 0.0368 | 18,115 | 54.50 | 34.40 | 44.80 |       | 0.0197 | 9,392  | 40.00 | 17.80 | 33.30 |       | 32.50  | 20.00 | 29.40 |
| 【objectness】根据未匹配的 proposal 最有可能是前景对象的可能性得分 |            |        |        |       |       |        |        |       |       |       |       |        |        |       |       |       |       |        |       |       |
| ORE − EBUI                                                   | 2021，CVPR | 0.0621 | 10,459 | 56.00 | 4.9   | 0.0282 | 10,445 | 52.70 | 26.00 | 39.40 | 2.9   | 0.0211 | 7,990  | 38.20 | 12.70 | 29.70 | 3.9   | 29.60  | 12.40 | 25.30 |
| OW-DETR                                                      | 2022，CVPR | 0.0571 | 10,240 | 59.20 | 7.5   | 0.0278 | 8,441  | 53.60 | 33.50 | 42.90 | 6.2   | 0.0156 | 6,803  | 38.30 | 15.80 | 30.80 | 5.7   | 31.40  | 17.10 | 27.80 |
| SA                                                           | 2022，ICLR | 0.0417 | 4,889  | 56.20 | -     | 0.0213 | 2,546  | 53.39 | 26.49 | 39.94 | -     | 0.0146 | 2,120  | 38.04 | 12.81 | 29.63 | -     | 30.11  | 13.31 | 25.91 |
| UC-OWOD                                                      | 2022，ECCV | 0.0136 | 9,294  | 50.66 | 2.4   | 0.0117 | 5,602  | 33.13 | 30.54 | 31.84 | 3.4   | 0.0073 | 3,801  | 28.80 | 16.34 | 24.65 | 8.7   | 25.57  | 15.88 | 23.14 |
| RandBox                                                      | 2023，ICCV | 0.0240 | 4,498  | 61.80 | 10.6  | 0.0078 | 1,880  | -     | -     | 45.30 | 6.3   | 0.0054 | 1,452  | -     | -     | 39.40 | 7.8   | -      | -     | 35.40 |
| 使用额外的 proposal 生成技术例如 selected search             |            |        |        |       |       |        |        |       |       |       |       |        |        |       |       |       |       |        |       |       |
| CAT                                                          | 2023，CVPR | 0.0581 | 7,070  | 59.90 | 21.8  | 0.0263 | 5,902  | 54.00 | 33.60 | 43.80 | 18.6  | 0.0177 | 5,189  | 42.10 | 19.80 | 34.70 | 23.9  | 35.10  | 17.10 | 30.60 |
| 不使用伪标签                                                 |            |        |        |       |       |        |        |       |       |       |       |        |        |       |       |       |       |        |       |       |
| OCPL                                                         | 2022，ICIP | 0.0423 | 5,670  | 56.64 | 8.3   | 0.0220 | 5,690  | 50.65 | 27.54 | 39.10 | 7.7   | 0.0162 | 5,166  | 38.63 | 14.74 | 30.67 | 11.9  | 30.75  | 14.42 | 26.67 |
| 2B-OCD                                                       | 2022，HCMA | 0.0481 | -      | 56.37 | 12.1  | 0.0160 | -      | 51.57 | 25.34 | 38.46 | 9.4   | 0.0137 | -      | 37.24 | 13.23 | 29.24 | 11.7  | 30.06  | 13.28 | 25.82 |
| PROB                                                         | 2023，CVPR | 0.0569 | 5,195  | 59.50 | 19.4  | 0.0344 | 6,452  | 55.70 | 32.20 | 44.00 | 17.4  | 0.0151 | 2,641  | 43.00 | 22.20 | 36.00 | 19.6  | 35.70  | 18.90 | 31.50 |
| Annealling-RCNN                                              | 2023，CVPR | 0.0604 | 8,332  | 56.67 | 12.8  | 0.0269 | 9,454  | 51.96 | 29.13 | 40.55 | 5.0   | 0.0157 | 6,635  | 40.82 | 14.56 | 32.07 | 9.8   | 31.68  | 13.09 | 27.03 |
| Annealling-DETR                                              | 2023，CVPR | 0.0564 | 46,589 | 59.34 | 13.6  | 0.0274 | 24,709 | 53.18 | 37.98 | 45.58 | 10.0  | 0.0194 | 14,952 | 43.62 | 26.66 | 37.97 | 14.3  | 33.54  | 21.76 | 30.60 |
|                                                              |            |        |        |       |       |        |        |       |       |       |       |        |        |       |       |       |       |        |       |       |
| RandUn-owod                                                  | ours       | 0.0582 | 4,375  | 60.10 | 23.1  | 0.0242 | 2,329  | 51.66 | 34.06 | 42.86 | 18.5  | 0.0156 | 1,886  | 40.55 | 23.13 | 34.75 | 21.8  | 34.61  | 19.40 | 30.80 |
| RandUn-owdetr                                                |            | 0.0416 | 2373   | 64.13 | 28.34 | 0.0240 | 1423   | 53.87 | 47.46 | 50.66 | 28.33 | 0.0175 | 1333   | 49.39 | 43.06 | 47.28 | 33.66 | 47.01  | 45.43 | 46.61 |

指标分析：

* 相比较于使用 objectness 伪标签的方法，未知类召回率（23.1 ）大大提升，是现有 SOTA RandBox（10.6 ）的2.18 倍；
* 已知类的性能也直逼 SOTA 方法的性能，甚至超过了很多基于 DETR 的方法

模型优势：

* 简单 + 高效

## 方法尝试

### 文本和随机方法结合对比

|                            | Task 1 |       |       |       |
| -------------------------- | ------ | ----- | ----- | ----- |
|                            | WI     | AOSE  | C-m   | UR    |
| 仅使用文本相似性阈值       |        |       |       |       |
| c0.6                       | 0.0446 | 1,678 | 55.43 | 24.94 |
| c0.7                       | 0.0461 | 1,677 | 55.07 | 24.96 |
| c0.8                       | 0.0481 | 1,777 | 55.50 | 24.96 |
| c0.9                       | 0.0558 | 2,373 | 56.23 | 24.38 |
| 文本相似性阈值+随机选择5个 |        |       |       |       |
| c0.5 + rand5               | 0.0637 | 4,991 | 60.02 | 22.94 |
| c0.6 + rand5               | 0.0627 | 4,975 | 60.30 | 22.71 |
| c0.7 + rand5               | 0.0601 | 4,718 | 59.88 | 23.40 |
| c0.8 + rand5               | 0.0604 | 4,611 | 59.93 | 22.86 |
| c0.9 + rand5               | 0.0605 | 4,749 | 60.24 | 22.81 |

文本融合方法：首先将图片的候选框特征投射到语义空间（具体：将与 GT 匹配的候选框特征经过投射后与相应的标签文本特征做对齐），然后根据图像 caption 种的非已知类标签名词所对应的文本特征信息从剩下的候选框种选择未知类伪候选框。

分析：

* 若仅使用文本相似性阈值挑选未知类伪候选框，很可能因为非已知类标签名词所对应的文本特征 与 很多未能成功与 gt 匹配的已知类候选框相似性较高从而被误选则为未知类伪候选框，因此和 objectness 方法一样，仍然对已知类存在偏差，从而严重影响模型对已知类对象的识别能力；
* 通过在文本相似性阈值的基础上（具体：先根据相似性选出 m 个 sim 大于阈值的框，然后随机选择min（m， 5）个框作为最终的未知类伪候选框），加入随机性挑选部分候选框作为未知类伪候选框的方法能够很好的缓解对已知类的偏差。因为随机选择的方式能够在一定程度上减少误选已知类候选框作为未知类伪候选框的错误概率，因此能够达到去除偏差的目的。

### objectness-top5 vs ours-5

|                                    | Task 1 |        |       |       | Task 2 |        |       |       |       |       | Task 3 |        |       |       |       |       | Task 4 |       |       |
| ---------------------------------- | ------ | ------ | ----- | ----- | ------ | ------ | ----- | ----- | ----- | ----- | ------ | ------ | ----- | ----- | ----- | ----- | ------ | ----- | ----- |
|                                    | WI     | AOSE   | C-m   | UR    | WI     | AOSE   | P-m   | C-m   | B     | UR    | WI     | AOSE   | P-m   | C-m   | B     | UR    | P-m    | C-m   | B     |
| baseline-obj1（无nms）             | 0.0761 | 77,927 | 56.42 | 9.99  | 0.0428 | 39,682 | 45.94 | 30.03 | 37.99 | 6.94  | 0.0214 | 19,824 | 35.21 | 20.64 | 30.35 | 8.51  | 29.41  | 16.13 | 26.09 |
| baseline-obj1（有nms）             | 0.0654 | 33,240 | 57.70 | 9.69  | 0.0343 | 20,066 | 47.47 | 30.65 | 39.06 | 6.75  | 0.0189 | 11,525 | 36.47 | 21.16 | 31.37 | 8.31  | 30.61  | 16.52 | 27.09 |
| baseline-obj5（无nms）             | 0.0755 | 72,358 | 55.87 | 13.70 | 0.0437 | 38,361 | 43.72 | 29.29 | 36.51 | 10.28 | 0.0210 | 18,088 | 32.93 | 18.86 | 28.24 | 11.73 | 28.09  | 15.51 | 24.94 |
| c0.5 + rand5 + c0.8                | 0.0627 | 5,030  | 60.17 | 23.13 | 0.0239 | 2,502  | 52.19 | 34.06 | 43.13 | 19.62 | 0.0151 | 1,917  | 41.11 | 23.62 | 35.28 | 21.83 | 35.17  | 20.05 | 31.39 |
| c0.5 + rand5 + c0.8 + smallmin0.01 | 0.0610 | 4,854  | 60.10 | 23.31 | 0.0239 | 2,448  | 51.80 | 34.48 | 43.14 | 19.03 | 0.0152 | 1,971  | 40.75 | 23.17 | 34.89 | 22.06 | 34.99  | 19.46 | 31.11 |

unknown选择对比可视化：

​                obj-5                                  ours-5                                    unknown-gt

<img src="C:\Users\wangxuefei\Desktop\psd_boxes\obj5\000000027451.jpg" alt="000000027451" style="zoom:30%;" /><img src="C:\Users\wangxuefei\Desktop\psd_boxes\ours\000000027451.jpg" alt="000000027451" style="zoom:30%;" /><img src="C:\Users\wangxuefei\Desktop\psd_boxes\gt\000000027451.jpg" alt="000000027451" style="zoom:30%;" />

<img src="C:\Users\wangxuefei\Desktop\psd_boxes\obj5\000000381610.jpg" alt="000000381610" style="zoom:30%;" /><img src="C:\Users\wangxuefei\Desktop\psd_boxes\ours\000000381610.jpg" alt="000000381610" style="zoom:30%;" /><img src="C:\Users\wangxuefei\Desktop\psd_boxes\gt\000000381610.jpg" alt="000000381610" style="zoom:30%;" />

<img src="C:\Users\wangxuefei\Desktop\psd_boxes\obj5\000000571764.jpg" alt="000000571764" style="zoom:30%;" /><img src="C:\Users\wangxuefei\Desktop\psd_boxes\ours\000000571764.jpg" alt="000000571764" style="zoom:30%;" /><img src="C:\Users\wangxuefei\Desktop\psd_boxes\gt\000000571764.jpg" alt="000000571764" style="zoom:30%;" />

分析：基于 objectness 选的 unknown 框的重复性很高（缺点），但是框的面积一般都比较大（优点）

​			本文的方法，选出来的 unknown 伪框重复性较低（框之间的iou小）（优点），但框的面积有很多都偏小（缺点）

后续可以考虑在选择 unknown 伪框的时候，去掉 area 很小的框（一般 area 很小的框都不太可能包含对象）

### **方法 c0.5 + rand5 + c0.8 + smallmin0.01  **

在选择伪未知类框的时候，不考虑 area 小于当前图片中最小已知类对象面积*0.01 的框

结果：有些指标是会好点（也不是很明显），但不是一致偏好。

### 使用前序任务训练得到的原型指导后序任务

做法：在 t2 训练过程中，选择伪标签的时候，首先将 pred_embed 与 t1 训练后得到的前20个类别的原型计算相似度，把相似度高的框去掉，以减少误选之前已知类的概率。

结果：不理想，反向优化。

分析：可能是使用原型的方法，无差别的拉大不同类类原型之间的距离并不合理。（eg：猫原型的距离和狗原型的距离 d1 应当比猫和足球的距离d2 要小，即合理的情况下应该 d1 < d2）

### **下一步计划：**

首先考虑一下去掉原型约束，而是直接将模型最后一个训练 epoch 的时候提取的特征保存下来，以平均值作为类中心。

然后，先试试上面的指导未知类选择的方法，看看是否有效；

另外，还可以在训练 t2 的时候，对于那些与前序任务类中心相近的（以相似性或 l2 距离衡量）样例，减小其所对应损失计算的权重（类似于权重正则化的机制）。
