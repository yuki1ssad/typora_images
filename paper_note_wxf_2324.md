笔记

## 20230901

### 1_Cross-stage neural pattern similarity in the hippocampus predicts false memory derived from post-event inaccurate information_Nature Communications 2023_认知神经科学

> 作者：Xuhao Shao 1,2,3,4, Ao Li1, Chuansheng Chen5, Elizabeth F. Loftus5 &Bi Zhu

> 贡献：

该研究发现，人脑海马体在记忆测试阶段同时存在原始事件和虚假信息的记忆痕迹。而且，每个人的大脑海马体对虚假信息的记忆表征只能预测自己的错误记忆行为。此外，当人脑海马体的虚假信息记忆痕迹较强而原始事件记忆痕迹较弱时，**这种记忆信号差异能够引发大脑外侧前额叶的监控**。

该研究系统阐明了虚假信息引发人类错误记忆的脑机制，支持并扩展了学习记忆的**多重痕迹理论**和**激活监控理论**，对了解人脑记忆重构本质有重要科学意义，对教育和法律有重要应用价值。

> 主要内容：

**虚假信息效应**（misinformation effect）是指原始事件记忆因事后虚假信息而改变。它包括三阶段：首先个体看到某个事件，然后接受事后虚假信息，最后完成针对原始事件的记忆测验(图1)。

人脑海马体似乎与错误信息效应有关键关系，然而海马体的记忆表征在目击原始事件、接受虚假信息、记忆测验这**三个阶段**是如何变化的还不清晰。

人们提出了三个理论观点来解释错误信息效应： non-retention, trace-alteration, and multiple-trace models

![image-20230824105309582](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230824105309582.png)

* 不保留（a）

  原始事件的表征不会形成或者是在接收错误信息之前就会丢失；

* 路径更改（b）

  原始事件的海马体表征在 post-event 阶段被保留，但随后被错误信息覆盖，因此不会延续到记忆测试阶段；

* 多路径（c）

  原始事件的海马表征在三个阶段保持完整，它们在记忆测试阶段与错误信息的表征**竞争**。与其他两个观点不同的是，多路径模型在记忆测试阶段预测时，原始事件的海马体表征的存在，即使错误信息可以有效地产生错误记忆。

  这里涉及的一个关键机制是**认知控制**，由前额叶皮层的某些部分提供的服务。在记忆测试中，解决海马体中两个记忆痕迹之间的竞争，腹外侧、内侧和背外侧前额叶皮层可能参与选择目标相关的记忆原始信息，抑制不适当的记忆错误信息，并监控错误信息和原始信息之间的差异。

  然而，在记忆测试中，**前额叶皮层的哪些部分可能与海马体协同工作来解决这种冲突尚不清楚**。

因此，该研究采用功能磁共振脑成像技术和基于模型的多体素模式分析方法，**计算了人脑海马体在跨记忆阶段的神经模式相似性，揭示了人脑海马体中真假信息记忆表征互相竞争而产生错误记忆的神经机制**。

首先，进行了一项行为研究，确保为神经成像研究修改的错误信息范式能够按照预期进行工作（即错误信息组比中性和一致的组导致了更多的错误记忆）。

然后，实验二通过比较记忆反应类型（即真实记忆（true）、错误记忆（false）、干扰选项（foil）、正确控制（correct）和错误控制（incorrect））之间的行为和神经指标来检验错误信息效应。利用 fMRI 数据的表征相似性分析，比较了三个阶段的海马模式相似性，并检验了上述三种理论的预测。我们的研究结果显示，**海马体的跨阶段神经模式相似性预测了错误信息效应。**

> 结果：

**Behavioral results**

在第一个实验中，不同组别的参与者在记忆测试中的表现存在差异。统计分析显示，组别（错误信息组、中性信息组和一致信息组）与记忆类型（真实、虚假、干扰、正确和错误）之间存在显著交互作用。

预期结果是，错误信息组的真实记忆（即原始信息的认可率）低于中性信息组，而中性信息组又低于一致信息组。相反，错误信息组的虚假记忆（即对虚假信息的认可率）高于中性信息组，而中性信息组又高于一致信息组。

![image-20230824160943556](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230824160943556.png)

这种错误信息效应在两个实验中得到了进一步的证实。在第一个实验的错误信息组中，虚假记忆的比例（41%）高于干扰信息的认可率（9%）。在第二个实验（fMRI 研究）中，错误信息组的虚假记忆（42%）也高于干扰信息的认可率（14%)。

**Hippocampal representations of true, false, correct, and incorrect memories**

结果显示，在某些情况下，四种记忆类型的海马区模式相似性在对应项目上高于不对应项目，这表明存在项目特异性表示。这种情况在不同的阶段对于不同的记忆类型有所不同。

对于真实记忆，海马区模式相似性在原始事件和后续事件之间（OP）中，对应项目高于不对应项目，但在其他阶段（OM、PM）没有这种情况。
对于虚假记忆，项目特异性的效应明显，但是阶段的效应以及阶段与项目特异性之间的交互作用并不明显。
对于正确对照和错误对照，项目特异性的效应也是明显的，但阶段的效应和交互作用并不明显。
在两个阶段的两两组合中，只有在后续事件和记忆测试之间（PM）中，虚假记忆和正确对照的项目特异性的效应显著，而真实记忆和错误对照则不显著。此外，在这个阶段，虚假记忆和正确对照的物品特异性效应也大于真实记忆。

![image-20230824154617384](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230824154617384.png)

真实记忆在某些情况下与项目特异性相关，而虚假记忆和正确对照则在后续事件和记忆测试之间具有更强的项目特异性表示。这些结果与多重痕迹理论相吻合。

**Hippocampal representation of post-event information predicts false memory**

通过对虚假记忆和海马区后续事件信息表示之间的个体内和个体间相关性的比较，研究发现每个个体在海马区对后续事件信息的表示上有部分独特性，而这种表示与虚假记忆的行为模式相关联。然而，这种映射关系在其他情况下不明显，如海马区对原始信息的表示以及正确对照的行为模式。

**Prefrontal activity correlates with hippocampal representations when false memory occurs**

通过全脑探索性分析，并在记忆测试期间的海马区活动水平控制下，我们发现了两个集群，位于左侧外侧前额叶皮层和右侧外侧前额叶皮层。这些集群的活动与虚假记忆中 PM 中的海马区项目特异性表示正相关程度高于 OM，且虚假记忆中该效应比正确对照组更加明显。

**Cortical representations and their connectivity with hippocampus**

当前研究主要关注虚假记忆效应下的海马区表示。然而，皮层表示也可能在这个过程中发挥作用。因此，研究探索了在 OM 和 PM 的项目特异性表示方面，真实记忆和虚假记忆之间是否存在其他大脑区域的差异。

多个大脑区域（例如左角回）在真实记忆的OM中显示出比虚假记忆更大的项目特异性表示，但只有后扣带回在虚假记忆的PM中显示出比真实记忆更大的项目特异性表示（图6a）；

外侧顶叶皮层（例如左角回）显示出真实记忆的个体特异性神经行为相关，而内侧顶叶皮层（例如后扣带回）显示出虚假记忆的个体特异性神经行为相关（图6b）；

基于以海马区为种子的全脑分析，海马区与一些皮层区域之间的相关系数（例如左角回）在OM中在真实记忆中比虚假记忆更高，但海马区与枕上皮层（延伸至后扣带回）之间在PM中的相关系数在虚假记忆中比真实记忆更高（图6c）。

![image-20230824160807288](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230824160807288.png)

### 2_CLIPN for Zero-Shot OOD Detection: Teaching CLIP to Say No_ICCV 2023_有代码

> 作者：Hualiang Wang, Yi Li, Huifeng Yao, Xiaomeng Li*

> 代码：https://github.com/xmed-lab/CLIPN

> 贡献：

背景：现有 OOD 方法基本上是在卷积网络或 Transformer 上做的改进，对于难 OOD 样本的性能较差；另外使用 CLIP 来做 OOD 检测的方法还没怎么被探索。

![image-20230825194027987](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230825194027987.png)

本文致力于探索 CLIP 中的 *open-world* knowledge，从而使模型能够检测 ID-ness 较高的难 OOD 样本。

由于 CLIP 模型不具备 ”no“ 的逻辑（如下图），因此本文主要的贡献在于教会 CLIP 说 ”no“，即提出模型 **CLIPN**。为了让 CLIP 具备通过阳性语提示和否定语义提示来区分 OOD 和 ID 样本的能力，作者设计了可学习的 “no” 提示和 “no” 文本编码器，以捕获图像中的否定语义。另外还引入了两个损失函数：图像文本二元对立损失和文本语义对立损失，教 CLIPN 将图像与 “no” 提示关联起来，从而使其能够识别未知样本。然后作者提出了两种无阈值的推理算法，利用来自 “no” 提示和文本编码器的否定语义来执行 OOD 检测。

![image-20230825194737288](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230825194737288.png)

> 方法：

Image Encoder 和 Text Encoder 来自 CLIP，训练时都冻结参数；”no“ text encoder 用 Text Encoder 初始化，训练时更新。可学习的 ”no“ 提示采用 CoOp 那种半可学习的形式（由可学习参数+类名构成）。

![image-20230825195417103](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230825195417103.png)

**训练损失的设计：**

* **Image-Text Binary-Opposite Loss (ITBO)**

  在特征空间中通过 match-ness 让图片和对应的 ”no“ text 对应上：

  ![image-20230825200248240](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230825200248240.png)，![image-20230825200303728](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230825200303728.png)

* **Text Semantic-Opposite Loss (TSO)**

  在特征空间中，与同一张图片对应的正向语义和负向语义应当远离彼此：

  ![image-20230825200418176](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230825200418176.png)，g 是 ”no“ text encoder 的输出；

![image-20230825200053287](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230825200053287.png)

**Inference algorithm of CLIPN**

对于 ID 类别，通过![image-20230825200856265](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230825200856265.png) 计算对应 ID 中 C 个类别的概率；

对于 OOD 类别，作者提出两个  threshold-free 的算法来检测：

* **Competing-to-win Algorithm（CTW）**

  $p^{no}$ 由公式4计算得到，然后根据 $p^{no}$ 和 $p^{yes}(即1-p^{no})$ 的大小来判断是否为 OOD，如果 $p^{no} > p^{yes}$，则检测为 OOD；![image-20230825201332103](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230825201332103.png)

* **Agreeing-to-differ Algorithm（ATD）**

  如果 ID 的每个类别分数都差不多的话，CTW 方法则显得有些莽撞（因为根据 max p_ik 选出的类别不具有说服力了），因此作者提出类似投票表决的方法来检测 OOD，首先根据所有 ID 类的分类概率和 match-ness 添加一个 unknown 分类概率：![image-20230825204750112](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230825204750112.png)，然后 OOD 的判别如下：![image-20230825204915719](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230825204915719.png)

  即根据公式 8 计算得出的 unknown 类概率如果大于所有 ID 类概率，则将该图片检测为 OOD。

> 总结：

1. 本文在 CLIP 的训练数据中，加入了 ”no“ text prompt 进行联合训练，让 CLIPN 具备识别 ”no“ 的能力；
2. 本文中预训练好的 ”no“ text encoder 可能可以用用；
3. ATD 这种类似于投票表决的方法挺好（先用所有已知类的分类分数结合一个类似于 ID-ness 的分数进行加权平均得到 ”已知类分数“，然后用 1 减它表示未知类分数）。

## 20230908

### 3_IOMatch: Simplifying Open-Set Semi-Supervised Learning with Joint Inliers and Outliers Utilization_ICCV 2023_有代码

> 作者：Zekun Li, Lei Qi, Yinghuan Shi*, Yang Gao

> 代码：https://github.com/nukezil/IOMatch

> 贡献：

背景：开放集半监督任务的当前主流方法倾向于首先检测异常值，然后将其过滤掉。作者发现，当标签极其稀缺时，这种方法可能会导致更严重的性能下降，因为不可靠的异常值检测器可能会错误地排除相当一部分有价值的内部值。

为了解决这个问题，作者提出 IOMatch，联合利用内部值和异常值。使用多二元分类器与标准闭集分类器相结合来产生统一的开放集分类目标，将所有异常值作为一个新类；通过采用这些目标作为开放集伪标签，使用所有未标记样本（包括内部值和异常值）优化开集分类器。

> 方法：

总体流程：

**对于 labeled batch**，首先进行弱增强，然后编码得到特征 $h_{i}$，闭集分类器作用于 $h_{i}$ 后的到闭集分类概率 $p_{i}$，使用标准交叉熵损失进行优化![image-20230905180450041](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230905180450041.png)，使用一个投射变换头将 $h_{i}$ 降维得到 $z_{i}$， $z_{i}$ 送入多二元分类器获得 inliers or outliers 的分数 $o_{i}$；

**对于unlabeled batch** ，分别进行弱增强和强增强，并获得相应的 $h,p,z,o$。另外开集分类器会预测 unlabeled samples 的开集概率分布，其中所有的 outliers 会被视为一个单一的 new class。

![image-20230905175434803](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230905175434803.png)

在每次迭代时，使用闭集分类器和多二元分类器来产生开集 targets，开集 target 生成方式如 Figure 2 所示。

<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230905175726155.png" alt="image-20230905175726155"  />

 **Unified Open-Set Targets Production**

对于 unlabeled sample，结合闭集分类结果和多二元分类结果 ![image-20230905181620364](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230905181620364.png)来衡量其属于某个可见类的概率；那么某个 unlabeled sample 是 outlier 的概率为![image-20230905181716115](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230905181716115.png)；

合在一起为：![image-20230905181802043](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230905181802043.png)，这将作为所有 unlabeled samples 的开集 targets。

**Joint Inliers and Outliers Utilization**

对于 **Outliers**：![image-20230905182336038](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230905182336038.png)，s 代表强增强版本；

对于 **Inliers**：![image-20230905182425843](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230905182425843.png)，其中 F 表示使用的双层过滤策略，![image-20230905182510138](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230905182510138.png)

总体的算法流程如下：

![image-20230905121210853](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230905121210853.png)



> 总结：

1. 对弱增强版本的无标签数据，使用闭集分类器结合多二元分类器的方式为对应的强增强版本样例产生伪标签
2. **闭集分类器结合二元分类器的使用确定某个样例属于已知类的概率**

### 4_Dual Compensation Residual Networks for Class Imbalanced Learning_TPAMI 2023_无代码

> 作者：Ruibing Hou, Hong Chang, Bingpeng Ma, Shiguang Shan, Xilin Chen

> 贡献：

背景：对于类不平衡的数据，学习可推广的表征和分类器对于数据驱动的深度模型来说是一个挑战。大多数研究试图重新平衡数据分布，这容易导致模型对尾部类别过拟合并对头部类别欠拟合。

因此本文提出双补偿残差网络（ DCRNets，Dual Compensation Residual Networks），以更好地适应尾部和头部类。首先，作者提出**双特征补偿模块（FCM）**和 **Logit补偿模块（LCM）**来缓解过拟合的问题。这两个模块的设计是基于观察的结果：导致过拟合的一个重要因素是在尾类的训练数据和测试数据之间存在严重的特征漂移，即尾部类别的测试特征倾向于向多个相似头部类别的特征漂移。因此，FCM 估计每个尾部类别的多模式特征漂移方向，并对其进行补偿。此外，LCM 将 FCM估计的确定性特征漂移向量沿类内变化进行转换，从而覆盖更大的有效补偿空间，从而更好地拟合测试特征。然后，作者还提出了一种残余平衡多代理分类器（**RBMC**）来缓解欠拟合的问题。由于观察到再平衡策略阻碍了分类器学习足够的头部知识并最终导致不拟合，RBMC 利用具有残差路径的统一学习来促进分类器学习。

> 方法：

如下图，输入端又两个分支，分别是均匀采样分支和类平衡采样分支。其中分均匀采样支主要旨在学习可推广的特征；类平衡采样分支则用于学习 less biased 分类器。

基于此，还有三个提出的新模块：FCM 和 LCM 旨在通过补偿训练数据和测试数据之间的特征漂移来缓解尾部类的过拟合问题；RBMC 旨在通过添加一个从均匀分支到类平衡分支的残差路径来缓解头部类的欠拟合问题。

![image-20230906134433909](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230906134433909.png)

**Feature Compensation Module**

**特征飘移的分析：**

1. 由于尾部类别包含少量的训练样本，导致特征提取器仅能到不完整的特征表示，因此会对训练样本的部分特征过拟合。而有的尾部样本会和某些头部样本存在相似的特点，因此尾部类别的测试特征可能会向相似头部类别的特征空间漂移。
2. 一个尾类别的测试样本通常具有不同的特征，如不同的形状、大小和姿势。因此，不同的测试样本可能与不同的头部类别共享特征，导致向多个相似的头部类别漂移。

​	**特征飘移的计算：**——基于类别之间的相似性

首先基于类原型之间的相似性来找到尾部类的相似头部类：![image-20230906142913439](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230906142913439.png)，

```St stores m nearest head categories with respect to the tail category t.```

特征飘移量：![image-20230906143107650](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230906143107650.png)，$\alpha$是补偿系数，由于学习特征的泛化往往与训练样本的数量呈正相关，因此训练样本较少的类别更容易更接近相似的头部类别，因此系数设置为和样本训练数量相关：![image-20230906143139847](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230906143139847.png)，Nmax表示训练样本数量最多的类中包含的训练样本数。

​	**特征飘移补偿**：

基于上述评估的特征飘移向量，FCM 补偿原始的训练特征，以减少特征漂移。根据![image-20230906150609859](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230906150609859.png)计算漂移概率 $s_{tj}$和保留特征概率 $s_{tt}$。

补偿特征计算公式：![image-20230906150757081](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230906150757081.png)。

![image-20230906151319405](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230906151319405.png)

**Logit补偿模块**：

虽然 FCM 可以粗略估计特征漂移方向，但尾类的测试特征在训练过程中是未知的，通常表现出复杂的分布。因此，由 FCM 估计的漂移向量容易出现误差，难以覆盖所有的漂移方向。为此，作者提出了**将不确定性纳入漂移估计**的 LCM。加入不确定性可以防止后续的分类器对 FCM 误差的过拟合，从而提高其鲁棒性。此外，引入不确定性可以扩展估计区域，覆盖更多的漂移方向，从而产生更多多样化的尾类特征，以改善分类边界。

具体做法为，引入高斯噪声：![image-20230906151624490](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230906151624490.png)

补偿logits：![image-20230906152829547](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230906152829547.png)，

**残差平衡多代理分类器：**

如上分析，一个尾部类的测试特征倾向于向多个相似的头部类漂移。因此，一个尾类的测试特征的分布呈现多模态势。在简单的线性分类器中，每个类的权值向量作为一个单一的代理，这在多模式设置下很难进行优化。为此，作者考虑了一个多代理分类器来更好地捕获复杂的特征分布。

![image-20230906153745166](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230906153745166.png)

![image-20230906153759484](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230906153759484.png)

在这种设计中，RBMC 在CBS（通过残差分类器）和US（通过均匀分类器）方案下共同学习。因此，通过均匀分类器可以恢复被 CBS 破坏的头部信息。从这个意义上说，RBMC 可以利用来自整个训练数据集的信息来缓解拟合不足问题，同时避免了对头部类的高度倾斜。

总体算法流程：

![image-20230906154229168](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230906154229168.png)



> 总结：

* 根据类别原型的相似性来评估特征飘移的程度
* 尾类特征飘移 ---> 头部类特征（一到多）
* LCM 中通过加入噪声，增加随机性来减少分类器对 FCM 错误的偏差，并且提高分类器的鲁棒性；

## 20230915

### 5_Semi-Supervised Object Detection in the Open World_xx_无代码

> 作者：Garvita Allabadi Ana Lucic Peter Pao-Huang Yu-Xiong Wang Vikram Adve

> 贡献：

背景：现有的半监督目标检测方法假设训练和未标注数据集中存在一组固定的类，即分布内（ID）数据。当这些方法部署在开放世界中时，由于未标注和测试数据可能包含训练过程中 unseen 的目标，即分布外（OOD）数据，性能会显著降低。

在本文中探讨的两个关键问题是：能检测到这些 OOD 样本吗？如果能，能从中学习吗？

考虑到这些因素，作者提出了**开放世界半监督检测框架**（OWSSD），该框架可以有效地检测 OOD 数据，以及从 ID 和 OOD 数据中学习的半监督学习 pipeline。作者介绍了一种**基于集成的 OOD 检测器**，该检测器由仅根据 ID 数据训练的轻量级自动编码器网络组成。

> 方法：

本文提出的框架由三个组成部分组成：

1. 用于开放世界目标检测的类不可知proposal生成器；
2. 用于 OOD 检测的基于集成的网络；
3. 在学习过程中包括 ID 和 OOD 数据的 OOD 感知半监督学习pipeline

采用师生模式，使用两阶段的训练过程。

在第一阶段，使用弱增广的标记数据来训练教师模型；

在第二阶段，使用标记数据和强增广的未标记数据训练学生模型，并使用一致性正则化范式，即使在增广后，也强制模型对未标记样本输出相同的预测。

![image-20230913151518488](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230913151518488.png)

**Open-World Object Proposals**：

使用 OLN 网络在标记的数据上训练，然后用于无标记数据来生成 proposals，并基于(1) objectness scores 和(2) individual areas of proposals（较小的框会被过滤掉，以减少整体噪声） 筛选 proposals。

**Ensemble-Based OOD Detector**：

![image-20230913152454458](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230913152454458.png)

![image-20230913153322965](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230913153322965.png)<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230913153338899.png" alt="image-20230913153338899" style="zoom:80%;" />

> 总结：

* 使用 OLN 产生类不可知 proposals
* 基于多个低纬度数据特征自编码网络集成的投票决策机制

### 6_Attention with or without working memory: mnemonic reselection of attended information_Trends in Cognitive Sciences 2023

> 作者：Yingtao Fu,1,3 Chenxiao Guan,1,3 Joyce Tam,2 Ryan E. O’Donnell,2 Mowei Shen,1,* Brad Wyble,2,*and Hui Chen ,1,*

> 贡献：

长期以来，注意一直被视为人类实现信息选择的决定性因素，即通过注意选择少部分重要信息进行深入加工，并将其存储至工作记忆，而未被注意的信息则被过滤。因此，注意往往被比喻为信息进入工作记忆的“闸门”。

陈辉教授课题组近年来的系列研究证明，即使已经注意并使用过的信息，工作记忆仍然会对其进行选择性存储，挑战了“注意决定工作记忆”这一传统观点。本文系统梳理了这部分研究的新进展，并在此基础上，首次提出了**“工作记忆再选择”**理论模型。该理论认为，人类信息选择存在两个过程：**注意选择过程**和**记忆再选择过程**。对于那些已被注意选择的信息，还存在第二轮的记忆选择，该过程最终决定了哪些信息可以被记忆存储。该理论模型描述了注意选择和记忆选择的协同作用过程，强调了人类信息选择的灵活性和适应性。

> 模型：

属性失忆（Attribute Amnesia）：人们经常无法报告他们刚才刚刚关注到的信息。

这种报告的失败被认为是由于缺乏将被参与的信息整合到工作记忆中，这表明了注意力和工作记忆之间的分离。

在这些发现的基础上，本文提出了一个称为**记忆重新选择**的新概念来描述被参与信息中的第二轮选择。这些发现挑战了关于注意力和工作记忆是如何相互关联的传统观点，并为将注意力和记忆建模为可分离的过程提供了新的线索。

**工作记忆再选择模型**

![image-20230914224956130](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230914224956130.png)

* 图的最下面部分：表示对外部世界的注意力选择。（选择的对象具有高分辨率，而未选择的对象则显得模糊）
* 图的中间部分：代表我们在关注的选定对象的具体特征（如颜色和形状）
* 图的最上面部分：显示了工作记忆如何**重选**关注的特征。（例如，一个特征（颜色）被引入工作记忆，而其他特征（如形状）则被排除）

> 总结：

* **属性遗忘**将任务中注意和记忆信息编码解耦，证明注意力选择的信息并不总是被工作记忆选择，甚至可能受到积极抑制；
* 因此本文提出了一个**记忆重选模型**，该模型描述了如何通过选择性存储关注的信息来形成工作记忆表征。

## 20230922

### 7_On the Robustness of Open-World Test-Time Training: Self-Training with Dynamic Prototype Expansion_ICCV 2023_有代码

> 作者：Yushu Li1   Xun Xu2   Yongyi Su1   Kui Jia1

> 代码：https://github.com/Yushu-Li/OWTTT

> 贡献：

背景：提高模型泛化能力是推动基于视觉的感知方法落地的重要基础，测试段训练和适应（Test-Time Training/Adaptation）通过在测试段调整模型参数权重，将模型泛化至未知的目标域数据分布段。现有 TTT/TTA 方法通常着眼于在闭环世界的目标域数据下提高测试段训练性能。

为促进开放场景下的 TTT 应用，研究的重点已转移到调查 TTT 方法可能失败的场景。人们在更现实的开放世界环境下开发稳定和强大的 TTT 方法已经做出了许多努力。

而在本文工作中，作者深入研究了一个很常见但被忽略的**开放世界场景**，其中目标域可能包含从显著不同的环境中提取的测试数据分布，例如与源域不同的语义类别，或者只是随机噪声。作者将这样的测试数据称为**强分布外数据**（strong OOD），相应的，**弱 OOD 数据**则是分布偏移的测试数据，例如常见的合成损坏。

如下图所示，作者首先对现有的 TTT 方法在 OWTTT 设定下进行评估，发现通过自训练和分布对齐的 TTT 方法都会受到强 OOD 样本的影响。这些结果表明，应用现有的 TTT 技术无法在开放世界中实现安全的测试时训练。作者将它们的失败归因于以下两个原因：1、基于自训练的 TTT 很难处理强 OOD 样本，因为它必须将测试样本分配给已知的类别。尽管可以通过应用半监督学习中采用的阈值来过滤掉一些低置信度样本，但仍然不能保证滤除所有强 OOD 样本；2、当计算强 OOD 样本来估计目标域分布时，基于分布对齐的方法将会受到影响。全局分布对齐和类别分布对齐都可能受到影响，并导致特征分布对齐不准确。

![image-20230915223017951](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230915223017951.png)

因此本文提出了一种**自适应阈值的强域外数据样本过滤**方法，提高了自训练 TTT 方法的在开放世界的鲁棒性。该方法进一步提出了一种基于**动态扩展原型**来表征强域外样本的方法，以改进弱 / 强域外数据分离效果。最后，通过分布对齐来约束自训练。

> 方法：

**任务设定：**
TTT 的目的是使源域预训练模型适应目标域，其中目标域可能会相对于源域有分布迁移。在标准的封闭世界 TTT 中，源域和目标域的标签空间是相同的。然而在开放世界 TTT 中，目标域的标签空间包含源域的目标空间，也就是说目标域具有未见过的新语义类别。
为了避免 TTT 定义之间的混淆，本文采用 TTAC 中提出的顺序测试时间训练（sTTT）协议进行评估。在 sTTT 协议下，测试样本被顺序测试，并在观察到小批量测试样本后进行模型更新。对到达时间戳 t 的任何测试样本的预测不会受到到达 t+k（其 k 大于 0）的任何测试样本的影响。

**TTT by Prototype Clustering**

受到域适应任务中使用聚类的工作启发，将测试段训练视为发现目标域数据中的簇结构。通过将代表性原型识别为聚类中心，在目标域中识别聚类结构，并鼓励测试样本嵌入到其中一个原型附近。原型聚类的目标定义为最小化样本与聚类中心余弦相似度的负对数似然损失：

![image-20230915224441997](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230915224441997.png)

如果强 OOD 样本被强制归类为任何源类别，对有噪声的标记样本进行自训练会混淆网络对弱OOD样本的鉴别能力。因此作者开发了一种**无超参数的方法来滤除强 OOD 样本**，以避免调整模型权重的负面影响。具体来说，为每个测试样本定义一个强 OOD 分数 os 作为与源域原型的最高相似度：

![image-20230915224656897](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230915224656897.png)

由于观察到离群值服从双峰分布，如下图所示。因此，我们没有指定固定阈值，而是将最佳阈值定义为分离两种分布的的最佳值。具体来说，问题可以表述为将离群值分为两个簇，最佳阈值将最小化中的簇内方差。优化下式可以通过以 0.01 的步长穷举搜索从 0 到 1 的所有可能阈值来有效实现：

![image-20230915225449312](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230915225449312.png)![image-20230915225504921](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230915225504921.png)

![image-20230915225418889](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230915225418889.png)

**Open-World TTT by Prototype Expansion：**

扩展强 OOD 原型池需要同时考虑源域和强 OOD 原型来评估测试样本。为了从数据中动态估计簇的数量，之前的研究了类似的问题。确定性硬聚类算法 DP-means 是通过测量数据点到已知聚类中心的距离而开发的，当距离高于阈值时将初始化一个新聚类。DP-means 被证明相当于优化 K-means 目标，但对簇的数量有额外的惩罚，为动态原型扩展提供了一个可行的解决方案。
为了减轻估计额外超参数的难度，首先定义一个测试样本，其具有扩展的强 OOD 分数作为与现有源域原型和强 OOD 原型的最近距离，![image-20230915225835048](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230915225835048.png)。因此，测试高于此阈值的样本将建立一个新的原型。为了避免添加附近的测试样本，增量地重复此原型扩展过程。

随着其他强 OOD 原型的确定，定义了用于测试样本的原型聚类损失，并考虑了两个因素。

首先，分类为已知类的测试样本应该嵌入到更靠近原型的位置并远离其他原型，这定义了 K 类分类任务；

其次，被分类为强 OOD 原型的测试样本应该远离任何源域原型，这定义了 K+1 类分类任务。

原型聚类损失定义为：![image-20230915225959496](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230915225959496.png)

**Distribution Alignment Regularization：**

由于自训练容易受到错误伪标签的影响，目标域包含 OOD 样本时，情况会更加恶化。为了降低失败的风险，作者进一步将分布对齐作为自训练的正则化约束：

![image-20230915230115642](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230915230115642.png)

算法流程：

![image-20230915230224438](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230915230224438.png)

> 总结：

* 原型扩张 ---> 相当于对 OOD 也建立老巢，以降低其对 ID 的影响

### 8_Adaptive Plasticity Improvement for Continual Learning_CVPR 2023_有代码

> 作者：Yan-Shuo Liang and Wu-Jun Li*

> 代码：https://github.com/liangyanshuo/Adaptive-Plasticity-Improvement-for-Continual-Learning

> 贡献：

背景：许多研究都试图解决持续学习中的灾难性遗忘（CF）问题。然而，在旧任务上追求不遗忘可能会损害模型对新任务的可塑性。虽然已经提出了一些方法来实现稳定性-可塑性的权衡，但**没有一种方法考虑评估模型的可塑性和提高可塑性**。

在这项工作中，作者提出了一种新的方法，称为**自适应可塑性改进（API）**。除了能够克服对旧任务的 CF 外，API 还试图评估模型的可塑性，然后在必要时自适应地提高模型的可塑性，以便学习新任务。

> 方法：

如下图，以一个简单的三层神经网络为例解释 API：除了最后一层，其他层都根据相应需要来决定是否增加输入维度，并增加相应的权重参数。

对于每个 task t，API 在克服遗忘的基础上，首先评估模型的可塑性，然后根据评估结果考虑是否增加输入维度，即扩张权重参数，如果可塑性足够，那么就不需要扩张权重。

![image-20230721100143169](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230721100143169.png)

API 采用梯度修正策略 GPM 来克服 CF。基于该策略的方法修正了新的任务梯度，使其不影响模型在旧任务上的表现。但由于 GPM 存在一个问题，即内存占用会一直增加，因此作者使用 **DualGPM** 来解决内存占用一直增加的问题：<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230721102803997.png" alt="image-20230721102803997" style="zoom:80%;" />

![image-20230721103227991](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230721103227991.png)

**Plasticity Evaluation**

gradient retention ratio (GRR)：<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230721102903418.png" alt="image-20230721102903418" style="zoom:80%;" />

扩张标准：<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230721103019037.png" alt="image-20230721103019037" style="zoom:80%;" />

![image-20230721103310368](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230721103310368.png)

> 总结：

* 从梯度的角度去考虑问题，在克服遗忘的前提下同时保持模型持续学习过程中的可塑性；

## 20230929

### 9_EdaDet: Open-Vocabulary Object Detection Using Early Dense Alignment_ICCV 2023_代码待上传

> 作者：Cheng Shi, Sibei Yang†（上科大）

> 贡献：

背景：开放词汇目标检测旨在在只有基础类别的训练标签的情况下，定位和识别基础类别和新类别的对象。过去的方法利用视觉-语言模型的强大的零样本识别能力，将对象级别的嵌入与类别的文本嵌入进行对齐。然而，**现有方法中存在对基础类别过拟合的问题**，即与基础类别最相似的新类别的性能特别差，因为它们被识别为相似的基础类别。

 过去的方法通常通过生成与新类别相关的伪proposal，并使用基础和新类别进行训练来解决开放词汇目标检测问题。然而，这些方法需要额外的训练资源，这些资源与新类别有很大的重叠或相关性，限制了它们的实际应用。

本文首先发现现有方法由于**丢失了关键的细粒度局部图像语义**而无法实现强大的基础到新类别的泛化能力。然后，作者提出了**早期密集对齐（EDA）**方法，以弥合泛化的局部语义和对象级别预测之间的差距。EDA 使用对象级别监督来学习密集级别而不是对象级别的对齐，以保持局部细粒度语义的一致性。

> project page：https://chengshiest.github.io/edadet

> 方法：

作者将开放词汇表检测分解为两个后续分支： (1)生成类不可知的对象建议，(2)识别这些对象建议的开放词汇表类别。

![image-20230908165323649](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230908165323649.png)

如图 3b 所示。作者认为，分类和回归这两个分支之间高度相关的预测损害了类别不可知的建议框生成。因此，作者将开放词汇表分类从解码器更浅层的提案生成分支中分离出来，如图3a所示，以深入地解耦这两个分支。

**Early Semantic Alignment at Dense Level (Eda)**

与之前的方法将 object-level 特征对齐到 base classifier（基类文本特征）不同，本文在早期阶段使用 object-level 的监督将密集局部图片信息（dense local image semantics）对齐到 base classifier。然后根据 dense probabilities 将proposals 分类到不同的类别。由于密集级别对齐可以保持细粒度的识别能力，以区分相似的新类别和基类别的局部语义细节，从而能够区分它们。

局部图片语义信息：![image-20230908170131620](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230908170131620.png)

CLIP’s dense probability map：![image-20230908170220071](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230908170220071.png)

最终计算得到的 dense score map S：![image-20230908170247919](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230908170247919.png)

某个 proposal 属于某个类别的概率：![image-20230908170423444](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230908170423444.png)

**Global Semantic Alignment**

若只有局部对齐会导致图像的全局语义信息的丢失，因此作者将集成的局部图像语义与 CLIP 的图像编码器进行对齐，以提高密集对齐。

![image-20230908170557818](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230908170557818.png)

> 总结：

* 尽早解耦分类头与回归头，减少两者的干预程度；
* 在 low-level 处引入细粒度密集图片语义信息的监督，使模型能够辨别相似类别之间的差异，从而减少模型将与基类较为相似的新类误检为基类；
* mark：等代码出来看看代码

### 10_OpenGCD: Assisting Open World Recognition with Generalized Category Discovery_arxiv 2023.08_有代码

> 作者：Fulin Gao, Weimin Zhong, Zhixing Cao, Xin Peng, Zhi Li

> 代码：https://github.com/Fulin-Gao/OpenGCD

> 贡献：

背景：OWR需要解决三个主要任务：开放集识别、广义类别发现和增量学习。现有的方法大多假设第二个任务完全由人工完成，缺乏自动化的解决方案。

为了填补这一空白，本文提出了**OpenGCD**方法，通过结合三个关键思想顺序解决了上述问题。

​	首先，通过评估分类器预测的不确定性，评估实例的来源（未知或特定已知）；

​	其次，作者首次将广义类别发现（GCD）技术引入 OWR，以帮助人们对未标记数据进行分组；

​	最后，为了顺利进行增量学习和 GCD，作者为每个类别保留了相等数量的信息丰富的示例。

一些相关工作：

**Open set recognition**：在 OSR 场景中，训练集中存在对世界的不完整知识，在测试过程中，可能碰到未知类别。这就要求在线模型不仅能对已知/见过的类别进行分类，还要能拒绝未知/未见/新颖的类别。**1-vs-all 原则**、**阈值化**和**未知概率估计**是最受欢迎的三种 OSR 策略 。1-vs-all 原则为基础的方法是最早出现的，但相对较为繁琐；基于阈值的方法具有高兼容性和低计算开销；用于估计未知概率的方法是最直观的。

**Generalized category discover**：在 GCD 场景中，未标注的测试集（可用于训练）可能包含了在训练过程中已知/已见过和未知的类别。它要求模型不仅能对已知/见过的类别进行分类，还需要对未知/未见/新颖的类别进行聚类。GCD 与 OSR 的三个主要区别在于1、是否支持在线运行，2、未标注的测试集是否可用于训练，以及3、未知类别应该被拒绝还是聚类。此外，如果测试集和训练集是类不相交的，这个问题就退化为了 NCD。

<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230921144210191.png" alt="image-20230921144210191" style="zoom: 67%;" />

> 方法：

特征嵌入：使用在未标记的 ImageNet 数据上训练的 Vision Transformer（ViT）模型作为特征提取器，获得实例的特征表示；

样本选择：使用 DS3 算法从标记的特征集中选择信息量丰富的实例，并将选定的样本存储在缓冲区中以供进一步使用；

​	**DS3** ：Dissimilarity-based sparse subset selection

分类器（重新）拟合：使用多层感知器（MLP）或 SVM/XGBoost 分类器对选定的样本进行训练，使分类器学习已知类别的现有知识；

基于不确定性的开放集识别：根据预测概率为未标记的实例分配标签，通过近似分类器预测的不确定性来获得新的概率分布，将不确定性较高的实例分配给标签0（未知），其他实例分配给已知类别；

![image-20230921152545996](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230921152545996.png)

辅助广义类别发现的手动注释：使用广义类别发现（GCD）来过滤和分组来自新类别的特征，首先使用 GCD 对被 OSR 拒绝的未标记特征进行过滤和分组，然后进行手动修正和标记，并使用 GCD 确定估计的类别数；

估计类别数：使用 ss-k-means++ 算法估计类别数，确定已知类别的质心并选择剩余的质心作为新类别，估计的类别数在 GCD 过程中使用；

<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230921151929487.png" alt="image-20230921151929487" style="zoom:80%;" />

> 总结：

* DS3样本选择算法：以多样性为目标而选择的样本从原始数据中保留了**尽可能多的空间信息**，避免了未知空间的扩展，从而**减少了开放空间的风险**，即将已知实例归类为未知实例的风险。

## 20231006

### 11_Modeling Inter-Class and Intra-Class Constraints in Novel Class Discovery_CVPR 2023_有代码

> 作者：Wenbin Li, Zhichen Fan, Jing Huo, Yang Gao

> 代码：https://github.com/FanZhichen/NCD-IIC

> 贡献：

背景：新类发现任务（NCD）的目的是学习一个模型，该模型将 common knowledge 从一个类不相交的标记数据集转移到另一个未标记的数据集，并在其中发现新的类。作者发现，现有的方法并没有充分利用 NCD 设置的本质（即忽视了标记和未标记类别之间的不相交特性）。

为此，本文提出了基于**对称 KL 散度**（sKLD）对 NCD 中的**类间**和**类内**约束进行建模。

* 类间 sKLD 约束：有效地利用标记类和非标记类之间的不相交关系，增强了嵌入空间中不同类的可分性；
* 类内 sKLD 约束：明确地约束样本与其相应增强样本之间的内部关系，并同时确保训练过程的稳定性。

结果：实验结果表明，所提出的方法大幅优于现有的最先进方法。该方法有效区分了未标记数据和标记数据，有助于发现新类别。类间 sKLD 约束在提高未标记类别的性能方面起到了关键作用。该方法防止将标记图像错误分类为未标记类别，反之亦然。

<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230708223409247.png" alt="image-20230708223409247" style="zoom:80%;" />

> 方法：

与之前的工作一致，假设未标记类 $C_{u}$ 的数量是预先已知的。

思路比较简单，对于分类头的输出，通过对称 KL 散度尽可能增大有标记数据和无标记数据的分类差异，减小有标记数据（无标记数据）的分类差异。

<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230708231548494.png" alt="image-20230708231548494" />

 类间 KL 散度约束使每个小批量中标记和无标签样本之间的距离更大：

<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230708231619879.png" alt="image-20230708231619879" style="zoom:80%;"/>

<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230708232125312.png" alt="image-20230708232125312" style="zoom:80%;" />

类内 KL 散度约束确保同一图像的不同增强之间的一致性：

<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230708232235545.png" alt="image-20230708232235545" style="zoom:80%;" />

除上述两个损失外，还有标准交叉熵（CE）损失：<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230708231920678.png" alt="image-20230708231920678" style="zoom:80%;" />

总体训练损失函数为：

<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230708231941021.png" alt="image-20230708231941021" style="zoom:80%;" />

> 总结：

对分类头输出的 logits 分布进行约束。

### 12_When Noisy Labels Meet Long Tail Dilemmas:A Representation Calibration Method_ICCV 2023_无代码

> 作者：Manyi Zhang1,* Xuyang Zhao2,∗ Jun Yao3 Chun Yuan1,† Weiran Huang4,

> 贡献：

背景: 深度学习在许多领域取得了快速进展，但实际中很难获得完美的数据集，其中一部分数据由于固有的模糊性和注释者的错误而被错误标记，同时数据也存在类别不平衡的问题。过去的方法主要针对噪声标签和长尾数据**分别进行处理**，但当这两种不完美的情况同时存在时，这些方法无法很好地工作。
本文的动机是开发更先进的方法来解决在长尾数据上学习噪声标签的实际问题。作者提出了一种称为 **RCAL** （representation calibration method）的表示校准方法来解决这个问题。

RCAL 使用无监督对比学习提取的表示，并假设每个类别中实例的表示符合多元高斯分布。从由错误标记和类别不平衡数据导致的污染的表示中恢复出潜在的表示分布；并从恢复的分布中采样额外的数据点以提高泛化能力。

> 方法：

问题背景：K （K >= 2）类别的分类问题。在本文中，目的是通过只使用不平衡和噪声标记的训练数据集来稳健地学习一个分类器。

RCAL 包含两个阶段：

1. 通过对比学习进行表征增强
2. 通过表征校准提高分类器的鲁棒性

<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231006161059755.png" alt="image-20231006161059755" style="zoom:80%;" />

**Enhancing Representations by Contrastive Learning**

从直观上看，由于自监督对比学习中的表示学习不能访问训练数据的标签，因此所获得的表示不会受到错误标签的影响。作者采用MOCO设定，对于输入 $x$，首先获得两个对应的随机增强版本 $x^q, x^k$，然后分别送入 query、key encoder中获得编码特征 $z^q, z^k$，随后使用2-layer MLP 将其投射到低维特征空间$\hat{z}^q, \hat{z}^k$，对比训练损失：

![image-20231006162000316](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231006162000316.png)

其中来自query encoder 的降维前表征  $z^q$将用于后续的表征校准。

**Distributional Representation Calibration**

在本文中，作者假设在被分类不平衡和噪声标签损坏之前，每类训练数据的深度表示都符合多元高斯分布。（已有工作证明这一点）

受到相似的类具有相似均值和协方差的启发，借用头部类的统计数据来帮助尾部类的校准。具体地说，通过计算不同类的表示方法之间的欧氏距离来度量相似性，对于尾部类别，选择与其最相似的topk个头部类别的统计数据对该尾部类的统计数据进行校准；

<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231006164100633.png" alt="image-20231006164100633" style="zoom:80%;" /><img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231006164111993.png" alt="image-20231006164111993" style="zoom:80%;" />

**Individual Representation Calibration**

为了进一步提高表示的鲁棒性，作者进行了个体表示校准，包含两部：

* 考虑到自监督对比学习提供了鲁棒表示，作者严格规定后续学习表示与对比学习带来的表示之间的距离：

![image-20231006164311173](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231006164311173.png)

* 为了进一步使学习到的表示具有鲁棒性来处理长尾情况下的噪声标签，采用了混合方法：

![image-20231006164355740](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231006164355740.png)

![image-20231006164406107](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231006164406107.png)![image-20231006164417636](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231006164417636.png)是交叉熵损失

## 20231013

### 13_Detect Every Thing with Few Examples_arxiv 202309_有代码

> 作者：Xinyu Zhang, Yuting Wang, Abdeslam Boularias

> 代码：[mlzxy/devit (github.com)](https://github.com/mlzxy/devit)

> 贡献：

背景: 目标识别和定位是计算机视觉中的核心任务之一。本文关注的是**开放集物体检测**，即检测训练过程中未见过的任意类别的物体。
过去的研究主要采用开放词汇的范式，利用视觉-语言骨干来表示类别。然而，使用语言作为类别表示存在一些局限性，如某些物体难以准确描述或缺乏简洁的名称，以及视觉概念与语言之间的关联是不断演变的。此外，基于语言的分类方法没有充分利用图像注释信息。

本文提出了一种名为 ==DE-ViT== 的开放集物体检测器，**它使用示例图像而不是语言来学习新的物体类别**。与过去的少样本方法相比，DE-ViT 不需要在新类别上进行进一步的训练，从而避免了复杂和繁琐的过程。此外，DE-ViT 在实践中的准确性也超过了开放词汇的解决方案，特别是在大规模数据集（如COCO和LVIS）上。

文中指出*open-vocabulary object detection* 设定存在以下限制：

1. 某些物体很难仅用语言来准确描述，或者缺乏简洁的定义；
2. 视觉概念和语言之间的联系是不断发展的，而不是静态的，而开放词汇模型只涉及到语料库中以前已连接的对象和名称；
3. 基于语言的分类在图像注释可用时不会利用这些注释。

> 方法：

总体架构与流程如下图：DE-ViT 使用仅基于视觉的 DINOv2 骨干网络，并构建每个类别的原型向量。

两个技术点：

* 将多分类任务转化为二分类任务
* 提出了一种新颖的区域传播技术用于定位

![image-20231007101959431](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231007101959431.png)

Similarity map **s** 使用proposal features 和 prototypes计算（D是特征维度）：

<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231007103310552.png" alt="image-20231007103310552" style="zoom:80%;" />

ViT backbones, prototypes, 以及 the similarity maps 在检测器训练阶段保持冻结状态。

 **CLASSIFICATION WITH AN UNKNOWN NUMBER OF CLASSES**

![image-20231007105211388](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231007105211388.png)

average 的特征仅用来选择 topk 个类，分类使用的还是下面分支的特征。

**LOCALIZATION WITH REGION PROPAGATION**

![image-20231007114500369](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231007114500369.png)

直觉上，只有与对象重叠的 proposal 才是重要的，因为其他的 proposal 会被分类网络拒绝。如果我们扩大这种 proposal ，它们可能会覆盖底层对象的整个区域。因此，原始 proposal  可以通过预测扩展 proposal 内的目标区域来细化。

> 总结：

* 通过对 proposal box 进行扩张（或缩小）的方式来提高该 proposal 分类正确的概率。

### 14_Unsupervised Recognition of Unknown Objects for Open-World Object Detection_arxiv 202308_有代码

> 作者：Ruohuan Fang, Guansong Pang, Lei Zhou, Xiao Bai, Jin Zheng

> 代码：[frh23333/mepu-owod: Code Implementation of "Unsupervised Recognition of Unknown Objects for Open-World Object Detection" (github.com)](https://github.com/frh23333/mepu-owod)

> 贡献：

背景：文中指出根据 objectness score 选择伪未知类候选框的缺点：这种方法可以成功地检测出与已知对象具有相似特征的未知对象。然而，它们对已知类别存在严重的==标签偏差==问题，也就是说，它们倾向于将所有与已知对象不同的区域（包括未知对象区域）检测为背景。

【我自己的想法是，根据 objectness score 选择伪未知类候选框的缺陷在于，在选择框的时候，很有可能误选到已知类对象（见如下示意图，数字表示 objectness score ）。】

为了解决标签偏差问题，本文提出了一种名为 **MEPU**（**m**odel and **e**xtend **p**seudo **u**nknown） 的方法，将未知对象识别问题分解为两个子问题：通过无监督区域生成器生成的原始伪标签进行无监督建模，以及扩展未在这些伪标签中涵盖的新未知对象。通过引入重建误差基于 Weibull 模型的模块（REW），利用特征频率建模并识别未知对象，MEPU 方法有效地解决了标签偏差问题。此外，通过将 REW 产生的软标签与无分类器的自训练相结合，进一步扩展了伪标签的覆盖范围，从而减轻了标签偏差问题的影响。

> 方法：

MEPU方法由两部分组成：基于重构误差的 Weibull 模型（**REW**）和增强的目标定位网络（**ROLNet**）

REW 模块旨在对由无监督区域 proposal 生成器生成的伪标签中隐藏的未知对象进行建模。它使用无监督方法从常见背景区域中识别各种类型的未知对象，并为所有伪标签分配软标签，以估计其作为真实未知对象的可能性。
ROLNet 模块利用 REW 得分和无分类头的目标定位网络（OLN）进行自训练过程。它通过使用 REW 得分和 OLN 来识别和定位未标记区域中的新未知对象，有效地扩展未知对象集合。

![image-20231008102618234](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231008102618234.png)

***Reconstruction Error-based Weibull Modeling of Foreground and Background Regions***

![image-20231008105316981](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231008105316981.png)

背景区域往往具有频繁出现的特征，与不同前景对象区域的罕见特征相比，使其更好地重建，导致重建误差更小。如下图，很明显，已知目标区域的重建误差通常比背景区域的重建误差要大得多。虽然未知物体可能与已知物体有不同的外观，但我们可以假设它们的低出现频率和高重建误差相似（因为存在各种类型的未知物体）。因此，**本文利用采样的已知对象的重建误差来估计所有前景区域的分布**。

![image-20231008105454140](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231008105454140.png)

给定图像 I 种的伪未知对象框，通过求平均的方式计算其重建误差，然后通过下式评估其是 unknown 对象的分数，作为 soft label。

![image-20231008105939309](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231008105939309.png)

然后，该 soft label 会被用于计算加权分类损失：

![image-20231008110205235](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231008110205235.png)

![image-20231008110215881](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231008110215881.png)

**ROLNet: REW-enhanced Object Localization Network for Extending the Set of Unknown Objects**

![image-20231008110746462](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231008110746462.png)

> 总结：

* 背景大差不差所以重构损失小，前景各有不同所以重构差异大；以此来选择合适的未知类（前景）伪候选框。

## 20231020

### 15_Bridging the Gap: Learning Pace Synchronization for Open-World Semi-Supervised Learning_arxiv 202309_无代码

> 作者：Bo Ye, Kai Gan, Tong Wei†, Min-Ling Zhang

> 贡献：

背景: 在开放世界半监督学习中，已知类别和新类别之间存在着显著的学习差距。

![image-20231016174131143](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231016174131143.png)

目前的方法通常假设未标记数据仅包含已知类别的样本，而忽略了其中可能存在的新类别样本。已有的开放世界半监督学习方法在解决这个问题时采用了两个策略：一是识别已知类别的未标记样本并分配伪标签，二是自动聚类新类别的未标记样本。然而，这些方法在实际应用中仍然存在一些问题，特别是在新类别的学习上。

鉴于已知类别和新类别之间的学习差距，本文提出了一种新的方法来同步学习步调，以平衡已知类别和新类别的学习。通过自适应边界损失和伪标签对比聚类，提高新类别的发现和学习效果。

> 方法：

本文方法 LPS 旨在同步 OpenSSL 中已知和新颖类别的学习速度。

* 引入自适应边界损失（LAM），通过引入**类依赖边界**来调节已知类别的学习速度；
* 结合了伪标签对比聚类损失（LPC），通过在输出空间中将可能来自同一类别的样本聚集在一起来增强新颖类别的发现。
* 利用多个正负对作为监督信号，并结合对置信度较低的无标签样本进行无监督对比学习。

![image-20231016174018124](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231016174018124.png)

**Adaptive Distribution-Aware Margin**

为了缓解 seen 类和 novel 类学习速度差异问题，作者提出了一种**分布感知的边际损失机制**，旨在引导模型的注意力转向 novel 类的学习。值得注意的是，随着模型的预测类分布接近潜在的（类平衡）分布，这种边际减小。

![image-20231016180805489](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231016180805489.png)

**Pseudo-Label Contrastive Clustering**

与现有的工作不同，本文引入了一种新的聚类方法，监督对比学习驱动的聚类，以充分利用可靠的模型预测作为监督信号。

在一个 mini batch 中，有标签的样本 和 预测置信度高的样本 均会被作为 $B_{l}$ 来计算对比损失；

![image-20231016181201672](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231016181201672.png)

**Unsupervised Contrastive Learning**

(7) 式通过具有高预测置信度的未标记样本来**增强**标记数据集，作者考虑是否可以使用具有低预测置信度的未标记样本来丰富表示学习。为了实现这一点，采用无监督对比学习来鼓励类似的预测，而不是给定样本和增强对应样本之间的嵌入。

![image-20231016181511128](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231016181511128.png)

> 总结：

* 聚类时，使用高预测置信度的未标记样本来**增强**标记数据集

### 16_CoTDet: Affordance Knowledge Prompting for Task Driven Object Detection_arxiv 202309_有代码链接

> 作者：Jiajin Tang*, Ge Zheng*, Jingyi Yu, Sibei Yang

> 代码：https://toneyaya.github.io/cotdet/

> 贡献：

背景: 传统的物体检测任务旨在在图像中检测给定类别的物体实例，但在实际应用中，任务驱动的物体检测更关注的是检测适合完成特定任务的物体实例。然而，由于任务所需的物体类别过于多样化，直接将物体的视觉特征或类别与任务进行映射无法解决这一挑战。

以往的方法通过学习物体的视觉上下文特征或类别之间的映射来解决任务驱动的物体检测问题，但效果不理想。这些方法无法充分考虑到任务所需的**常识知识**，导致错误的物体选择。

 鉴于大型语言模型（LLMs）在编码世界知识方面的出色能力，本文提出了一种新颖的方法，通过从 LLMs 中提取 Affordance 知识，对任务和物体之间建立联系。此外，本文还提出了一种知识条件检测框架，通过利用 Affordance 知识来指导物体检测和定位，从而更好地利用知识来提高检测性能。

> 方法：

本文提出的方法 ==CoTDet== 旨在通过获取**视觉可行性知识**并利用其来检测适合给定任务的物体实例，从而弥合任务驱动的目标检测与传统目标检测之间的差距。该方法包括以下几个步骤：

* 获取视觉可行性知识：作者提出了一种多级思维链（MLCoT）的方法，从大型语言模型（LLMs）中提取视觉可行性知识。这涉及生成代表性的物体示例，并为这些物体能够完成任务提供理由。MLCoT 能捕捉超越物体类别的视觉可行性的本质；

* 知识条件检测框架：作者提出了一种知识条件检测框架 CoTDet，该框架将检测器与视觉可行性知识相结合。该框架生成物体查询并使用视觉可行性知识指导框回归。另外，作者还使用去噪训练来教导解码器如何利用视觉知识进行框回归。

  （扩展到实例分割：CoTDet 网络可以通过使用分割头部进行任务驱动的实例分割而进行轻松扩展。）

![image-20231017102809742](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231017102809742.png)

**Visual Affordance Knowledge from LLMs**：为了检测以提供特定任务的对象，自然会首先考虑任务的需求，然后定位满足需求的合适对象。然而，任务要求是抽象的，不能直接对应于图像中的视觉内容来定位对象。基于此，作者建议显式地提取常见的视觉属性（即视觉启示），这些属性使不同的对象能够负担任务，并使用视觉启示来连接图像中的任务需求和对象实例。

​		 **Multi-Level Chain-of-Thought Prompting**：

​				**Object-level Prompting as Brainstorming**：在第一级中，提示 llm 生成提供输入任务 S 的日常对象示例。但是这是不可行的，因为： (1) 对象示例过于局限于部分对象类别，导致了对象类别与实际任务需求之间的差距。例如，图2中的fork不在 llm 返回的对象之中；(2) 可能会输出一些有噪声的不合适的对象。虽然有噪声的对象很少，但完全依赖于这些对象的示例是有风险的。例如，对于灭火任务，llm归还火斧，一种常见的消防工具，但它不能直接用于灭火。

​				**Affordance-level Prompting with Rationales**：为了解决上述挑战，并捕捉在代表性对象例子中隐含的基本视觉启示，作者提出为这些对象为什么能够承担任务生成理由，并从理由中总结视觉启示。

​		**Knowledge-conditioned Decoder**：

​				首先得到任务驱动的特征，然后结合前面提取的 common 视觉属性知识。

​				**Knowledge-conditional Query Generation**：<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231017103448000.png" alt="image-20231017103448000" style="zoom:80%;" />

​		   	 **Knowledge-conditional Decoding**：<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231017103532086.png" alt="image-20231017103532086" style="zoom:80%;" />

> 总结：

* 文中的泛化思想：从某几个常见对象中提取 common visual attributes 从而泛化到新的对象

## 20231027

### 17_Deep Feature Deblurring Diffusion for Detecting Out-of-Distribution Objects_ICCV 2023_有代码

> 作者：Aming Wu1 Da Chen2 Cheng Deng1*

> 代码：https://github.com/AmingWu/DFDD-OOD

> 贡献：

背景: 无监督的分布外对象检测（OOD-OD）任务的目标是在没有访问任何分布外数据的情况下检测未见过的分布外对象。

传统的对象检测模型通常遵循封闭集假设，即训练和测试阶段共享相同的类别空间。然而，实际场景中存在大量未知对象，这对基于封闭集假设的检测器提出了重大挑战。

为了解决这个问题，无监督的分布外对象检测任务被提出，旨在只利用已知分布数据来提高区分分布外对象的能力，并避免降低对已知分布对象检测的性能。

为了解决无监督的分布外对象检测任务中的两个关键挑战，即如何利用已知分布数据提高区分分布外对象的能力以及如何避免降低对已知分布对象检测的性能，本文从扩散模型的角度出发，提出了深度特征去模糊扩散（==DFDD==）方法。通过前向模糊和反向去模糊过程，合成接近已知分布对象和分布外对象分类边界的虚拟分布外特征，从而提高对象分类器的区分能力。**前向过程**逐渐对提取的特征进行高斯模糊，以合成接近 ID 和 OOD 目标分类边界的虚拟 OOD 特征；**反向过程**恢复前向过程中丢失的细节。

> 方法：

DFDD 利用扩散模型的思想合成虚拟的离群分布（OOD）特征，提高检测OOD目标的能力。该方法包括两个主要过程：前向模糊扩散和反向去模糊过程。

在前向过程中，逐渐对提取的特征图应用高斯模糊，得到虚拟的OOD图；

在反向过程中，使用 U-Net 结构模型对模糊特征进行去模糊处理，恢复原始特征。恢复的特征和原始特征用于训练，增强目标分类器的区分能力。

<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231018143953657.png" alt="image-20231018143953657" style="zoom:80%;" />

**Deep Feature Deblurring Diffusion**

作者通过实验发现，直接添加噪声可能会阻碍扩散模型在特征生成的应用。为此，探索用高斯模糊代替添加噪声。

**Forward Blurring Diffusion**：

为了减轻缺乏 OOD 数据的影响，利用正向过程来合成虚拟 OOD 特性。具体来说，扩散过程被固定为一个马尔可夫链，该链根据方差调度 σ1，...，σT 逐步对提取的特征图执行高斯模糊：

<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231018145007276.png" alt="image-20231018145007276" style="zoom:80%;" />

为了保证模糊效果，方差呈线性增加。作者忽略了内核大小可以更改的事实，而是将大小固定为5×5，正向过程没有可学习的参数。

<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231018145102045.png" alt="image-20231018145102045" style="zoom:80%;" />

**Reverse Deblurring Process**：

<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231018145134984.png" alt="image-20231018145134984" style="zoom:80%;" />

在训练过程中，使用损失函数 $L_{DFDD}$ 来训练所设计的 U-Net 模型，使其具有细节预测的能力：<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231018145256750.png" alt="image-20231018145256750" style="zoom:80%;" />

**DFDD-Driven OOD Object Detection**：

目标检测任务包括目标定位和分类。因此，加强特征 F0 中与目标相关的信息有利于准确地检测目标。如图 2 所示，首先在 F0 和模糊特征 FT 之间进行残差操作，**其输出涉及到丰富的对象结构信息**。然后，将剩余输出与 F0 连接，得到增强的结果。

对于 ID ：<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231018150101285.png" alt="image-20231018150101285" style="zoom:80%;" />

对于 OOD：<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231018150117631.png" alt="image-20231018150117631" style="zoom:80%;" />

总体训练目标：<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231018150139698.png" alt="image-20231018150139698" style="zoom:80%;" />

**Inference for OOD Object Detection**

通过设定的阈值决定被测样例是 ID 还是 OOD：<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231018150229679.png" alt="image-20231018150229679" style="zoom:80%;" />

算法伪代码：

<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231018150256699.png" alt="image-20231018150256699" style="zoom:80%;" />

> 总结：

* 使用扩散模型的思想，通过对 ID 样本特征添加高斯噪声的方式合成虚拟 OOD 特征（特征合成扩散版）
* 比较有意思的做法：用 (F0 - FT) 拼接到原来的 F0 上面去，起到一个信息增强的作用==！！！==

* mark：等代码放出来了看下代码

### 18_COCO-O: A Benchmark for Object Detectors under Natural Distribution Shifts_ICCV 2023_有代码

> 作者：Xiaofeng Mao, Yuefeng Chen, Yao Zhu, Da Chen, Hang Su, Rong Zhang, Hui Xue

> 代码：https://github.com/alibaba/easyrobust/tree/main/benchmarks/coco_o

> 贡献：

背景: 深度学习在计算机视觉领域取得了巨大成功，但实际目标检测应用在**自然分布偏移**的图像输入上可能失去效果。为了解决这个问题，研究界开始关注目标检测器在分布偏移下的鲁棒性。

 过去的研究主要集中在分类任务的鲁棒性上，而对于目标检测等其他视觉任务的鲁棒性研究相对较少。现有的鲁棒性基准数据集也存在一些问题，如缺乏普适性和领域多样性。

鉴于过去的研究和现有的基准数据集的限制，本研究提出了 ==COCO-O== 数据集，旨在评估目标检测器在自然分布偏移下的鲁棒性。通过对该数据集上的实验，可以更全面地评估现代目标检测器的鲁棒性，并探索影响鲁棒性的因素，从而为未来的目标检测算法提供指导。

> 方法：

测试数据种类（如下右图 6 种）：

|                         | COCO-O | Sketch | Weather | Carton | Painting | Tatto | Handmake |
| ----------------------- | ------ | ------ | ------- | ------ | -------- | ----- | -------- |
| images                  | 6783   | 992    | 961     | 1996   | 954      | 918   | 961      |
| labelled bounding boxes | 26624  | 3707   | 4509    | 8774   | 4879     | 3266  | 3266     |

![image-20231023193558116](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231023193558116.png)

数据收集方式：

通过使用来自 COCO 的 OOD 场景关键词和对象类别的组合来搜索互联网来收集COCO-O图像。例如，“卡通+狗”的目的是收集动画狗的图像。一般来说，“卡通+dog”搜索的大多数图像都是标志性的（单个高质量的物体以图像为中心，很容易定位）。为了获得更多的非标志性图像，遵循 COCO 中使用的方法，在关键词组合中添加更多的对象类别，如“卡通+dog+car”。手动控制每个关键字组合检索到的图像数量，以确保类别之间的平衡。对于只返回少量图像的组合，如“fog+bowl+tv”，尝试使用多个搜索引擎来收集更多的图像。

评价标准：

Effective Robustness

![image-20231023194929168](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231023194929168.png)



**分析检测器架构、增强方式、预训练对 OOD 鲁棒性的影响**

评价结果：

**Robustness vs. Detection Architecture**：

具有更高的 COCO mAP 的检测架构并不意味着更好的鲁棒性；

相比于 neck 和 head 的先进技术对鲁棒性的影响，backbone 扮演着更重要的角色。

**Robustness vs. Augmentations**：

所有的增强方式都有助于增强模型的健壮性；

MixUp 对于实现目标检测的领域泛化能力效果最好。

![image-20231023195323630](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231023195323630.png)

**Robustness vs. Pre-training**：

当使用 ImageNet-1K 进行预训练时，检测器变得更加鲁棒，所获得的鲁棒性取决于一个适当的预训练方法。

![image-20231023195404078](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231023195404078.png)

## 20231103

### 19_Anomaly Detection under Distribution Shift_ICCV 2023_有代码

> 作者：Tri Cao, Jiawen Zhu, and Guansong Pang*

> 代码：https://github.com/mala-lab/ADShift

> 贡献：

背景: 异常检测旨在从一组正常训练样本中学习模式，以识别测试数据中的异常样本。然而，现有的异常检测方法通常假设训练和测试数据来自相同的数据分布，而在实际应用中，由于不同的自然变化（如光照条件、物体姿态或背景外观），测试数据可能存在较大的分布偏移，导致现有方法在这种情况下效果不佳。

 过去的研究主要集中在无监督的异常检测方法上，如基于单类分类、基于重构的方法和自监督学习方法。然而，这些方法都假设训练和测试数据来自相同的分布，无法有效应对分布偏移的问题。

鉴于现有方法在分布偏移下的异常检测问题上的不足，本研究旨在提出一种新的方法，通过学习 generalized normality 表示来解决分布偏移下的异常检测问题。该方法在无监督的情况下**最小化训练和推断阶段中的分布差异**，从而能够在各种分布偏移的数据上取得更好的性能。

> 方法：

generalized normality learning（==GNL==）由两个主要组成部分组成：分布不变正态性学习和分布偏移下的异常检测的测试时增强。
GNL 建立在 RD4AD 模型的基础上，在分布不变 normality 学习中，引入了保持 normality 的损失函数 $L_{abs}和L_{lowf}$。该损失函数量化了原始样本的嵌入特征与每个转换的正常样本的嵌入特征之间的差异。该损失函数加在编码器的瓶颈层和最终块处。通过最小化这个损失函数，GNL能够学习到分布不变的 normality 表示。
测试时增强组件解决测试过程中的分布偏移问题。GNL 使用特征分布匹配（FDM）在教师编码器的多层级层次上将训练分布注入到推理样本中。这种增强应用于教师编码器的前两个残差块。推理过程和异常得分计算遵循原始 RD4AD 框架。

![image-20231017190451944](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231017190451944.png)

**Distribution-invariant Normality Learning**

作者通过加入一个相似性损失，来量化原始样本的嵌入特征和每个转换后的正常样本的嵌入特征之间的差异，这代表了与原始数据不同的风格。具体来说，在瓶颈层和解码器的最后一个块上都计算这种损失：

![image-20231017190734900](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231017190734900.png)

![image-20231017190746014](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231017190746014.png)

**Test Time Augmentation for Anomaly Detection under Distribution Shift**

为了解决测试期间数据分布之间不匹配的问题，作者提出在推理阶段使用教师编码器的多层特征分布匹配（FDM），将训练分布注入到推理样本中。所提出的测试框架如图3 (b) 所示，测试时增强被应用于教师编码器的前两个残差块。从第三个残差块开始的推理过程，以及异常评分的计算，遵循原始的RD4AD框架，没有任何修改。

<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231017190929929.png" alt="image-20231017190929929" style="zoom:80%;" /><img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231017190943632.png" alt="image-20231017190943632" style="zoom:80%;" />

> 总结：

* 在测试阶段，使用训练样本的随机挑选的 normal 样本来调整测试时的样本（公式4，5），以解决测试期间数据分布之间不匹配的问题

### 20_Unified Out-Of-Distribution Detection: A Model-Specific Perspective_ICCV 2023_无代码

> 作者：Reza Averly Wei-Lun Chao

> 贡献：

背景: 在可靠的机器学习中，使模型具备识别 “自己不知道的东西” 的能力至关重要。然而，现有的模型在面对具有协变量偏移（例如不同图像领域或风格）或语义偏移（例如新的类别）的数据时，往往准确率大幅下降。

过去的离群检测方法主要关注语义偏移导致的离群样本，忽视了其他可能的原因，如协变量偏移。这限制了这些方法在非受控环境中应用的适用性。

鉴于现有方法的局限性，本文试图扩展离群检测的范围，进一步包括协变量偏移。通过从“模型特定” 的角度定义离群样本，提出了一种统一的框架 ==MS-OOD Detection==，能够更好地研究不同原因导致的离群样本，并更好地适应实际应用场景。

> 方法：

MS-OOD Detection 将测试样本分为两种情况：模型特定接受（**MS-A**）和模型特定拒绝（**MS-R**）。MS-A 包含由部署的分类器正确分类的ID和协变量偏移样本；MS-R 包含被错误分类的 ID 和协变量偏移样本以及语义偏移样本。MS-OOD Detection 的目标是区分 MS-A 和 MS-R 样本。

![image-20231025130153385](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231025130153385.png)

本文研究了影响 MS-OOD Detection 性能的三个维度：**离群样本的来源**、**部署的分类器**和**离群检测方法**。

ID/OOD 样本的来源包括 ImageNet-1K （用于ID数据），SVHN、 Texture (DTD) 、Places365等用于S-OOD数据，具有不同程度和类型的协变量偏移的数据集用于 C-OOD 数据；

不同的神经网络模型：包括ResNet50、ResNet18、ResNet152、鲁棒ResNet50、CLIP-ResNet50和ViT-B-16；

使用的后处理的检测方法，包括基于输出、基于特征和混合方法。

结论：

1. CLIP-ResNet50 模型相对于基准的 ResNet50 模型对于 C-OOD 样本更具鲁棒性。然而，对 C-OOD 样本进行分类的准确性仍然不完美，表明接受所有 C-OOD 样本存在风险。另一方面，拒绝所有 C-OOD 样本将导致浪费，除了在 ImageNet-A 数据集上的 ResNet50 模型。结果表明需要接受正确分类的 C-OOD 样本，同时拒绝错误分类的样本。

![image-20231025130707589](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231025130707589.png)

2. 对于 ID 样本，分类 ID 数据的准确性越高，错误接受错误分类的 ID 样本的假阳性率越低。然而，使用更强的模型会导致更高的假阳性率。MSP 方法在区分正确分类和错误分类的ID样本方面表现优于其他检测方法。

   ![image-20231025130934472](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231025130934472.png)

3. 对于 C-OOD 样本，分类 C-OOD 数据的准确性越高，识别正确分类的 C-OOD 样本的 F1-Score 越高。MSP 在区分 C-OOD+ 和 C-OOD- 数据方面表现最好。

   ![image-20231025131059478](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231025131059478.png)

4. 对于 S-OOD 样本，分类 ID 数据的准确性越高，错误接受 S-OOD 样本的假阳性率越低。然而，使用更强的模型会导致更高的假阳性率的例外情况。在不同的检测方法中没有明确的优胜者。

   ![image-20231025131120587](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231025131120587.png)

> 总结：

协变量偏移：不同图像领域或风格 ----> 域变化 ----> 域增量

语义偏移：新的类别 ----> 类变化 ----> 类变化

## 20231110

### 21_Enhancing Your Trained DETRs with Box Refinement_arxiv 2307_有代码

> 作者：Yiqun Chen1,2 Qiang Chen3 Peize Sun4 Shoufa Chen4 Jingdong Wang3 Jian Cheng1

> 代码：https://github.com/YiqunChen1999/RefineBox

> 贡献：

本文的动机是改进DETR模型的定位能力。作者观察到，改进定位能力可以带来显著的性能提升，而改进分类能力的提升效果较低。因此，作者提出了一种新的框架 **RefineBox**，通过细化网络对训练好的 DETR 模型的预测框进行改进，从而提高定位质量。

作者观察到一种现象，即多级级联的 decoder 存在后期降低真阳性，升级假阳性的情况。因此作者提出以下两种猜想：

1. 在训练过程中，所有阶段都以同等的方式进行监督，缺乏将重点放在训练后期阶段的机制；
2. 简单的级联结构会造成错误的积累，顺序结构无法让后期训练阶段从前序阶段获得更有意义的信息（目前是，后续阶段进对前一个阶段的信息全盘接收）。

RefineBox 通过向训练良好的模型添加轻量级的细化网络，而不是从头设计和训练新模型。该方法易于实现和训练，因为它仅使用来自训练良好的检测模型的特征和预测框。在训练过程中，训练好的检测器被冻结，使得该方法高效。

RefineBox 可以轻松应用于各种训练好的检测模型。实验结果表明，RefineBox 在 COCO 和 LVIS 1.0 数据集上的效果显著，DETR、Conditional-DETR、DAB-DETR和DN-DETR 的性能分别提高了 2.4 AP、2.5 AP、1.9 AP 和 1.6 AP。

> 方法：

作者分别将定位和分类的预测设置为 gt ，以观察这两个子任务在理想情况下的性能：

![image-20230804162650178](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230804162650178.png)

如上图可见，消除定位误差会使模型得到很大的改进，而消除分类错误会带来的收益小很多。因此得出定位是当前 DETR 类模型的瓶颈：**定位误差限制了提高分类能力的可能**。即使是具有理想分类性能的模型，也只能带来有限的收益。

因此作者提出一个轻量级的网络来优化 DETR 类模型的定位和分类能力：

**RefineBox**（如下图）包含了两个组成部分： 一个对象检测器和一个用于细化预测的边界框的细化网络。

![image-20230804163021097](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230804163021097.png)

细化网络主要执行两个步骤：

首先，细化网络从训练后的检测模型的主干中提取特征金字塔；

其次，它通过一系列细化器模块有效地利用多尺度特征来细化模型预测的边框。

在训练过程中，冻结了训练好的目标检测器的参数，只更新 FPN 和 Refiner 模块的权重。

每个 Refiner 模块包括一个ROI对齐层、一个残差模块和一个多层感知器。残差模块由几个瓶颈块组成。

![image-20230804155810410](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20230804155810410.png)

RefineBox 框架专注于细化预测的边界框，不修改分类结果。训练 RefineBox 框架使用**回归损失**，具体包括 GIoU 损失和 L1 损失。在推理过程中，选择具有最高分类分数的前K个预测进行细化。RefineBox 框架引入了约 0.5M 个参数，并产生了显著的改进。

> 总结：

* 通过消除定位差异和分类差异来看模型性能的提升情况发现，提高定位的性能具有很大潜力（还有很大上升空间）-> 定位能力是当前 DETR 类检测模型的瓶颈；
* 定位框细化的思想可以考虑借鉴，比如在训练过程中加一个细化分支；

### 22_Anomaly Heterogeneity Learning for Open-set Supervised Anomaly Detection_arxiv 2310_有代码链接

> 作者：Jiawen Zhu, Choubo Ding, Yu Tian, Guansong Pang

> 代码：https://github.com/mala-lab/AHL

> 贡献：

背景: 异常检测是识别与大多数数据显著不同的数据点的任务，具有广泛的应用领域。由于异常数据的收集困难，大多数现有的异常检测方法将其视为一分类问题，只使用正常样本进行训练。然而，在许多应用中，通常存在一些可访问的异常样本，如以往工业检查中发现的缺陷样本和以前患者的肿瘤图像。这些异常样本提供了关于异常的重要先验知识，但现有的方法没有利用它们。目前已经提出了一些用于开放式监督异常检测的方法，通过利用有限的训练异常数据来学习用于检测未知异常的广义模型。然而，这些方法通常认为异常样本来自**同质分布**，限制了它们在检测未知异常样本的性能。

![image-20231108211606136](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231108211606136.png)

鉴于现有方法对异常样本的同质性假设存在的缺陷，本文提出了一种学习**异质异常分布**的方法 ALH，以更好地适应未知异常，并提高开放式监督异常检测的性能。通过模拟多样化的异常分布并学习统一的异质异常模型，AHL 方法能够在检测已知异常和未知异常方面取得更好的性能。

> 方法：

AHL 框架由两个主要组成部分组成：**HADG**（Heterogeneous Anomaly Distribution Generation ）和 **CDL**（Collaborative Differentiable Learning of the anomaly heterogeneity ）

HADG 通过将正常样本划分为簇，并将每个簇与随机抽样的异常样本关联起来，学习不同的异常分布，从而创建 T 组训练异常分布数据集，每个数据集包含正常样本和异常样本的混合；

CDL 旨在通过在生成的数据集上训练 T 个基础模型，并协同优化它们，学习异构异常分布。基础模型捕捉不同的异常分布，统一模型则以在各种可能的异常分布上表现良好为目标进行优化。通过动态估计每个基础模型的重要性，增强协同优化过程。

![image-20231108210832061](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231108210832061.png)

> 总结：

* 结合元学习以及集成学习的思想；
* 通过共同学习多个检测带有某种偏向的 base model（比如：与正常样本簇A混合在一起的异常样本，可能相对来说，相较于 B 类正常样本，和A类正常样本在一起时更容易被检测出来），从而使得 final model 对多种可能的异常样本具有检测能力；

## 20231117

### 23_Intra-Modal Proxy Learning for Zero-Shot Visual Categorization with CLIP_ NeurIPS'23_有代码

> 作者：Qi Qian, Yuanhong Xu, Juhua Hu

> 代码：https://github.com/idstcv/InMaP

> 贡献：

背景: 视觉-语言预训练方法（如CLIP）在使用文本嵌入的类别代理进行零样本视觉分类方面取得了令人印象深刻的性能。然而，文本和视觉空间之间的**模态差异**可能导致性能下降。

 过去的方法主要集中在改进零样本学习本身，其中一些方法利用大型预训练语言模型（如GPT-3）来获得更好的文本代理，而另一些方法则利用目标领域的无标签数据进行微调。然而，这些方法并未解决文本和视觉空间之间的模态差异问题。

 鉴于 CLIP 模型和零样本传输中不使用任何辅助网络，本研究旨在理解 CLIP 的行为，并提出一种名为 InMaP 的方法，通过学习**视觉代理**来改善 CLIP 在零样本视觉分类中的性能，减少文本和视觉空间之间的模态差异，从而改善零样本视觉分类的性能。该方法能够在单GPU上一分钟内从图像数据中获取视觉代理，并将零样本准确率从 77.02% 提高到 80.21%。

> 方法：

InMaP 通过学习视觉代理而不是直接使用文本代理来解决 CLIP 中文本和视觉空间之间的模态差距。

为了提高在视觉空间中的零样本性能，作者考虑直接获得视觉代理，而不是使用文本代理。主要的挑战来自于标签是不可用的。幸运的是，可以通过模拟其他模态的代理来学习模态内代理。**通过最小化文本代理和可学习视觉代理估计的分布之间的 KL 散度**，可以有效地获得最优视觉代理。

![image-20231114141934198](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231114141934198.png)

文中的定理2表明，恢复的视觉代理与最优视觉代理之间的距离受文本代理预测的伪标签与地面真标签分布之间的差距的限制。在大规模数据集上对 CLIP 模型进行预训练后，可以近似于真实数据的分布，从而可以学习模态内代理。通过估计的文本代理预测的伪标签和真实标签分布之间的差距来限制恢复的视觉代理与最优视觉代理之间的距离。

伪标签细化：

![image-20231114142447842](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231114142447842.png)

InMaP算法的详细步骤可以参考算法1。

![image-20231114141526267](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231114141526267.png)

> 总结：

利用文本信息提供伪标签来训练图像空间的分类器

### 24_LEVERAGING CROSS-MODAL NEIGHBOR REPRESENTATION FOR IMPROVED CLIP CLASSIFICATION_Under review at ICLR 2024_无代码

> 作者：

> 贡献：

背景：一些现有研究直接使用CLIP的图像编码器进行少样本分类等任务，但这种方法忽视了CLIP的多模态能力，导致与其预训练目标的不一致。此外，由于CLIP没有针对单模态场景进行优化，其在单模态任务中的性能也无法保证。

本研究的动机是要利用 CLIP 强大的多模态能力来**提取更好的图像特征**，旨在改进CLIP在单模态特征提取方面的性能，从而提高 CLIP 在下游任务中的性能。为了实现这一目作者**将文本特征视为 CLIP 空间中图像特征的精确邻居**，并提出了一种基于图像与其邻居文本之间距离结构的 CrOss-moDal nEighbor Representation (==CODER==)方法。通过构建高质量的文本样本与图像的邻居关系，CODER 能够更好地与 CLIP 的预训练目标相一致，充分发挥 CLIP 的跨模态能力。

> 方法：

本文引入 CrOss-moDal nEighbor Representation (CODER) 来构建基于 CLIP 特征空间内图像与文本之间距离关系的图像表示。

CODER 纠正了图像之间错误的距离关系，并改善了特征表示的对齐性。如下图：

![image-20231114162820984](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231114162820984.png)

文章还讨论了构建良好的 CODER 的密集采样邻居样本的重要性，并提出了**降维**和**增加文本数量**作为优化技术。为了构建 CODER，引入了自动提示生成器（APG）模块，用于生成构建 CODER 所需的高质量提示。APG 根据类别名称、属性、类比类别、同义词和1v1比较生成提示。

![image-20231114162911313](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231114162911313.png)

> 总结：

本文没有像以往的方式去增加文本prompt的多样性（例如CoOp系列，让文本 embedding 部分可学习），而是通过添加文本的多样性，利用 GPT 生成多种相关文本提示来增强 CLIP 的分类能力。

## 20231124

### 25_Hyperbolic Vision Transformers: Combining Improvements in Metric Learning_CVPR 2022_有代码

> 作者：Aleksandr Ermolov, Leyla Mirvakhabova, Valentin Khrulkov, Nicu Sebe, Ivan Oseledets

> 代码：https://github.com/htdt/hyp_metric

> 贡献：

背景: 度量学习旨在学习一个高度区分性的模型，鼓励相似类别的嵌入在选择的度量中靠近，而不相似类别的嵌入则被推开。

过去方案通常使用编码器提取嵌入和基于距离的损失函数来匹配表示，通常使用欧几里得距离。

本文提出了一种新的基于超几何的度量学习模型。该模型的核心是一个**将输出嵌入映射到超几何空间**的视觉Transformer。这些嵌入直接使用改进的成对交叉熵损失进行优化。通过在四个数据集上进行实验评估，本文提出的模型在六种不同的配方中取得了最新的性能表现。

> 方法：

本文提出了一种新的度量学习损失函数，将双曲空间的表达能力与交叉熵损失函数的简单性相结合，将嵌入投影到庞加莱球并使用双曲距离。**该损失函数在双曲空间中作用**，鼓励相似对象的嵌入更加接近，同时将不相似对象的嵌入推开。

![image-20231120110344692](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231120110344692.png)

**Hyperbolic Embeddings**

n 维双曲空间 $H^n$是一个常负曲率的黎曼流形。双曲空间存在几种等距模型，在本文的工作中，使用的是庞加莱球模型$(D^n _c，g^D)$，曲率参数： c（实际曲率值为c^2）。

![image-20231120110652327](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231120110652327.png)

局部距离是由球边界附近接近无穷大的因子 $λ_c$缩放的。这就产生了双曲空间的“空间膨胀”性质。在欧几里得空间，一个直径为 r 的物体的体积在空间中呈多项式尺度。在双曲空间中，体积随 r 呈指数尺度变化。

双曲空间不是向量空间，为了能够执行加法等操作，需要引入陀螺向量形式主义。对于一对$x，y∈D^n_c$，加法被定义为：

![image-20231120110903723](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231120110903723.png)

相应的，双曲距离：

![image-20231120110932997](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231120110932997.png)

当 c 趋于 0 时，双曲距离相当于欧式距离。

需要定义一个从欧几里得空间与庞加莱模型的双映射。从欧几里得空间到庞加莱模型的映射被称为指数，而从双曲空间到欧几里得的逆映射则称为对数映射。

对于固定的基点 $x∈D^n_c$，指数映射是一个函数定义为：

![image-20231120111214623](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231120111214623.png)

映射到双曲空间的特征表示为 $z_i$

基点 x 通常设为 0 以简化公式，经验表明这对得到的结果影响很小。

**Pairwise Cross-Entropy Loss**

![image-20231120111423842](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231120111423842.png)

***δ*-hyperbolicity**

使用  Gromov *δ* 方法评估数据双曲性的“度量”：

![image-20231120111849325](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231120111849325.png)

> 总结：

* 双曲空间能有效地嵌入分层数据，即使是在低维空间中

### 26_Enhancing Few-shot CLIP with Semantic-Aware Fine-Tuning_arxiv 2311_无代码

> 作者：Yao Zhu, Yuefeng Chen, Wei Wang, Xiaofeng Mao, Xiu Yan, Yue Wang, Zhigang Li, Wang lu, Jindong Wang, Xiangyang Ji

> 贡献：

背景: 在资源有限的情况下，从有限的训练样本中学习泛化表示对于应用深度神经网络至关重要。以CLIP为基础的方法在 Few-shot 任务中表现出色，但现有方法通常冻结在大规模数据集上预训练的 CLIP 参数，忽视了某些参数可能不适用于下游任务的可能性。

本研究从语义注意力的角度探讨了 CLIP 的可微调性，并提出方法 **SAFE**（Semantic-Aware FinE-tuning），通过对 CLIP 的注意力池化层进行微调来引导模型关注任务特定的语义信息，以增强 Few-shot CLIP 的适应性和可解释性。

SAFE 旨在增强 CLIP 模型的少样本适应性能。该方法主要关注 CLIP 的视觉编码器中的注意力汇聚层，该层对密集特征图进行空间加权求和。作者认为，在不同的少样本任务中，使用相同的加权求和操作可能不合适，因为不同的语义对不同的下游任务具有不同的重要性。为了解决这个问题，提出在训练过程中对注意力汇聚层的参数进行微调，**以鼓励模型关注任务特定的语义**。在推理过程中，通过在经过微调的注意力汇聚层和原始注意力汇聚层之间进行残差混合，将少样本知识和预训练CLIP的先验知识结合起来。

> 方法：

作为人类，我们能够通过匹配给定图像的语义特征与我们已知的类别之间的对应关系来识别给定的新图像。例如，我们使用眼睛、耳朵和鼻子等语义特征来确定一个物体是否是猫。问题是，对比语言-图像预训练（CLIP）能否学习图像之间的语义对应关系，并利用这些信息来提高 Few-shot 的性能？Zeiler和Fergus [47]，Mehrer等人的[48]表明，早期的神经网络层（更接近输入）专注于局部特征，如边、角和线。因此，作者研究了 CLIP 模型的**深度特征**（而不是早期特征）是否表现出广义的语义，可以用来建立不同图像之间的对应关系。

![image-20231120150836749](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231120150836749.png)

SAFE 主要包括**训练阶段使用少样本微调 clip 的注意力池化层**（对应 resetnet 的backbone，如果是 transformer 的backbone，则微调最后一个transformer 层）以及推理阶段对微调后的注意池层和原始的注意池层之间进行残差混合，以纳入来自训练前的先验知识和 few-shot 微调学到的知识。

![image-20231120151204403](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231120151204403.png)

**算法流程：**

![image-20231120151421294](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231120151421294.png)

> 总结：

为了根据密集特征中不同部分的权重对下游任务的意义，对其进行修正，作者提出使用 few-shot 训练样本对 CLIP 的注意池层进行微调。在 few-shot 训练样本的指导下，注意池层更关注密集特征中的特定任务语义，而不是像现有方法那样在不同任务中采用相同的注意机制。

## 20231201

### 27_Guiding Pseudo labels with Uncertainty Estimation for Source-free Unsupervised Domain Adaptation_CVPR 2023_有代码

> 作者：Mattia Litrico, Alessio Del Bue, Pietro Morerio

> 代码：https://github.com/MattiaLitrico/Guiding-Pseudo-labels-with-Uncertainty-Estimation-for-Source-free-Unsupervised-Domain-Adaptation

> 贡献：

传统的无监督域自适应方法需要在自适应过程中访问源域数据，而无源无监督域自适应方法则无法使用这些方法，因此需要新的解决方案。

 本研究的动机在于解决无源无监督域自适应中伪标签的噪声问题，提出了一种**基于不确定性估计的伪标签引导策略**，用于无源无监督域自适应。以增强对伪标签噪声的鲁棒性。通过估计伪标签的不确定性来重新加权分类损失。通过从相邻样本中聚合知识来逐步改进伪标签。此外，还利用自监督对比框架作为目标空间正则化器来增强知识聚合。引入了一种负样本排除策略，用于识别和排除由具有相同类别的样本组成的负样本对，即使伪标签中存在噪声也能实现。通过对伪标签的可靠性进行评估，减轻了伪标签中的噪声影响，并在三个主要基准数据集上取得了显著的性能提升。

> 方法：

自适应过程从使用训练好的源模型为未标记的目标数据生成伪标签开始。然后通过从最近邻样本中聚合知识来改进伪标签。使用软投票策略对所选邻居的概率输出进行平均，得到改进的伪标签。改进过程基于相似样本应具有相同标签的假设。为了估计改进的伪标签的不确定性，引入了一种基于熵的不确定性估计方法。计算平均分数向量的熵，并根据熵值为每个样本分配权重。在训练过程中使用权重对分类损失进行重新加权。此外，使用一个时间队列来在对比训练过程中排除负样本对。

![image-20231127210217974](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231127210217974.png)

不确定性的评估方式：

![image-20231127210252778](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231127210252778.png)

排除由具有相同类别的样本组成的负样本对：

![image-20231127210509149](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231127210509149.png)

### 28_Proposal-Level Unsupervised Domain Adaptation for Open World Unbiased Detector__有代码

> 作者：Xuanyi Liu, Zhongqi Yue, Xian-Sheng Hua

> 代码：https://github.com/lxycopper/PLU

> 贡献：

过去的开放世界目标检测方法通常采用基于top-k选择策略的伪标签生成方法，但这种方法存在**刚性**和**偏见**的问题，无法灵活处理不同情况，并且预测器对已知类别有偏见。

本文提出了一种面向开放世界无偏检测器的 proposal 级无监督领域自适应方法Proposal-Level Unsupervised Domain Adaptation（==PLU==），通过重新定义任务并采用自训练方法，构建了一个无偏的前景预测器，实现了对外观变化的鲁棒预测，并在开放世界目标检测任务中，在基于Faster-RCNN框架和DDETR框架的实验中展示了其最先进的性能。

如下图：检测器倾向于给与已知类（苹果）有相似外观的未知物体更高的分数。

![image-20231127172559355](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231127172559355.png)

> 方法：

（PLU）旨在通过重新制定无监督域自适应（UDA）任务来构建一个**无偏的前景/背景（FG/BG）预测器**。该方法涉及对已知和未知类别进行标记，使用UDA的自训练方法，并将UDA模块扩展到各种目标检测框架中。

**Domain 的划分**

源域：与 gt 匹配的 proposals + 背景置信度很高的 proposals

目标域：除源域之外的 proposals

![image-20231127172811780](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231127172811780.png)

**PLU Module**：

![image-20231127173546028](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231127173546028.png)

训练：

![image-20231127173608268](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231127173608268.png)

![image-20231127173615346](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231127173615346.png)

![image-20231127173621910](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231127173621910.png)

算法流程：

![image-20231127173451625](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231127173451625.png)

> 总结：

感觉本质上还是依据已知类别的特征来选择未知类伪标签，但是本文的方法域之前 topk 的方法相比，最大的区别在于扩大了可参考的样本量（原来 topk 方法的可参考样本量是当下一张图（或者说此前见过的图）的已知类，本文的可参考样本量是整个任务的已知类），所以训练出来的 FG/BG 预测器的准确率会更好一些。

## 20231208

### 29_Rejuvenating image-GPT as Strong Visual Representation Learners_xx 2023_有代码

> 作者：Sucheng Ren, Zeyu Wang, Hongru Zhu, Junfei Xiao, Alan Yuille, Cihang Xie

> 代码：https://github.com/OliverRensu/D-iGPT

> 贡献：

背景: 大语言模型（LLMs）在自然语言处理（NLP）领域取得了巨大成功，但自回归预训练在计算机视觉领域的进展尚未达到同样的高度。

本研究的动机在于挖掘自回归预训练在构建强大视觉学习模型方面的潜力，通过对 Image-GPT 进行关键改进，提出了 D-iGPT，证明了其在视觉表示学习中具有出色的能力。

> 方法：

![image-20231208112227915](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231208112227915.png)

对于 iGPT 的两个关键改进：

首先，将预测目标从原始像素转移到**语义标记**，从而实现对视觉内容的更高级理解。

与文本固有的语义丰富的本质相比，图像中的原始像素通常缺乏这样的意义深度。在iGPT等模型中，解决这种语义差异对于提高学习效率至关重要。为了弥补这一差距，本文的方法受到BEiT的启发，将D-iGPT的自回归目标从原始像素转换为语义标记，可以写成：

![image-20231208112350970](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231208112350970.png)

f 时对应图片区域的语义编码器。

其次，模型不仅训练预测下一个标记，还训练预测可见标记，以补充自回归建模。

为了进一步加强模型的训练，引入了针对可见集群的额外监督。这种方法的定义如下：

![image-20231208112540118](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231208112540118.png)

这两种修改的整合显著提高了 iGPT 对视觉表征学习的能力。虽然模型 fϕ(x) 有各种选项来生成语义标记，但作者发现，像CLIP这样的区别训练模型就比较好用。

### 30_Segment Every Out-of-Distribution Object_xx 2023_无代码

> 作者：Wenjie Zhao, Jia Li, Xin Dong, Yu Xiang, Yunhui Guo

> 贡献：

现有的语义分割中的 OoD 检测方法通过为每个像素分配一个异常分数来解决这一问题。异常得分高的像素将被视为 OoD 对象的一部分。异常分数通常是由分割模型对每个像素进行的概率预测得出的。虽然基于异常评分的OoD检测方法能够准确地识别异常像素，但它们缺乏分割整个 OoD 对象的有效方法。特别是，为了推导出 OoD 对象的掩模，使用一个精心选择的阈值来区分异常像素和正常像素是至关重要的。然而，在实际应用中确定最优阈值可能是一项困难的任务。

本文介绍了一种将异常分数转换为分割掩码的方法 S2M，这是一种简单而有效的语义分割中的 OoD 检测框架。与给像素分配异常分数不同，S2M 直接分割整个 OoD 对象。通过将异常分数转换为可提示分割模型的提示，S2M 消除了阈值选择的需要。

> 方法：

![image-20231208135130732](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231208135130732.png)

利用简单有效的框架处理语义分割中的面向对象检测问题，解决了现有的基于异常评分的OoD检测方法要提供像素级的异常评分的固有局限性。虽然这些异常分数可以表明给定像素是否可能属于一个 OoD 对象，但准确获取整个 OoD 对象的分割掩码是困难的。相比之下，本文提出的 S2M 利用异常得分图来创建方框提示，作为 OoD 对象存在的信号。该方框提示作为可提示分割框架的输入，该框架处理这些提示和原始图像，以为 OoD 对象生成掩码。

![image-20231208135627184](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231208135627184.png)

在推理过程中，使用现有的 OoD 检测方法对输入图像进行处理，以计算出异常得分图。然后将此映射输入提示生成器，从而生成指示潜在 OoD 对象位置的边界框提示。随后，我们采用了一个可提示的分割模型，它将提示和原始图像作为输入，并产生 OoD 对象的掩模。如图5所示。由于异常分数高的区域可以被分割，因此提示生成器可能会产生多个方框提示。在这种情况下，将它们全部输入到可提示的分割模型中，并使用所有生成的掩码的联合区域作为最终结果。

## 20231215

### 31_Open World Object Detection in the Era of Foundation Models_202312 _有代码

> 作者：Orr Zohar, Alejandro Lozano, Shelly Goel, Serena Yeung, Kuan-Chieh Wang

> 代码：[FOMO | Orr Zohar](https://orrzohar.github.io/projects/fomo/)

> 贡献：

作者认为由于现有OWD任务严格的基准和任务定义，限制了基础模型的使用，OWD方法的发展受到了阻碍。
作者提出现有OWD的数据的一个缺陷，为所谓的“未知物体”是大多数现代检测方法可以很好地检测的日常物体，这让学出来的模型是否真的能够泛化到现实中的未知类存疑。
通过在OWD基准上进行评估，作者发现即使是简单的基础模型也几乎使基准达到饱和状态。因此本文放宽OWD定义，并探究在OWD中利用预训练的基础模型的可行性。新的基准，将来自不同真实应用领域的多个数据集进行了整合，包括水下、空中和医学领域。
作者发现在大多数实际应用中，未知对象与基础类别共享视觉/功能属性。与之前的OWD方法不同，本文将属性属于分布内但在已知类中属于外分布的确定为未知对象。并提出基于 FOMO （Foundation Object Detection Model for the Open world,简称FOMO）的基础目标检测模型的开放世界方法。具体来说，FOMO 学习检测对象的属性以及这些属性与已知对象之间的映射关系，使用少量的对象示例进行训练。为此，FOMO 利用少量的对象示例来选择和完善由大型语言模型首先提出的属性嵌入。

> 方法：

**新基准**

![image-20231214195616542](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231214195616542.png)
RWD基准包含五个现有的应用驱动的目标检测数据集。这些数据集中的类别被分成两个子集：最常见的50％类别和最不常见的50％类别。方法在两个阶段进行评估：任务1（T1）和任务2（T2）。在T1中，只考虑了最常见的50％类别，而最不常见的50％类别对模型来说仍然未知。在T2中，剩下的50％最不常见的类别将被揭示，模型将根据其在先前/当前已知类别集上的性能进行评估。
**FOMO**

FOMO 尝试通过识别与基础类别具有视觉、功能属性共享的对象来检测未知对象。![image-20231214195655537](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231214195655537.png)

**Attribute Generation**
如图 2（i）所示，我们利用大型语言模型为视觉属性生成（文本）候选项。接下来，使用算法1生成一个包含N个与类别无关的属性列表，表示为A。并编码产生属性嵌入矩阵![image-20231214195717153](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231214195717153.png)。通过![image-20231214195732666](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231214195732666.png)计算视觉嵌入和属性嵌入之间的得分。

![image-20231214195818472](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231214195818472.png)

**Attribute Selection and Refinement**
并非所有生成的属性都与手头的问题相关，本文使用已知类别的训练图像来选择相关属性。选择每个对象类的前$\hat{N}$个属性，并删除未用于分类任何对象类的所有属性。
**Unknown Class Inference**
“未知”：在属性方面是分布内，但在已知类别方面是分布外

![image-20231214195921442](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231214195921442.png)

![image-20231214195930710](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231214195930710.png)

最终的未知物体预测分数：![image-20231214195941897](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231214195941897.png)
**实验结果**
在传统owd设定下：

![image-20231214195957665](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231214195957665.png)

在RWD设定下：

![image-20231214200032705](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231214200032705.png)

> 总结：

* 将基础 VLMs 整合到开放世界目标检测任务中
* 将未知类定义为与已知类具有某些相似属性的未见过的类别

### 32_ProxyDet: Synthesizing Proxy Novel Classes via Classwise Mixup for Open Vocabulary Object Detection_AAAI 2023_有代码

> 作者：Joonhyun Jeong, Geondo Park, Jayeon Yoo, Hyungsik Jung, Heesu Kim

> 代码：https://github.com/clovaai/ProxyDet

> 贡献：

背景：作者认为现有的OVD伪标签方法尽管简单，但对真正的新类性能仍然有待改进。

在本文中，作者提出了一种新颖的，但简单的技术，帮助泛化新颖类的总体分布。作者观察到，许多新类位于CLIP嵌入空间中由基类构造的凸包中，受此启发，提出通过**一对基类之间的线性混合**来合成近似新类的代理新类。

<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231215221850024.png" alt="image-20231215221850024" style="zoom:80%;" /><img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231215221903475.png" alt="image-20231215221903475" style="zoom:80%;" />

> 方法：

**ProxyDet: Learning Proxy Novel Classes via Class-wise Mixup**

![image-20231215222238850](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231215222238850.png)

**Synthesizing Proxy-Novel Classes**

给定⼀个批次中基类别的视觉和文本原型 rB 和 T(cB)，随机选择⼀对包含基类别 cBi 和cBj 的类别特定特征的原型 【rBi 和 T(cBi)】 和原型 【rBj 和 T(cBj)】。新类代理由下式得到：

![image-20231215222700541](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231215222700541.png)

V(•) 表⽰L2归一化，λ遵循Beta(γ, γ)分布

**Constructing Prototype**

文本原型：通过将类别名 cBi 作为文本编码器的输入来获得

视觉原型：⼀种简单的解决⽅案是计算所有同类别对应的区域嵌入的质心<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231215223407381.png" alt="image-20231215223407381" style="zoom:67%;" />

然而，简单地平均所有区域嵌入来构建一个质心原型嵌入可能是次优的，因为positive proposal有可能包括低质量的定位结果，如图4所示。因此提出将加权函数F(r)与考虑到proposal质量的度量函数ϕ(•)相结合，ϕ(•)由RPN中的物体置信度或与最近的真实框的IoU决定。

所以：**Robust Visual Prototype**中的F(r)：<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231215223446443.png" alt="image-20231215223446443" style="zoom:67%;" />

<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231215223135924.png" alt="image-20231215223135924" style="zoom:67%;" />

**Proxy Loss**

Proxy Loss的目标是将代理新颖类别的区域嵌入与代理新颖类别的文本嵌入之间的距离最小化，从而有效让模型探索新颖类别的嵌入空间。<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231215223853826.png" alt="image-20231215223853826" style="zoom:80%;" />

**Inference**

代理损失旨在提高新类的分类性能，其目标不同于传统的BCE损失，后者优先考虑基类的性能。为了防止这两个损失之间的潜在竞争，作者将每个损失分别应用于具有相同架构的两个独立的探测器头上。在推理阶段，我们通过计算与所有类的文本嵌入的点积相似度，来计算各自头部的区域嵌入的分类分数。然后用几何平均值合并这两个分类分数，如下：

![image-20231215223657038](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231215223657038.png)

> 总结：

闪光点：原型的计算方式，考虑不同嵌入的质量
## 20231222

### 33_Simple Image-level Classification Improves Open-vocabulary Object Detection_AAAI 2024_有代码

> 作者：Ruohuan Fang, Guansong Pang, Xiao Bai

> 代码：https://github.com/mala-lab/SIC-CADS

> 贡献：

最近的 OVOD 方法着重于通过例如区域级知识蒸馏、区域提示学习或区域文本预训练等方式，将图像级预训练视觉语言模型（VLMs）如CLIP适应到区域级对象检测任务中，以扩展检测词汇。这些方法在识别区域视觉概念方面表现出色，但在利用VLMs从十亿级图像级文本描述中学到的出色全局场景理解能力方面较弱。这限制了它们在检测来自新的/基本类别的外观模糊、遮挡或模糊的困难对象时的能力，这些对象的检测严重依赖上下文信息。
为了解决
这个问题，作者提出了一种新的方法，即Simple Image-level Classification for Context-Aware Detection Scoring（==SIC-CADS==），从全局角度利用CLIP产生的出色全局知识来补充当前OVOD模型。
SIC-CADS的核心是一个多模态多标签识别（MLR）模块，该模块从CLIP中学习基于物体共现的上下文信息， 以识别场景中所有可能的物体类别。这些图像级MLR分数可以用来优化当前OVOD模型的实例级检测分数，从而检测出那些难以检测的对象。

> 方法：

本文旨在利用CLIP的图像级/全局知识，以更好地识别难以检测的对象，实现更高效的OVOD。首先通过从CLIP传输全局多模态知识，训练一个多标签识别（MLR）模块，以识别整个场景中的所有现有类别。然后在推理过程中，MLR模块可以通过检测得分优化过程与现有的OVOD模型进行集成，以提高检测性能。

![image](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image.png)

**CLIP-driven Multi-modal MLR Modeling**

文本MLR分支：该分支将$e^{global}$与从CLIP的文本编码器中得到的不同类别的文本嵌入进行对齐;
视觉MLR分支: 该分支将全局图像嵌入从CLIP的图像编码器中提取出来。
在推理过程中，MLR对于识别新颖类别的能力较弱，因为它只是用基类别进行训练，而视觉MLR可以通过
提取CLIP的零样本识别能力来识别基类别和新颖类别。因此，结合两个分支的得分，以实现更好的新颖类
别和基类别的识别性能，这称为多模态MLR。

Multi-modal MLR：

![image-1](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-1.png)

尽管MLR模型不能定位对象，但它提供场景上下文信息和关于整个图像中可能存在的对象类型的先验知识。这种上下文信息增强了区域级别的检测性能，特别是在检测前述困难对象时。因此，利用以下加权几何平均值将图像级别分数$p^{mmlr}$和实例级别分数$p^{ovod}$ 结合起来，如下所示：

![image-2](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-2.png)

> 总结：

借用CLIP图像编码器，以多标签分配为检测前景对象提供先验信息

### 34_Weakly Supervised Open-Vocabulary Object Detection_AAAI 2024_有代码
> 作者：Jianghang Lin, Yunhang Shen, Bingquan Wang, Shaohui Lin, Ke Li, Liujuan Cao

> 代码：https://github.com/HunterJ-Lin/WSOVOD

> 贡献：

本文提出了一种新的弱监督开放词汇目标检测框架，即WSOVOD，以将传统的WSOD扩展到能够检测新概念并只利用图像级注释的多样数据集。
为了实现这一目标，作者探索了三种重要策略，包括**数据集级特征适应**、**图像级显著对象定位**和**区域级视觉语言对齐**。首先进行数据感知特征抽取，生成输入条件系数，并将其应用于数据集属性原型中，以识别数据集偏差并帮助实现跨数据集泛化。其次，提出了一种定位导向的弱监督区域建议网络，利用类别不可知的任意对象分割模型中的高级语义布局来区分对象边界。最后，引入了一个proposal-概念同步的多实例网络，即通过视觉-语义对齐进行对象挖掘和精炼，以发现与概念的文本嵌入相匹配的对象。

> 方法：

如图2所示，图像I首先经过视觉骨干提取全局图像特征$X^{img}$。 然后，数据感知的特征提取器使用$X^{img}$生成用于组合数据集属性原型的系数，得到数据感知特征$X^{daf}$。同时proposal生成器生成对象位置的假设。接下来，RoI池化从全局特征$X^{img}$中裁剪获得池化特征，并通过两个全连接层将其转换为proposal特征$X^{prop} ∈ R^{R×D}$，其中R是图像I中的proposal数量，D是特征向量的长度。然后进一步
将proposal特征$X^{prop}$与数据感知特征$X^{daf}$融合以处理数据集偏差，得到$X^{fuse}$。最后，proposal-概念同步的多实例学习模型接收$X^{fuse}$，通过图像级分类嵌入来发现受图像约束的对象，并对齐对象与概念之间的表示。总体训练目标函数定义如下：
$L_{WSOVOD} = L_{PG} + L_{OM} + L_{IR}$其中，$L_{PG}$是proposal生成器损失函数,而$L_{OM}$和 $L_{IR}$分别是用于proposal-概念同步多实例学习的对象挖掘和实例细化损失函数。

![image-3](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-3.png)

**Data-Aware Feature Extractor**

为了更好地对齐视觉-语言表示，有必要尽可能学习多个类别，然而，个别数据集包含了有限的概念。这
促使我们训练一个检测器来同时应用于多个数据集，以扩大WSOD的检测范围。主要挑战源于不同数据分布的
领域不兼容性。不同数据集的偏差很大程度上可以通过不同数据集的不同目标来解释,这样的数据集偏差会损
害表示学习，简单地组合多个数据集的性能较差。为此，设计了一个数据感知特征提取器（DAFE），用于生成具有不同情景和不同类别的跨数据集学习的广义数据集级特征。 DAFE的关键思路是基于全图像信息捕捉每个数据集的独特可识别的“特征”，并相应地调整建议特征。

**SAM Guided Proposal Generator**
大多数现有的弱监督目标检测（WSOD）方法使用传统的proposal方法，利用低层特征生成区域候选框，从而阻碍了模型在高级语义信息上的端到端学习。作者设计了一个面向定位的弱监督区域建议网络（LOWSRPN），用于识别无类别的潜在目标，并从SAM中进一步传递高级语义布局的知识。
LO-WSRPN与RPN类似有一个带有256个通道的3 × 3卷积层，后跟三个1 × 1 卷积层，用于定位和形状质量估计。为了缓解LOWSRPN的目标proposal在训练的早期阶段非常嘈杂的问题，作者利用SAM在训练期间生成额外的
proposal。SAM所包含的知识不仅有助于丰富高质量的目标proposal，还利用图像分割器中的高级语义布局来区分目标边界。

**Proposal-Concept Synchronized Multiple-Instance Network**

仅通过图像级别的监督来对齐物体级别的视觉-语言表示很具有挑战性。弱监督目标检测（WSOD）通常被
归纳为多实例学习（MIL）问题，并从图像级别的信息中隐含地学习基于实例的分类器。因此，WSOVOD扩展了常见的基于MIL的WSOD框架，以开放词汇的方式挖掘大规模类别概念。

> 总结：

本文和上篇文章都用到了 MIL 来扩充先验知识，是一个可以借鉴的方向。

## 20231229

### 35_SCLIP: Rethinking Self-Attention for Dense Vision-Language Inference_xx 2312_有代码

> 作者：Feng Wang, Jieru Mei, Alan Yuille

> 代码：https://github.com/wangf3014/SCLIP

> 贡献：

由于在密集预测任务中，CLIP经常难以定位图像中的视觉特征，无法给出准确的像素级预测，这使其无法作为广义视觉基础模型。

首先，作者确定了普通CLIP在语义分割中失败的原因是CLIP学习了“空间不变”的视觉特征，这意味着局部特征倾向于对其在图像中的空间位置不变，并且该模型侧重于整体的视觉表示。这导致普通的CLIP能够准确地推理出图片中有水和火烈鸟，但是不能正确的对应位置（如下图左下所示）。然而，在语义分割等密集的预测任务中，我们实际上需要”空间相关“特征，这意味着局部表示应该随着它们在图像中的空间位置而发生变化。

![image-20231225115132435](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231225115132435.png)

在本文中，作者的目标是提高CLIP在语义分割方面的潜力。通过简单地引入一种新的相关自我注意Correlative Self-Attention （CSA）机制来适应密集的预测任务。具体来说，用**CSA**模块替换传统的CLIP视觉编码器**最后一层的自注意块**，并重用了其预先训练好的q、k、v的投影矩阵，提出**SCLIP** (**S**egmentation-adapted **CLIP** model)，从而实现了对CLIP的零镜头语义分割的**无训练自适应方法**。

> 方法：

SCLIP方法的核心概念是通过架构修改，将从CLIP范式中学习到的空间不变的视觉特征转换为协变表示，以便CLIP模型可以推广到密集的预测任务。如图3给出了原本注意力和CSA模块的对比。CSA中：<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231225115736754.png" alt="image-20231225115736754" style="zoom:80%;" />

![image-20231225115300505](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231225115300505.png)

Attn的每个元素$a_{ij}∈[0,1]$度量xi到xj的注意得分，因此高对角线值表明每个局部token主要关注自己的位置，因此每个位置的视觉信息都很好地局部化。这就解释了为什么之前的MaskCLIP可以工作，它强制令$a_{ij} \neq 0，i = j$和$a_{ij} = 1，i = j$。在CSA模块中，当i = j时，$x_iW_r$和$x_jW_r$之间的相关性总是达到最大值。

除了其显著的特征定位能力外，CSA模块还考虑了局部token之间的语义相关性，从而产生鲁棒和平滑的密集预测结果。直观地说，对于每个局部token$x_i$，CSA不仅给$x_i$本身提供很高的注意力分数，也给共享相似语义内容的令牌很高的注意力分数。在图4中可视化了这种效果，对于每个源点，只有语义相似度高的位置被分配有明显的注意，因此在注意源图中可以清楚地识别每个源点对应的对象（如椅子和猫）。

![image-20231225120355546](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231225120355546.png)

**Segmentation-Adapted CLIP Model**

与MaskCLIP保持一致，将CLIP的图像编码器的最后一个Transformer块视为解码层，以实现自适应，同时保持其余组件不变。

![image-20231225120416372](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231225120416372.png)

$W_q$和$W_k$是CLIP预训练好的参数。

 **Ablation Study**-**Projection matrices in correlative self-attention**

结论：不同的方式结果相差不大，CSA具有较强的鲁棒性。

*Identity Projection*：<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231225120744125.png" alt="image-20231225120744125" style="zoom: 67%;" />（与MaskCLIP不一样）

*Ensemble of Random Initializations*：随机初始化几个投影矩阵求平均。<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231225120908035.png" alt="image-20231225120908035" style="zoom: 67%;" />

*Projection with Single* **W**q *or* **W**k：单独使用，作为结合二者的消融

*Learned Projection*：为了充分利用CSA的潜力，专门从每个数据集的训练分割中学习一个投影矩阵。由于可学习的参数很少，该模型能够在很少的情况下很好地收敛于训练样本（对每个数据集使用64个）。

<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231225120710242.png" alt="image-20231225120710242" style="zoom:80%;" />

> 总结：

* 通过约束q和v的投射矩阵，使token特征具有空间相关性。（mark）

### 36_A Simple Knowledge Distillation Framework for Open-world Object Detection_xx 2312_无代码

（**同Detecting the open-world objects with the help of the “Brain”**）

> 作者：**Shuailei Ma**† **, Yuefeng Wang**† **, Ying Wei**† ♣**, Jiaqi Fan**† **,**Xinyu Sun**‡ **, Peihao Chen**‡ **, Enming Zhang**†

> 贡献：

大型预先训练的视觉语言基础模型（VLM，如GLIP）对开放世界有丰富的知识，但受到文本提示的限制，无法定位难以描述的对象。然而，在推理过程中，存在许多没有预定义语言描述的检测场景。

在本文中，作者试图通过将OWOD任务的开放世界知识提炼为一个与语言无关的检测器，来专门化针对OWOD任务的VLM模型。作者观察到，将OWOD中简单的知识蒸馏方法与自动伪标记机制相结合，即使使用少量数据的未知对象检测，也能获得更好的性能。但是对未知物体的知识蒸馏严重影响了对已知物体的传统结构检测的学习，导致灾难性的遗忘。为了缓解这个问题，提出了从视觉语言到单一视觉形态的知识提炼的down-weight loss 函数。同时将定位和识别的学习解耦，以减少已知和未知对象的类别交互的影响。

> 方法：

图1显示了总体框架。对于一个给定的图像𝑥，它首先被同时发送到开放世界的检测器和大型的预训练的视觉语言grounding模型中。该检测器利用输入的视觉特征来预测定位、方框得分和分类。大型的预先训练的视觉语言基础模型从输入中挖掘未知的开放世界知识。已知的gt和未知的提炼知识聚集了开放世界的监督。在训练阶段，根据回归损失、分类和**监督置信度**来匹配预测和开放世界监督。匹配后，根据预测的方框得分选择伪标签，**伪标签可以防止模型完全属于大型预先训练的视觉语言基础模型的知识**，**并帮助它了解提炼知识之外的看不见物体**。然后，利用所有的标签，通过down-weight训练损失函数来训练开放世界检测器。此外，当在每一次引入新的类别时，基于范例回放的微调来减轻学习类的灾难性遗忘，并通过使用存储于所有已知类的平衡样本集进行微调。



![image-20231225125441001](https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231225125441001.png)

> 总结：

* 使用大模型的zero-shot检测首先获取一部分未知类，并用zero-shot结果的置信度为监督训练一个预测器进一步选择k个未知类伪标签。

# 实验

* idea：结合文本和随机
  * 例如，先随机选择 5/10 个候选框，然后再根据选出的候选框与 caption 中名词的相似性来筛选（选择依据，topk 或相似性阈值）最终的 unknown 候选框；
    * 没什么效果，和仅随机相比，效果没有明显提升。
* 若仅使用文本相似性选择伪标签框，unknown recall 较高（24.93），但是对已知类 mAP 影响较大
  * 分析：可能是由于 clip text encoder 编码得到的类别文本特征之间的相似度本来就很高（80个类别各自之间，大部分相似度都在0.8，0.9的数值上），所以导致仅根据相似性选伪标签的话，会以较大的概率误选到包含已知类对象的框，从而造成对已知类学习产生很大偏差。
    * 或许可以试试，先以较小的相似性阈值初步选择候选框，然后再随机选择，最后再加一步高阈值筛选的操作。

|        | t1     | t1    | t1    | t1    |
| ------ | ------ | ----- | ----- | ----- |
|        | WI     | AOSE  | C-m   | UR    |
| randun | 0.0582 | 4,375 | 60.10 | 23.09 |
| r5c0.8 | 0.0625 | 4770  | 59.73 | 23.28 |
| c0.8   | 0.0462 | 1723  | 55.11 | 24.93 |

r5c0.8：先随机选 5 个，然后根据相似性阈值筛选（ > 0.8）

c0.8：仅根据相似性阈值（ > 0.8）选择 unknown 框

* 消融实验思路
  * 多模态信息（caption 文本）引导的未知类伪标签选择（使用相似性阈值+topk_cap）（存在的问题：由于CLIP的训练物料范围极大，导致各类别特征值区分度不够，因此对已知类检测性能影响较大）
  * 在此基础上，结合使用随机选择的方式来选择未知类伪标签，以达到对已知类去偏的效果。（知识再选择）

## 与现有方法对比

（仅包含已被接收的文章202308）

|                                                              |            | Task 1 |        |       |       | Task 2 |        |       |       |       |       | Task 3 |        |       |       |       |       | Task 4 |       |       |
| ------------------------------------------------------------ | ---------- | ------ | ------ | ----- | ----- | ------ | ------ | ----- | ----- | ----- | ----- | ------ | ------ | ----- | ----- | ----- | ----- | ------ | ----- | ----- |
|                                                              |            | WI     | AOSE   | C-m   | UR    | WI     | AOSE   | m-AP  |       |       | UR    | WI     | AOSE   | m-AP  |       |       | UR    | m-AP   |       |       |
|                                                              |            |        |        |       |       |        |        | P-m   | C-m   | B     |       |        |        | P-m   | C-m   | B     |       | P-m    | C-m   | B     |
| Faster-RCNN + Finetuning                                     |            | 0.0699 | 13,396 | 56.40 |       | 0.0371 | 12,291 | 51.00 | 25.00 | 38.00 |       | 0.0213 | 9,174  | 38.20 | 13.60 | 30.00 |       | 29.70  | 13.00 | 25.60 |
| DDETR + Finetuning                                           |            | 0.0608 | 33,270 | 60.30 |       | 0.0368 | 18,115 | 54.50 | 34.40 | 44.80 |       | 0.0197 | 9,392  | 40.00 | 17.80 | 33.30 |       | 32.50  | 20.00 | 29.40 |
| 【objectness】根据未匹配的 proposal 最有可能是前景对象的可能性得分 |            |        |        |       |       |        |        |       |       |       |       |        |        |       |       |       |       |        |       |       |
| ORE − EBUI                                                   | 2021，CVPR | 0.0621 | 10,459 | 56.00 | 4.9   | 0.0282 | 10,445 | 52.70 | 26.00 | 39.40 | 2.9   | 0.0211 | 7,990  | 38.20 | 12.70 | 29.70 | 3.9   | 29.60  | 12.40 | 25.30 |
| OW-DETR                                                      | 2022，CVPR | 0.0571 | 10,240 | 59.20 | 7.5   | 0.0278 | 8,441  | 53.60 | 33.50 | 42.90 | 6.2   | 0.0156 | 6,803  | 38.30 | 15.80 | 30.80 | 5.7   | 31.40  | 17.10 | 27.80 |
| SA                                                           | 2022，ICLR | 0.0417 | 4,889  | 56.20 | -     | 0.0213 | 2,546  | 53.39 | 26.49 | 39.94 | -     | 0.0146 | 2,120  | 38.04 | 12.81 | 29.63 | -     | 30.11  | 13.31 | 25.91 |
| UC-OWOD                                                      | 2022，ECCV | 0.0136 | 9,294  | 50.66 | 2.4   | 0.0117 | 5,602  | 33.13 | 30.54 | 31.84 | 3.4   | 0.0073 | 3,801  | 28.80 | 16.34 | 24.65 | 8.7   | 25.57  | 15.88 | 23.14 |
| RandBox                                                      | 2023，ICCV | 0.0240 | 4,498  | 61.80 | 10.6  | 0.0078 | 1,880  | -     | -     | 45.30 | 6.3   | 0.0054 | 1,452  | -     | -     | 39.40 | 7.8   | -      | -     | 35.40 |
| 使用额外的 proposal 生成技术例如 selected search             |            |        |        |       |       |        |        |       |       |       |       |        |        |       |       |       |       |        |       |       |
| CAT                                                          | 2023，CVPR | 0.0581 | 7,070  | 59.90 | 21.8  | 0.0263 | 5,902  | 54.00 | 33.60 | 43.80 | 18.6  | 0.0177 | 5,189  | 42.10 | 19.80 | 34.70 | 23.9  | 35.10  | 17.10 | 30.60 |
| 不使用伪标签                                                 |            |        |        |       |       |        |        |       |       |       |       |        |        |       |       |       |       |        |       |       |
| OCPL                                                         | 2022，ICIP | 0.0423 | 5,670  | 56.64 | 8.3   | 0.0220 | 5,690  | 50.65 | 27.54 | 39.10 | 7.7   | 0.0162 | 5,166  | 38.63 | 14.74 | 30.67 | 11.9  | 30.75  | 14.42 | 26.67 |
| 2B-OCD                                                       | 2022，HCMA | 0.0481 | -      | 56.37 | 12.1  | 0.0160 | -      | 51.57 | 25.34 | 38.46 | 9.4   | 0.0137 | -      | 37.24 | 13.23 | 29.24 | 11.7  | 30.06  | 13.28 | 25.82 |
| PROB                                                         | 2023，CVPR | 0.0569 | 5,195  | 59.50 | 19.4  | 0.0344 | 6,452  | 55.70 | 32.20 | 44.00 | 17.4  | 0.0151 | 2,641  | 43.00 | 22.20 | 36.00 | 19.6  | 35.70  | 18.90 | 31.50 |
| Annealling-RCNN                                              | 2023，CVPR | 0.0604 | 8,332  | 56.67 | 12.8  | 0.0269 | 9,454  | 51.96 | 29.13 | 40.55 | 5.0   | 0.0157 | 6,635  | 40.82 | 14.56 | 32.07 | 9.8   | 31.68  | 13.09 | 27.03 |
| Annealling-DETR                                              | 2023，CVPR | 0.0564 | 46,589 | 59.34 | 13.6  | 0.0274 | 24,709 | 53.18 | 37.98 | 45.58 | 10.0  | 0.0194 | 14,952 | 43.62 | 26.66 | 37.97 | 14.3  | 33.54  | 21.76 | 30.60 |
|                                                              |            |        |        |       |       |        |        |       |       |       |       |        |        |       |       |       |       |        |       |       |
| RandUn-owod                                                  | ours       | 0.0582 | 4,375  | 60.10 | 23.1  | 0.0242 | 2,329  | 51.66 | 34.06 | 42.86 | 18.5  | 0.0156 | 1,886  | 40.55 | 23.13 | 34.75 | 21.8  | 34.61  | 19.40 | 30.80 |
| RandUn-owdetr                                                |            | 0.0416 | 2373   | 64.13 | 28.34 | 0.0240 | 1423   | 53.87 | 47.46 | 50.66 | 28.33 | 0.0175 | 1333   | 49.39 | 43.06 | 47.28 | 33.66 | 47.01  | 45.43 | 46.61 |

指标分析：

* 相比较于使用 objectness 伪标签的方法，未知类召回率（23.1 ）大大提升，是现有 SOTA RandBox（10.6 ）的2.18 倍；
* 已知类的性能也直逼 SOTA 方法的性能，甚至超过了很多基于 DETR 的方法

模型优势：

* 简单 + 高效

## 方法尝试

### 文本和随机方法结合对比

|                            | Task 1 |       |       |       |
| -------------------------- | ------ | ----- | ----- | ----- |
|                            | WI     | AOSE  | C-m   | UR    |
| 仅使用文本相似性阈值       |        |       |       |       |
| c0.6                       | 0.0446 | 1,678 | 55.43 | 24.94 |
| c0.7                       | 0.0461 | 1,677 | 55.07 | 24.96 |
| c0.8                       | 0.0481 | 1,777 | 55.50 | 24.96 |
| c0.9                       | 0.0558 | 2,373 | 56.23 | 24.38 |
| 文本相似性阈值+随机选择5个 |        |       |       |       |
| c0.5 + rand5               | 0.0637 | 4,991 | 60.02 | 22.94 |
| c0.6 + rand5               | 0.0627 | 4,975 | 60.30 | 22.71 |
| c0.7 + rand5               | 0.0601 | 4,718 | 59.88 | 23.40 |
| c0.8 + rand5               | 0.0604 | 4,611 | 59.93 | 22.86 |
| c0.9 + rand5               | 0.0605 | 4,749 | 60.24 | 22.81 |

文本融合方法：首先将图片的候选框特征投射到语义空间（具体：将与 GT 匹配的候选框特征经过投射后与相应的标签文本特征做对齐），然后根据图像 caption 种的非已知类标签名词所对应的文本特征信息从剩下的候选框种选择未知类伪候选框。

分析：

* 若仅使用文本相似性阈值挑选未知类伪候选框，很可能因为非已知类标签名词所对应的文本特征 与 很多未能成功与 gt 匹配的已知类候选框相似性较高从而被误选则为未知类伪候选框，因此和 objectness 方法一样，仍然对已知类存在偏差，从而严重影响模型对已知类对象的识别能力；
* 通过在文本相似性阈值的基础上（具体：先根据相似性选出 m 个 sim 大于阈值的框，然后随机选择min（m， 5）个框作为最终的未知类伪候选框），加入随机性挑选部分候选框作为未知类伪候选框的方法能够很好的缓解对已知类的偏差。因为随机选择的方式能够在一定程度上减少误选已知类候选框作为未知类伪候选框的错误概率，因此能够达到去除偏差的目的。

### objectness-top5 vs ours-5

|                                    | Task 1 |        |       |       | Task 2 |        |       |       |       |       | Task 3 |        |       |       |       |       | Task 4 |       |       |
| ---------------------------------- | ------ | ------ | ----- | ----- | ------ | ------ | ----- | ----- | ----- | ----- | ------ | ------ | ----- | ----- | ----- | ----- | ------ | ----- | ----- |
|                                    | WI     | AOSE   | C-m   | UR    | WI     | AOSE   | P-m   | C-m   | B     | UR    | WI     | AOSE   | P-m   | C-m   | B     | UR    | P-m    | C-m   | B     |
| baseline-obj1（无nms）             | 0.0761 | 77,927 | 56.42 | 9.99  | 0.0428 | 39,682 | 45.94 | 30.03 | 37.99 | 6.94  | 0.0214 | 19,824 | 35.21 | 20.64 | 30.35 | 8.51  | 29.41  | 16.13 | 26.09 |
| baseline-obj1（有nms）             | 0.0654 | 33,240 | 57.70 | 9.69  | 0.0343 | 20,066 | 47.47 | 30.65 | 39.06 | 6.75  | 0.0189 | 11,525 | 36.47 | 21.16 | 31.37 | 8.31  | 30.61  | 16.52 | 27.09 |
| baseline-obj5（无nms）             | 0.0755 | 72,358 | 55.87 | 13.70 | 0.0437 | 38,361 | 43.72 | 29.29 | 36.51 | 10.28 | 0.0210 | 18,088 | 32.93 | 18.86 | 28.24 | 11.73 | 28.09  | 15.51 | 24.94 |
| c0.5 + rand5 + c0.8                | 0.0627 | 5,030  | 60.17 | 23.13 | 0.0239 | 2,502  | 52.19 | 34.06 | 43.13 | 19.62 | 0.0151 | 1,917  | 41.11 | 23.62 | 35.28 | 21.83 | 35.17  | 20.05 | 31.39 |
| c0.5 + rand5 + c0.8 + smallmin0.01 | 0.0610 | 4,854  | 60.10 | 23.31 | 0.0239 | 2,448  | 51.80 | 34.48 | 43.14 | 19.03 | 0.0152 | 1,971  | 40.75 | 23.17 | 34.89 | 22.06 | 34.99  | 19.46 | 31.11 |

unknown选择对比可视化：

​                obj-5                                  ours-5                                    unknown-gt

<img src="C:\Users\wangxuefei\Desktop\psd_boxes\obj5\000000027451.jpg" alt="000000027451" style="zoom:30%;" /><img src="C:\Users\wangxuefei\Desktop\psd_boxes\ours\000000027451.jpg" alt="000000027451" style="zoom:30%;" /><img src="C:\Users\wangxuefei\Desktop\psd_boxes\gt\000000027451.jpg" alt="000000027451" style="zoom:30%;" />

<img src="C:\Users\wangxuefei\Desktop\psd_boxes\obj5\000000381610.jpg" alt="000000381610" style="zoom:30%;" /><img src="C:\Users\wangxuefei\Desktop\psd_boxes\ours\000000381610.jpg" alt="000000381610" style="zoom:30%;" /><img src="C:\Users\wangxuefei\Desktop\psd_boxes\gt\000000381610.jpg" alt="000000381610" style="zoom:30%;" />

<img src="C:\Users\wangxuefei\Desktop\psd_boxes\obj5\000000571764.jpg" alt="000000571764" style="zoom:30%;" /><img src="C:\Users\wangxuefei\Desktop\psd_boxes\ours\000000571764.jpg" alt="000000571764" style="zoom:30%;" /><img src="C:\Users\wangxuefei\Desktop\psd_boxes\gt\000000571764.jpg" alt="000000571764" style="zoom:30%;" />

分析：基于 objectness 选的 unknown 框的重复性很高（缺点），但是框的面积一般都比较大（优点）

​			本文的方法，选出来的 unknown 伪框重复性较低（框之间的iou小）（优点），但框的面积有很多都偏小（缺点）

后续可以考虑在选择 unknown 伪框的时候，去掉 area 很小的框（一般 area 很小的框都不太可能包含对象）

### **方法 c0.5 + rand5 + c0.8 + smallmin0.01  **

在选择伪未知类框的时候，不考虑 area 小于当前图片中最小已知类对象面积*0.01 的框

结果：有些指标是会好点（也不是很明显），但不是一致偏好。

### 使用前序任务训练得到的原型指导后序任务

做法：在 t2 训练过程中，选择伪标签的时候，首先将 pred_embed 与 t1 训练后得到的前20个类别的原型计算相似度，把相似度高的框去掉，以减少误选之前已知类的概率。

结果：不理想，反向优化。

分析：可能是使用原型的方法，无差别的拉大不同类类原型之间的距离并不合理。（eg：猫原型的距离和狗原型的距离 d1 应当比猫和足球的距离d2 要小，即合理的情况下应该 d1 < d2）

**下一步计划：**

首先考虑一下去掉原型约束，而是直接将模型最后一个训练 epoch 的时候提取的特征保存下来，以平均值作为类中心。

然后，先试试上面的指导未知类选择的方法，看看是否有效；（效果不好，unrecall 17.24）

另外，还可以在训练 t2 的时候，对于那些与前序任务类中心相近的（以相似性或 l2 距离衡量）样例，减小其所对应损失计算的权重（类似于权重正则化的机制）。

### 高斯模糊

做法，对特征图 F0 使用高斯核进行模糊化得到 FT，然后使用 F残差（F0-FT） 表示对象有用信息，将 F 残差拼接到 F0 后面然后通过 1*1 卷积进行维度还原。（如下图中间部分）

<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231020131344285.png" alt="image-20231020131344285" style="zoom:50%;" />

t1 结果如下表（高斯核大小 = 5， sigma 如下）：

| sigma | WI     | AOSE | C-m   | UR    |
| ----- | ------ | ---- | ----- | ----- |
| 0.1   | 0.0683 | 6228 | 56.94 | 22.89 |
| 0.5   | 0.0707 | 6684 | 56.78 | 23.79 |
| 0.8   | 0.0679 | 6045 | 57.46 | 23.41 |

分析：没有像文章中说的那样，特征残差貌似没起到让前景对象突出的作用。等原文代码出来看看它代码咋写的。

### 高斯模糊+残差结构

| sigma | WI     | AOSE | C-m   | UR    |
| ----- | ------ | ---- | ----- | ----- |
| 0.01  | 0.0691 | 5754 | 58.39 | 23.09 |
| 0.05  | 0.0693 | 5562 | 58.97 | 22.89 |
| 0.08  | 0.0705 | 5971 | 57.33 | 23.58 |
| 0.1   | 0.0672 | 5666 | 58.70 | 22.69 |
| 0.5   | 0.0676 | 5873 | 58.13 | 23.20 |
| 0.8   | 0.0671 | 5758 | 58.74 | 23.31 |

### 文本投射到双曲空间

<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231101201515141.png" alt="image-20231101201515141" style="zoom: 50%;" />

由于 clip 特征空间的文本特征中，不同类别之间的区别不大；所以考虑将其投射到双曲空间从而拉大不同类别之间文本特征的距离。

假设投射到双曲空间之后的特征为 $f_{text_h}$，直接拉近相应类别特征 $f$ 和 $f_{text_h}$ 之间的距离，因此模型学习出来的特征也是会呈现出不同类别特征区别更为明显的情况，从而更有利于模型对于不同类别的区分。

|                                    | Task 1 |       |       |       | Task 2 |       |       |       |       |       | Task 3 |       |       |       |       |       | Task 4 |       |       |
| ---------------------------------- | ------ | ----- | ----- | ----- | ------ | ----- | ----- | ----- | ----- | ----- | ------ | ----- | ----- | ----- | ----- | ----- | ------ | ----- | ----- |
|                                    | WI     | AOSE  | C-m   | UR    | WI     | AOSE  | P-m   | C-m   | B     | UR    | WI     | AOSE  | P-m   | C-m   | B     | UR    | P-m    | C-m   | B     |
| c0.5 + rand5 + c0.8                | 0.0627 | 5,030 | 60.17 | 23.13 | 0.0239 | 2,502 | 52.19 | 34.06 | 43.13 | 19.62 | 0.0151 | 1,917 | 41.11 | 23.62 | 35.28 | 21.83 | 35.17  | 20.05 | 31.39 |
| c0.5 + rand5 + c0.8 + smallmin0.01 | 0.0610 | 4,854 | 60.10 | 23.31 | 0.0239 | 2,448 | 51.80 | 34.48 | 43.14 | 19.03 | 0.0152 | 1,971 | 40.75 | 23.17 | 34.89 | 22.06 | 34.99  | 19.46 | 31.11 |
| c0.5 + rand5 + c0.8                | 0.0625 | 4954  | 60.11 | 22.67 |        |       |       |       |       |       |        |       |       |       |       |       |        |       |       |
| c0.5 + rand5 + c0.85               | 0.0645 | 5250  | 60.20 | 23.07 | 0.0241 | 2684  | 52.51 | 34.87 | 43.69 | 18.97 | 0.0157 | 2072  | 41.05 | 23.85 | 35.32 | 22.47 | 35.11  | 19.47 | 31.20 |

整体看下来，将 clip 文本特征投射到双曲空间对已知类 AP 的性能是有一点作用的。

## coco-o 域泛化

### cartoon（实例数：8774）

|                             | Task 1 |       |       |      |       | Task 2 |       |       |       |       |      |       | Task 3 |      |       |      |       |      |       | Task 4 |       |       |
| --------------------------- | ------ | ----- | ----- | ---- | ----- | ------ | ----- | ----- | ----- | ----- | ---- | ----- | ------ | ---- | ----- | ---- | ----- | ---- | ----- | ------ | ----- | ----- |
|                             | WI     | AOSE  | C-m   | U-AP | UR    | WI     | AOSE  | P-m   | C-m   | B     | U-AP | UR    | WI     | AOSE | P-m   | C-m  | B     | U-AP | UR    | P-m    | C-m   | B     |
| ORE − EBUI                  | 0.1443 | 3328  | 18.07 | 4.65 | 15.95 | 0.0766 | 4178  | 19.56 | 14.76 | 17.16 | 3.61 | 16.11 | 0.0443 | 2917 | 13.66 | 4.73 | 10.68 | 1.06 | 10.44 | 10.42  | 11.75 | 10.76 |
| OW-DETR                     | 0.2229 | 15270 | 2.34  | 0.84 | 29.75 | 0.1040 | 10509 | 3.33  | 0.72  | 2.03  | 2.53 | 33.32 | 0.0544 | 5799 | 1.42  | 3.72 | 2.18  | 0.25 | 24.01 | 0.17   | 0.80  | 0.33  |
| ours（c0.5 + rand5 + c0.8） | 0.1995 | 1502  | 13.63 | 2.99 | 49.02 | 0.0983 | 1236  | 13.75 | 20.71 | 17.23 | 1.80 | 52.45 | 0.0556 | 613  | 13.37 | 9.00 | 11.91 | 1.34 | 52.06 | 12.36  | 15.23 | 13.08 |



### handmake（实例数：3266）

|                             | Task 1 |      |       |      |       | Task 2 |      |       |       |       |      |       | Task 3 |      |       |       |       |      |       | Task 4 |       |       |
| --------------------------- | ------ | ---- | ----- | ---- | ----- | ------ | ---- | ----- | ----- | ----- | ---- | ----- | ------ | ---- | ----- | ----- | ----- | ---- | ----- | ------ | ----- | ----- |
|                             | WI     | AOSE | C-m   | U-AP | UR    | WI     | AOSE | P-m   | C-m   | B     | U-AP | UR    | WI     | AOSE | P-m   | C-m   | B     | U-AP | UR    | P-m    | C-m   | B     |
| ORE − EBUI                  | 0.1610 | 1973 | 22.19 | 7.38 | 27.38 | 0.0775 | 2238 | 20.33 | 17.81 | 19.07 | 3.71 | 22.58 | 0.0365 | 1431 | 16.55 | 9.63  | 14.24 | 1.35 | 20.75 | 13.26  | 10.79 | 12.64 |
| OW-DETR                     | 0.1911 | 6732 | 4.24  | 1.35 | 38.72 | 0.1167 | 6124 | 3.55  | 0.70  | 2.13  | 1.96 | 43.46 | 0.0530 | 3139 | 1.80  | 3.75  | 2.45  | 1.21 | 24.96 | 0      | 1.33  | 0.33  |
| ours（c0.5 + rand5 + c0.8） | 0.2158 | 945  | 21.16 | 5.57 | 68.56 | 0.1013 | 629  | 15.65 | 21.09 | 18.37 | 2.62 | 63.59 | 0.0416 | 342  | 16.39 | 13.94 | 15.57 | 1.50 | 68.33 | 15.76  | 21.70 | 17.24 |



### painting（实例数：4879）

|                             | Task 1 |      |       |      |       | Task 2 |      |       |       |       |      |       | Task 3 |      |       |       |       |      |       | Task 4 |       |       |
| --------------------------- | ------ | ---- | ----- | ---- | ----- | ------ | ---- | ----- | ----- | ----- | ---- | ----- | ------ | ---- | ----- | ----- | ----- | ---- | ----- | ------ | ----- | ----- |
|                             | WI     | AOSE | C-m   | U-AP | UR    | WI     | AOSE | P-m   | C-m   | B     | U-AP | UR    | WI     | AOSE | P-m   | C-m   | B     | U-AP | UR    | P-m    | C-m   | B     |
| ORE − EBUI                  | 0.0956 | 1485 | 30.09 | 2.61 | 17.01 | 0.0436 | 1467 | 26.39 | 18.03 | 22.21 | 1.69 | 12.61 | 0.0235 | 1184 | 19.19 | 18.70 | 19.02 | 0.74 | 14.02 | 18.89  | 16.27 | 18.24 |
| OW-DETR                     | 0.1352 | 5202 | 6.32  | 1.12 | 30.78 | 0.0542 | 3447 | 5.22  | 0.92  | 3.07  | 0.98 | 33.90 | 0.0284 | 1985 | 1.84  | 8.31  | 4.00  | 0.29 | 28.45 | 0      | 2.34  | 0.58  |
| ours（c0.5 + rand5 + c0.8） | 0.1218 | 656  | 28.72 | 2.65 | 51.58 | 0.0400 | 326  | 20.01 | 22.08 | 21.04 | 0.88 | 48.49 | 0.0207 | 218  | 19.01 | 27.93 | 21.99 | 0.91 | 57.92 | 22.93  | 22.52 | 22.83 |

### sketch（实例数：3707）

|                             | Task 1 |      |       |      |       | Task 2 |      |       |       |       |            |       | Task 3 |      |       |          |          |      |       | Task 4    |      |          |
| --------------------------- | ------ | ---- | ----- | ---- | ----- | ------ | ---- | ----- | ----- | ----- | ---------- | ----- | ------ | ---- | ----- | -------- | -------- | ---- | ----- | --------- | ---- | -------- |
|                             | WI     | AOSE | C-m   | U-AP | UR    | WI     | AOSE | P-m   | C-m   | B     | U-AP       | UR    | WI     | AOSE | P-m   | C-m      | B        | U-AP | UR    | P-m       | C-m  | B        |
| ORE − EBUI                  | 0.1203 | 893  | 13.14 | 1.12 | 9.66  | 0.0603 | 865  | 12.30 | 10.95 | 11.62 | 0.90       | 11.92 | 0.0282 | 558  | 10.75 | nan/3.38 | nan/8.29 | 0.36 | 8.11  | nan/8.79  | 6.27 | nan/8.16 |
| OW-DETR                     | 0.1488 | 4078 | 3.95  | 1.68 | 27.10 | 0.0829 | 3314 | 2.30  | 0.51  | 1.40  | 9.76(离谱) | 30.27 | 0.0345 | 1646 | 1.25  | 2.28     | 1.59     | 0.14 | 17.95 | 0         | 0.63 | 0.15     |
| ours（c0.5 + rand5 + c0.8） | 0.1419 | 446  | 11.50 | 2.32 | 47.81 | 0.0528 | 251  | 9.15  | 12.65 | 10.90 | 0.73       | 42.89 | 0.0348 | 144  | 9.36  | nan/3.18 | nan/7.30 | 0.59 | 45.72 | nan/10.32 | 6.72 | nan/9.42 |



### tattoo（实例数：1489）

|                             | Task 1 |      |           |      |       | Task 2 |      |           |           |           |      |       | Task 3 |      |          |       |          |      |       | Task 4   |           |           |
| --------------------------- | ------ | ---- | --------- | ---- | ----- | ------ | ---- | --------- | --------- | --------- | ---- | ----- | ------ | ---- | -------- | ----- | -------- | ---- | ----- | -------- | --------- | --------- |
|                             | WI     | AOSE | C-m       | U-AP | UR    | WI     | AOSE | P-m       | C-m       | B         | U-AP | UR    | WI     | AOSE | P-m      | C-m   | B        | U-AP | UR    | P-m      | C-m       | B         |
| ORE − EBUI                  | 0.0830 | 502  | nan/14.17 | 1.78 | 10.07 | 0.0310 | 381  | nan/11.63 | nan/10.33 | nan/10.98 | 0.51 | 7.26  | 0.0185 | 302  | nan/9.71 | 7.46  | nan/8.96 | 0.30 | 8.77  | nan/9.64 | nan/7.34  | nan/9.07  |
| OW-DETR                     | 0.0165 | 931  | 1.78      | 3.09 | 19.82 | 0.0170 | 1039 | 2.30      | 0.03      | 1.16      | 0.39 | 24.10 | 0.0140 | 887  | 0.89     | 2.15  | 1.31     | 0.50 | 35.31 | 0.05     | 0.38      | 0.13      |
| ours（c0.5 + rand5 + c0.8） | 0.0726 | 185  | nan/8.46  | 1.93 | 56.44 | 0.0381 | 128  | nan/8.40  | nan/13.52 | nan/10.96 | 0.71 | 47.86 | 0.0168 | 69   | nan/8.04 | 10.47 | nan/8.85 | 0.58 | 63.50 | nan/9.67 | nan/14.20 | nan/10.80 |

### weather（实例数：4509）

|                             | Task 1 |      |           |      |       | Task 2 |      |           |           |           |        |      | Task 3   |      |           |          |           |        |       | Task 4    |          |           |
| --------------------------- | ------ | ---- | --------- | ---- | ----- | ------ | ---- | --------- | --------- | --------- | ------ | ---- | -------- | ---- | --------- | -------- | --------- | ------ | ----- | --------- | -------- | --------- |
|                             | WI     | AOSE | C-m       | U-AP | UR    | WI     | AOSE | P-m       | C-m       | B         | U-AP   | UR   | WI       | AOSE | P-m       | C-m      | B         | U-AP   | UR    | P-m       | C-m      | B         |
| ORE − EBUI                  | 0.0645 | 927  | nan/26.97 | 0.75 | 11.96 | 0.0004 | 19   | nan/24.90 | nan/17.44 | nan/21.17 | 0      | 0    | 7.13e-05 | 5    | nan/19.71 | nan/0.89 | nan/13.44 | 0      | 0     | nan/13.50 | nan/0.65 | nan/10.29 |
| OW-DETR                     | 0.0324 | 1326 | 9.44      | 3.19 | 26.97 | 0.0002 | 20   | 8.09      | 0.39      | 4.24      | 0.0018 | 7.5  | 0        | 0    | 3.09      | 1.03     | 2.40      | 0.01   | 12.5  | 0.07      | 0.91     | 0.28      |
| ours（c0.5 + rand5 + c0.8） | 0.0752 | 386  | nan/29.96 | 1.14 | 44.84 | 0.0004 | 4    | nan/22.96 | nan/21.94 | nan/22.45 | 0.01   | 17.5 | 0        | 1    | nan/18.75 | nan/2.71 | nan/13.40 | 0.0075 | 31.25 | nan/13.18 | nan/2.79 | nan/10.59 |

### 注：

nan？ 修改方式，当 recall 和 ap 为 nan 时 置 0 ↓

  ```python
  # detectron2/evaluation/pascal_voc_evaluation.py
  def voc_eval():
      rec = [0 if math.isnan(i) else i for i in rec] # wxf 20231107 解决 domain-test 出现结果 nan 的问题 
      ap = 0 if math.isnan(ap) else ap # wxf 20231107 解决 domain-test 出现结果 nan 的问题
      return rec, prec, ap, is_unk_sum, n_unk, tp_plus_fp_closed_set, fp_open_set
  ```

OW-DETR t4 的测试结果，由于作者未上传 t4_ft 的模型训练权重，暂时使用的是 t4 的权重。（已发邮件请作者上传 t4_ft 权重）

## 方法尝试

动机：由于 CLIP 特征空间中，常见类别名称对应的文本特征之间的相似性很高，所以用于目标检测中时指导性较弱；

因此尝试将 CLIP 的文本特征投射到对应的图像特征空间中，并且增强不同类别的可辨认程度（通过可学习的网络变换 Net 之后，减小不同类名文本特征之间的相似度）

```
# Net:
import torch.nn as nn

class MLP_adapter(nn.Module):
    # Non-Linear Transformation in the paper, acting as the translator between modalities.
    def __init__(self, in_dim:int, hidden_dim:int, out_dim:int):
        super().__init__()
        self.linear1 = nn.Linear(in_dim, hidden_dim)
        self.linear2 = nn.Linear(hidden_dim, out_dim)
        self.relu = nn.ReLU(inplace=True)
    
    def forward(self, x):
        x = self.linear1(x)
        x = self.relu(x)
        # x = x + self.relu(x) # res
        x = self.linear2(x)
        return x
```



对于未知类别检测的指导方式：将 Caption 中的 novel 名词所对应的 CLIP 空间文本特征，经 Net 变换之后得到的特征，作为标注伪未知类标签的指导特征。

实验结果（pro-）：

|                             | t1     | t1    | t1    | t1    |
| --------------------------- | ------ | ----- | ----- | ----- |
|                             | WI     | AOSE  | C-m   | UR    |
| randun                      | 0.0582 | 4,375 | 60.10 | 23.09 |
| pro-sim0.5-rand5            | 0.0540 | 4410  | 60.02 | 21.62 |
| pro-sim0.5-rand5-res        | 0.0563 | 4429  | 59.98 | 21.88 |
| pro-sim0.5-rand5-sim0.8     | 0.0655 | 8680  | 60.76 | 18.16 |
| pro-sim0.5-rand5-sim0.8-res | 0.0666 | 8593  | 60.69 | 17.96 |

分析：

观察变换后的20个类原型的相似度，确实变得很小了，起到了区分作用；

从实验结果来看，模型对未知类的检测能力下降厉害。可能是因为，只能对已知类别的文本特征做约束，这导致学到的映射关系可能会偏向于已知类文本特征到图像特征的映射，而未知类的映射关系较弱，造成文本信息伪标签指导性不佳。



|                       |      | Task 1 |       |       |       | Task 2 |       |       |       |       |       | Task 3 |       |       |       |       |       | Task 4 |       |       |
| --------------------- | ---- | ------ | ----- | ----- | ----- | ------ | ----- | ----- | ----- | ----- | ----- | ------ | ----- | ----- | ----- | ----- | ----- | ------ | ----- | ----- |
|                       |      | WI     | AOSE  | C-m   | UR    | WI     | AOSE  | m-AP  |       |       | UR    | WI     | AOSE  | m-AP  |       |       | UR    | m-AP   |       |       |
|                       |      |        |       |       |       |        |       | P-m   | C-m   | B     |       |        |       | P-m   | C-m   | B     |       | P-m    | C-m   | B     |
| RandUn-owod           | ours | 0.0582 | 4,375 | 60.10 | 23.1  | 0.0242 | 2,329 | 51.66 | 34.06 | 42.86 | 18.5  | 0.0156 | 1,886 | 40.55 | 23.13 | 34.75 | 21.8  | 34.61  | 19.40 | 30.80 |
| RandUn_cap_s0.5r5s0.8 |      | 0.0587 | 4572  | 60.04 | 23.24 | 0.0240 | 2387  | 52.15 | 34.58 | 43.36 | 19.04 | 0.0154 | 1900  | 41.31 | 24.33 | 35.65 | 22.43 | 35.28  | 19.61 | 31.36 |
| RandUn_cap_pro_s0.6r5 |      | 0.0577 | 4901  | 60.36 | 21.49 | 0.0219 | 2612  | 53.42 | 33.53 | 43.47 | 17.80 | 0.0147 | 1945  | 41.43 | 22.66 | 35.17 | 20.23 | 35.66  | 19.58 | 31.64 |

RandUn_cap_s0.5r5s0.8：文本不做投射变换。

RandUn_cap_pro_s0.6r5  :文本做投射變換，相似性閾值為0.6，然後隨機選擇5個偽未知候選框。結果分析：已知類指標變好，未知類召回率變差。另外，在增量任务中，虽然整体上mAP是变好的，但是呈现出 pre-mAP 增大， cur-mAP 减小的趋势。

总体来看，对文本做投射变换后的结果还不如做投射变换前的。可能是因为 CLIP 的训练物料非常丰富，所学习到的类别特征表示已经将类别间的关系考虑在内，所以即使相似性高也没关系。？

|||||

|                            | t1     | t1   | t1    | t1    |
| -------------------------- | ------ | ---- | ----- | ----- |
|                            | WI     | AOSE | C-m   | UR    |
| RandUn_cap_pro_sim_only0.5 | 0.0412 | 1608 | 54.39 | 23.61 |
| RandUn_cap_pro_sim_only0.6 | 0.0515 | 2384 | 56.00 | 22.89 |
| RandUn_cap_pro_sim_only0.7 | 0.0581 | 3047 | 56.71 | 22.57 |

分析：对文本做投射之后的结果，甚至还不如不做映射之前（mAP高了0.几， UR降了将近2个点）。（link 方法尝试->文本和随机方法结合对比）

|||||

## 消融_RandUn_cap_s0.5r5s0.8

|                                 | Task 1 |        |       |       | Task 2 |        |       |       |       |       | Task 3 |        |       |       |       |       | Task 4 |       |       |
| ------------------------------- | ------ | ------ | ----- | ----- | ------ | ------ | ----- | ----- | ----- | ----- | ------ | ------ | ----- | ----- | ----- | ----- | ------ | ----- | ----- |
|                                 | WI     | AOSE   | C-m   | UR    | WI     | AOSE   | m-AP  |       |       | UR    | WI     | AOSE   | m-AP  |       |       | UR    | m-AP   |       |       |
|                                 |        |        |       |       |        |        | P-m   | C-m   | B     |       |        |        | P-m   | C-m   | B     |       | P-m    | C-m   | B     |
| Featurized Query R-CNN（无nms） | 0.0718 | 79,516 | 57.06 | 0.00  | 0.0390 | 40,259 | 47.30 | 30.73 | 39.01 | 0.00  | 0.0197 | 20,307 | 37.53 | 22.08 | 32.38 | 0.00  | 31.05  | 16.74 | 27.48 |
| Featurized Query R-CNN（有nms） | 0.0625 | 34,978 | 58.32 | 0.00  | 0.0322 | 21,548 | 48.79 | 31.22 | 40.00 | 0.00  | 0.0178 | 11,779 | 38.61 | 22.62 | 33.28 | 0.00  | 32.21  | 17.21 | 28.46 |
| baseline-obj1（无nms）          | 0.0761 | 77,927 | 56.42 | 9.99  | 0.0428 | 39,682 | 45.94 | 30.03 | 37.99 | 6.94  | 0.0214 | 19,824 | 35.21 | 20.64 | 30.35 | 8.51  | 29.41  | 16.13 | 26.09 |
| baseline-obj1（有nms）          | 0.0654 | 33,240 | 57.70 | 9.69  | 0.0343 | 20,066 | 47.47 | 30.65 | 39.06 | 6.75  | 0.0189 | 11,525 | 36.47 | 21.16 | 31.37 | 8.31  | 30.61  | 16.52 | 27.09 |
| baseline-obj5（无nms）          | 0.0755 | 72,358 | 55.87 | 13.70 | 0.0437 | 38,361 | 43.72 | 29.29 | 36.51 | 10.28 | 0.0210 | 18,088 | 32.93 | 18.86 | 28.24 | 11.73 | 28.09  | 15.51 | 24.94 |
| s5r5s8+hungari                  | 0.0756 | 15,169 | 56.33 | 22.79 | 0.0354 | 9,560  | 44.22 | 27.85 | 36.04 | 21.82 | 0.0179 | 5,975  | 29.71 | 16.59 | 25.34 | 22.65 | 29.78  | 16.03 | 26.34 |
| OTA                             | 0.0620 | 4,965  | 60.01 | 23.08 | 0.0242 | 2,454  | 52.01 | 34.17 | 43.09 | 19.26 | 0.0158 | 2,002  | 41.01 | 23.42 | 35.14 | 22.06 | 34.78  | 19.74 | 31.02 |
| ROI-Attn                        | 0.0629 | 5,098  | 60.19 | 23.23 | 0.0260 | 2,707  | 51.98 | 34.24 | 43.11 | 18.71 | 0.0152 | 1,938  | 41.37 | 23.35 | 35.36 | 22.14 | 35.16  | 19.40 | 31.22 |

说明：

* baseline-obj1，FQ + obj1，根据 ORE 的选择方式，从未匹配的预测框中选择一个 objectness 最高预测框作为未知类伪标签；
* Un-Select，未知类伪标签随机选择方式消融；随机选择未匹配的 5 各候选框作为 unknown；
* s5r5s8+hungari,baseline的选未知类伪框的方式换成文本+随机的组合；
* OTA，在上一行的基础上，将 HungarianMatcher 换成 OtaMatcher，ota 的 OTA_TOP_CANDIDATES = 5；
* ROI-Attn，在 OTA 的基础上，加上 ROI-Attn 模块，相当于做特征增强。

t1消融结果

|                           | WI     | AOSE   | C-m   | UR    |
| ------------------------- | ------ | ------ | ----- | ----- |
| baseline                  | 0.0718 | 79,516 | 57.06 | 0.00  |
| baseline+obj1             | 0.0761 | 77,927 | 56.42 | 9.99  |
| baseline+obj5             | 0.0755 | 72,358 | 55.87 | 13.70 |
| baseline+SRS              | 0.0756 | 15,169 | 56.33 | 22.79 |
| baseline+SRS+OTA          | 0.0637 | 5,143  | 59.96 | 22.84 |
| baseline+SRS+OTA+ROI-Attn | 0.0625 | 5,025  | 60.33 | 23.05 |

SRS对应伪未知类候选框选择方法

OTA表示将匈牙利匹配方式换成OTA多标签分配

ROI-Attn表示对ROI-feature做去噪增强，增强对象特征

（后面两行都是三次结果的平均值）

<img src="https://raw.githubusercontent.com/yuki1ssad/typora_images/main/image-20231214224008931.png" alt="image-20231214224008931" style="zoom:60%;" />

## ROI-Attn vs BiLstm vs GCN

|                           | WI     | AOSE  | mAP   | UR    |
| ------------------------- | ------ | ----- | ----- | ----- |
| baseline+SRS+OTA          | 0.0637 | 5,143 | 59.96 | 22.84 |
| baseline+SRS+OTA+ROI-Attn | 0.0625 | 5,025 | 60.33 | 23.05 |
| BiLstm                    | 0.0638 | 5136  | 60.3  | 22.69 |
| GCN                       | 0.0638 | 5052  | 59.94 | 23.52 |

BiLstm ：双向 Lstm，对所有 ROI 进行双向lstm编码，每个 ROI 特征增强结果取前向结果和后向结果的平均值，也是用也残差结构。

​	即： aug_roi = ori_roi + aug_roi

​			aug_roi = lstm_forward(roi) + lstm_backward(roi)

（**Learning Object Context for Dense Captioning ** 使用了单向LSTM来学习对象的上下文信息）

GCN： 参考 **Vision GNN: An Image is Worth Graph of Nodes**，将每个 roi 特征作为一个节点，根据 roi 之间的距离选择 topk 个 roi 作为邻居，进行一次聚合传播。

相对来说，使用roi特征增强都有点效果。
